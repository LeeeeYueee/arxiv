<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- PAGE settings -->
  <link rel="icon" href="https://templates.pingendo.com/assets/Pingendo_favicon.ico">
  <title>Arxiv Daily</title>
  <!-- CSS dependencies -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    type="text/css">
  <link rel="stylesheet" href="assets/wireframe.css">
</head>

<body class="bg-light">
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="text-center col-md-7 mx-auto"> <i class="fa d-block fa-bullseye fa-5x mb-4 text-info"></i>
          <h2><b>Arxiv Daily - 2024-02-21</b></h2>
        </div>
      </div>
    </div>
  </div>
  <div class="py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12">
          <ul class="nav nav-tabs">
            
            <li class="nav-item"> <a href="" class="nav-link active"
                data-toggle="pill" data-target="#SLAM"><i class="fa fa-home"></i> SLAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SFM"><i class="fa fa-home"></i> SFM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#LLM"><i class="fa fa-home"></i> LLM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#SAM"><i class="fa fa-home"></i> SAM</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#CLIP"><i class="fa fa-home"></i> CLIP</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#DINO"><i class="fa fa-home"></i> DINO</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#MAEVAD"><i class="fa fa-home"></i> MAEVAD</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#VAD"><i class="fa fa-home"></i> VAD</a>
            </li>
            
            <li class="nav-item"> <a href="" class="nav-link"
                data-toggle="pill" data-target="#MAE"><i class="fa fa-home"></i> MAE</a>
            </li>
            
          </ul>
          <div class="tab-content mt-2">
            
            <div class="tab-pane fade active show" id="SLAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_13255v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13255v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13255v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13255v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments. This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) representations. Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of SLAM progress through the lens of the latest advancements in radiance fields. It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13255v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的二十年里，同步定位和测绘（SLAM）领域的研究经历了重大的发展，突出了其在实现未知环境的自主探索方面的关键作用。这种演变从手工制作的方法到深度学习时代，再到最近专注于神经辐射场（NeRFs）和3D高斯散射（3DGS）表示的发展。认识到越来越多的研究和缺乏对该主题的全面调查，本文旨在通过辐射场的最新进展，首次全面概述SLAM的进展。它揭示了背景、进化路径、固有优势和局限性，并作为突出动态进展和具体挑战的基本参考。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13255v1" target="_blank">2402.13255v1</a>
                              </td>
                              <td>How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey</td>
                              <td>Fabio Tosi</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13255v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13255v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07429v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Particle Filter SLAM for Vehicle Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07429v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07429v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07429v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous Localization and Mapping (SLAM) presents a formidable challenge in robotics, involving the dynamic construction of a map while concurrently determining the precise location of the robotic agent within an unfamiliar environment. This intricate task is further compounded by the inherent "chicken-and-egg" dilemma, where accurate mapping relies on a dependable estimation of the robot's location, and vice versa. Moreover, the computational intensity of SLAM adds an additional layer of complexity, making it a crucial yet demanding topic in the field. In our research, we address the challenges of SLAM by adopting the Particle Filter SLAM method. Our approach leverages encoded data and fiber optic gyro (FOG) information to enable precise estimation of vehicle motion, while lidar technology contributes to environmental perception by providing detailed insights into surrounding obstacles. The integration of these data streams culminates in the establishment of a Particle Filter SLAM framework, representing a key endeavor in this paper to effectively navigate and overcome the complexities associated with simultaneous localization and mapping in robotic systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07429v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）在机器人技术中提出了一个巨大的挑战，涉及地图的动态构建，同时确定机器人代理在陌生环境中的精确位置。这项复杂的任务因固有的“鸡和蛋”困境而进一步复杂化，在这种困境中，准确的地图绘制依赖于对机器人位置的可靠估计，反之亦然。此外，SLAM的计算强度增加了额外的复杂性，使其成为该领域中一个至关重要但要求很高的主题。在我们的研究中，我们通过采用粒子滤波器SLAM方法来应对SLAM的挑战。我们的方法利用编码数据和光纤陀螺（FOG）信息来实现对车辆运动的精确估计，而激光雷达技术通过提供对周围障碍物的详细见解来促进环境感知。这些数据流的集成最终建立了粒子滤波器SLAM框架，这是本文中有效导航和克服机器人系统中同时定位和映射相关复杂性的一项关键努力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07429v2" target="_blank">2402.07429v2</a>
                              </td>
                              <td>Particle Filter SLAM for Vehicle Localization</td>
                              <td>Tianrui Liu</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07429v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07429v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_12551v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Landmark-based Localization using Stereo Vision and Deep Learning in GPS-Denied Battlefield Environment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_12551v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_12551v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_12551v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Localization in a battlefield environment is increasingly challenging as GPS connectivity is often denied or unreliable, and physical deployment of anchor nodes across wireless networks for localization can be difficult in hostile battlefield terrain. Existing range-free localization methods rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in dynamic and sparse wireless network topology. Vision-based methods like SLAM and Visual Odometry use expensive sensor fusion techniques for map generation and pose estimation. This paper proposes a novel framework for localization in non-GPS battlefield environments using only the passive camera sensors and considering naturally existing or artificial landmarks as anchors. The proposed method utilizes a customcalibrated stereo vision camera for distance estimation and the YOLOv8s model, which is trained and fine-tuned with our real-world dataset for landmark recognition. The depth images are generated using an efficient stereomatching algorithm, and distances to landmarks are determined by extracting the landmark depth feature utilizing a bounding box predicted by the landmark recognition model. The position of the unknown node is then obtained using the efficient least square algorithm and then optimized using the L-BFGS-B (limited-memory quasi-Newton code for bound-constrained optimization) method. Experimental results demonstrate that our proposed framework performs better than existing anchorbased DV-Hop algorithms and competes with the most efficient vision-based algorithms in terms of localization error (RMSE).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_12551v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>战场环境中的定位越来越具有挑战性，因为GPS连接经常被拒绝或不可靠，并且在敌对战场地形中难以在无线网络中物理部署锚节点进行定位。现有的无距离定位方法依赖于基于无线电的锚及其平均跳跃距离，这在动态和稀疏的无线网络拓扑中影响了准确性和稳定性。SLAM和视觉里程计等基于视觉的方法使用昂贵的传感器融合技术来生成地图和进行姿态估计。本文提出了一种在非GPS战场环境中定位的新框架，仅使用被动相机传感器，并将自然存在或人工地标视为锚。所提出的方法利用定制校准的立体视觉相机进行距离估计，并使用YOLOv8s模型，该模型与我们的真实世界数据集进行训练和微调，用于地标识别。使用有效的立体匹配算法生成深度图像，并且通过利用由地标识别模型预测的边界框提取地标深度特征来确定到地标的距离。然后使用有效的最小二乘算法获得未知节点的位置，然后使用L-BFGS-B（用于约束优化的有限记忆准牛顿码）方法进行优化。实验结果表明，我们提出的框架比现有的基于锚的DV-Hop算法性能更好，并且在定位误差（RMSE）方面与最有效的基于视觉的算法竞争。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.12551v1" target="_blank">2402.12551v1</a>
                              </td>
                              <td>Landmark-based Localization using Stereo Vision and Deep Learning in GPS-Denied Battlefield Environment</td>
                              <td>Ganesh Sapkota</td>
                              <td>2024-02-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_12551v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.12551v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_12149v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore the Momentum in Competitive Sports</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_12149v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_12149v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_12149v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tennis is so popular that coaches and players are curious about factors other than skill, such as momentum. This article will try to define and quantify momentum, providing a basis for real-time analysis of tennis matches. Based on the tennis Grand Slam men's singles match data in recent years, we built two models, one is to build a model based on data-driven, and the other is to build a model based on empirical formulas. For the data-driven model, we first found a large amount of public data including public data on tennis matches in the past five years and personal information data of players. Then the data is preprocessed, and feature engineered, and a fusion model of SVM, Random Forrest algorithm and XGBoost was established. For the mechanism analysis model, important features were selected based on the suggestions of many tennis players and enthusiasts, the sliding window algorithm was used to calculate the weight, and different methods were used to visualize the momentum. For further analysis of the momentum fluctuation, it is based on the popular CUMSUM algorithm in the industry as well as the RUN Test, and the result shows the momentum is not random and the trend might be random. At last, the robustness of the fusion model is analyzed by Monte Carlo simulation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_12149v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>网球如此受欢迎，以至于教练和运动员都对技巧之外的因素感到好奇，比如动量。本文将试图定义和量化动量，为网球比赛的实时分析提供依据。基于近年来网球大满贯男单比赛数据，我们构建了两个模型，一个是基于数据驱动的模型，另一个是根据经验公式的模型。对于数据驱动的模型，我们首先发现了大量的公共数据，包括过去五年网球比赛的公共数据和球员的个人信息数据。然后对数据进行预处理和特征工程，建立了支持向量机、随机福雷斯特算法和XGBoost的融合模型。对于机理分析模型，根据许多网球运动员和爱好者的建议选择了重要特征，使用滑动窗口算法计算重量，并使用不同的方法可视化动量。为了进一步分析动量波动，它基于业界流行的CUMSUM算法和RUN测试，结果表明动量不是随机的，趋势可能是随机的。最后，通过蒙特卡洛仿真对融合模型的鲁棒性进行了分析。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.12149v1" target="_blank">2402.12149v1</a>
                              </td>
                              <td>MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore the Momentum in Competitive Sports</td>
                              <td>Ruixin Peng</td>
                              <td>2024-02-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_12149v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.12149v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11790v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic Swarms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11790v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11790v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11790v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Collaborative state estimation using different heterogeneous sensors is a fundamental prerequisite for robotic swarms operating in GPS-denied environments, posing a significant research challenge. In this paper, we introduce a centralized system to facilitate collaborative LiDAR-ranging-inertial state estimation, enabling robotic swarms to operate without the need for anchor deployment. The system efficiently distributes computationally intensive tasks to a central server, thereby reducing the computational burden on individual robots for local odometry calculations. The server back-end establishes a global reference by leveraging shared data and refining joint pose graph optimization through place recognition, global optimization techniques, and removal of outlier data to ensure precise and robust collaborative state estimation. Extensive evaluations of our system, utilizing both publicly available datasets and our custom datasets, demonstrate significant enhancements in the accuracy of collaborative SLAM estimates. Moreover, our system exhibits remarkable proficiency in large-scale missions, seamlessly enabling ten robots to collaborate effectively in performing SLAM tasks. In order to contribute to the research community, we will make our code open-source and accessible at \url{https://github.com/PengYu-team/Co-LRIO}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11790v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用不同的异构传感器进行协作状态估计是机器人群在拒绝GPS的环境中运行的基本前提，这对研究提出了重大挑战。在本文中，我们介绍了一个集中式系统，以促进协作式激光雷达测距惯性状态估计，使机器人群能够在不需要部署锚的情况下运行。该系统有效地将计算密集型任务分配到中央服务器，从而减少了用于局部里程计计算的单个机器人的计算负担。服务器后端通过利用共享数据建立全局参考，并通过位置识别、全局优化技术和去除异常数据来完善联合姿态图优化，以确保精确和稳健的协作状态估计。利用公开可用的数据集和我们的自定义数据集，对我们的系统进行了广泛的评估，证明了协作SLAM估计的准确性显著提高。此外，我们的系统在大规模任务中表现出非凡的熟练度，使十个机器人能够无缝地有效协作执行SLAM任务。为了对研究社区做出贡献，我们将使我们的代码开源并可访问\url{https://github.com/PengYu-team/Co-LRIO}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11790v1" target="_blank">2402.11790v1</a>
                              </td>
                              <td>CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic Swarms</td>
                              <td>Shipeng Zhong</td>
                              <td>2024-02-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11790v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11790v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11680v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11680v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11680v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11680v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Storing and transmitting LiDAR point cloud data is essential for many AV applications, such as training data collection, remote control, cloud services or SLAM. However, due to the sparsity and unordered structure of the data, it is difficult to compress point cloud data to a low volume. Transforming the raw point cloud data into a dense 2D matrix structure is a promising way for applying compression algorithms. We propose a new lossless and calibrated 3D-to-2D transformation which allows compression algorithms to efficiently exploit spatial correlations within the 2D representation. To compress the structured representation, we use common image compression methods and also a self-supervised deep compression approach using a recurrent neural network. We also rearrange the LiDAR's intensity measurements to a dense 2D representation and propose a new metric to evaluate the compression performance of the intensity. Compared to approaches that are based on generic octree point cloud compression or based on raw point cloud data compression, our approach achieves the best quantitative and visual performance. Source code and dataset are available at https://github.com/ika-rwth-aachen/Point-Cloud-Compression.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11680v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>存储和传输激光雷达点云数据对于许多AV应用程序至关重要，例如训练数据收集、远程控制、云服务或SLAM。然而，由于数据的稀疏性和无序结构，很难将点云数据压缩到较低的体积。将原始点云数据转换为密集的二维矩阵结构是应用压缩算法的一种很有前途的方法。我们提出了一种新的无损和校准的3D-2D变换，该变换允许压缩算法有效地利用2D表示中的空间相关性。为了压缩结构化表示，我们使用了常见的图像压缩方法和使用递归神经网络的自监督深度压缩方法。我们还将激光雷达的强度测量重新排列为密集的2D表示，并提出了一种新的度量来评估强度的压缩性能。与基于通用八叉树点云压缩或基于原始点云数据压缩的方法相比，我们的方法实现了最佳的定量和可视化性能。源代码和数据集可在https://github.com/ika-rwth-aachen/Point-Cloud-Compression.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11680v1" target="_blank">2402.11680v1</a>
                              </td>
                              <td>3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods</td>
                              <td>Till Beemelmanns</td>
                              <td>2024-02-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11680v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11680v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ika-rwth-aachen/point-cloud-compression" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_09944v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Loopy-SLAM: Dense Neural SLAM with Loop Closures</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_09944v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_09944v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_09944v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural RGBD SLAM techniques have shown promise in dense Simultaneous Localization And Mapping (SLAM), yet face challenges such as error accumulation during camera tracking resulting in distorted maps. In response, we introduce Loopy-SLAM that globally optimizes poses and the dense 3D model. We use frame-to-model tracking using a data-driven point-based submap generation method and trigger loop closures online by performing global place recognition. Robust pose graph optimization is used to rigidly align the local submaps. As our representation is point based, map corrections can be performed efficiently without the need to store the entire history of input frames used for mapping as typically required by methods employing a grid based mapping structure. Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNet datasets demonstrate competitive or superior performance in tracking, mapping, and rendering accuracy when compared to existing dense neural RGBD SLAM methods. Project page: notchla.github.io/Loopy-SLAM.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_09944v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经RGBD SLAM技术在密集的同时定位和映射（SLAM）中显示出了前景，但也面临着诸如相机跟踪过程中的误差积累导致地图失真等挑战。作为回应，我们引入了Loopy SLAM，它可以全局优化姿势和密集的3D模型。我们使用基于数据驱动的点的子映射生成方法使用帧到模型跟踪，并通过执行全局位置识别在线触发循环闭合。鲁棒姿态图优化用于刚性对齐局部子映射。由于我们的表示是基于点的，因此可以有效地执行映射校正，而不需要像使用基于网格的映射结构的方法通常所要求的那样存储用于映射的输入帧的整个历史。与现有的密集神经RGBD SLAM方法相比，对合成副本和真实世界TUM-RGBD和ScanNet数据集的评估表明，在跟踪、映射和渲染精度方面具有竞争力或优越的性能。项目页面：notchla.gitub.io/Loopy-SLAM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.09944v1" target="_blank">2402.09944v1</a>
                              </td>
                              <td>Loopy-SLAM: Dense Neural SLAM with Loop Closures</td>
                              <td>Lorenzo Liso</td>
                              <td>2024-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_09944v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.09944v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08897v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RB5 Low-Cost Explorer: Implementing Autonomous Long-Term Exploration on Low-Cost Robotic Hardware</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08897v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08897v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08897v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This systems paper presents the implementation and design of RB5, a wheeled robot for autonomous long-term exploration with fewer and cheaper sensors. Requiring just an RGB-D camera and low-power computing hardware, the system consists of an experimental platform with rocker-bogie suspension. It operates in unknown and GPS-denied environments and on indoor and outdoor terrains. The exploration consists of a methodology that extends frontier- and sampling-based exploration with a path-following vector field and a state-of-the-art SLAM algorithm. The methodology allows the robot to explore its surroundings at lower update frequencies, enabling the use of lower-performing and lower-cost hardware while still retaining good autonomous performance. The approach further consists of a methodology to interact with a remotely located human operator based on an inexpensive long-range and low-power communication technology from the internet-of-things domain (i.e., LoRa) and a customized communication protocol. The results and the feasibility analysis show the possible applications and limitations of the approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08897v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了RB5的实现和设计，这是一种用于自主长期探索的轮式机器人，具有更少、更便宜的传感器。该系统只需要一台RGB-D相机和低功耗计算硬件，由一个带摇杆转向架悬架的实验平台组成。它在未知和GPS拒绝的环境中以及在室内和室外地形上运行。该探索包括一种方法，该方法通过路径跟随矢量场和最先进的SLAM算法扩展了基于边界和采样的探索。该方法允许机器人以较低的更新频率探索周围环境，从而能够使用性能较低、成本较低的硬件，同时仍保持良好的自主性能。该方法还包括一种基于物联网领域的廉价远程低功耗通信技术（即LoRa）和定制通信协议与远程操作员交互的方法。结果和可行性分析表明了该方法的可能应用和局限性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08897v1" target="_blank">2402.08897v1</a>
                              </td>
                              <td>RB5 Low-Cost Explorer: Implementing Autonomous Long-Term Exploration on Low-Cost Robotic Hardware</td>
                              <td>Adam Seewald</td>
                              <td>2024-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08897v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08897v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/adamseew/rb5-paper" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/adamseew/rb5" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08846v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Embarrassingly Simple Approach for LLM with Strong ASR Capacity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08846v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08846v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08846v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM). Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM. We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task. To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves the best performance on the Librispeech benchmark among LLM-based ASR models and even outperforms the latest LLM-based audio-universal model trained on massive pair data. Finally, we explore the capability emergence of LLM-based ASR in the process of modal alignment. We hope that our study can facilitate the research on extending LLM with cross-modality capacity and shed light on the LLM-based ASR community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08846v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们专注于解决语音处理领域中最重要的任务之一，即使用语音基础编码器和大型语言模型（LLM）的自动语音识别（ASR）。最近的工作具有复杂的设计，例如为语音编码器在时间上压缩输出，为投影仪处理模态对准，以及为LLM利用参数高效微调。我们发现，精致的设计是不必要的，而现成的语音编码器LLM和唯一可训练的线性投影仪的简单组合却能胜任ASR任务。更具体地说，我们对LLM和语音编码器的各种组合进行了基准测试和探索，从而产生了最佳的基于LLM的ASR系统，我们称之为SLAM-ASR。所提出的SLAM-ASR提供了一个干净的设置和很少的任务特定设计，其中只训练线性投影仪。据我们所知，在基于LLM的ASR模型中，SLAM-ASR在Librispeech基准测试上实现了最佳性能，甚至优于在海量配对数据上训练的最新基于LLM音频通用模型。最后，我们探讨了基于LLM的ASR在模态对准过程中的能力出现。我们希望我们的研究能够促进对具有跨模态能力的LLM扩展的研究，并为基于LLM的ASR社区提供启示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08846v1" target="_blank">2402.08846v1</a>
                              </td>
                              <td>An Embarrassingly Simple Approach for LLM with Strong ASR Capacity</td>
                              <td>Ziyang Ma</td>
                              <td>2024-02-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08846v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08846v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_06950v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Active Metric-Semantic SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_06950v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_06950v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_06950v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this letter, we address the problem of exploration and metric-semantic mapping of multi-floor GPS-denied indoor environments using Size Weight and Power (SWaP) constrained aerial robots. Most previous work in exploration assumes that robot localization is solved. However, neglecting the state uncertainty of the agent can ultimately lead to cascading errors both in the resulting map and in the state of the agent itself. Furthermore, actions that reduce localization errors may be at direct odds with the exploration task. We propose a framework that balances the efficiency of exploration with actions that reduce the state uncertainty of the agent. In particular, our algorithmic approach for active metric-semantic SLAM is built upon sparse information abstracted from raw problem data, to make it suitable for SWaP-constrained robots. Furthermore, we integrate this framework within a fully autonomous aerial robotic system that achieves autonomous exploration in cluttered, 3D environments. From extensive real-world experiments, we showed that by including Semantic Loop Closure (SLC), we can reduce the robot pose estimation errors by over 90% in translation and approximately 75% in yaw, and the uncertainties in pose estimates and semantic maps by over 70% and 65%, respectively. Although discussed in the context of indoor multi-floor exploration, our system can be used for various other applications, such as infrastructure inspection and precision agriculture where reliable GPS data may not be available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_06950v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这封信中，我们讨论了使用尺寸、重量和功率（SWaP）约束的空中机器人对多层GPS拒绝的室内环境进行探索和度量语义映射的问题。以前的大多数探索工作都假设机器人定位已经解决。然而，忽略代理的状态不确定性最终会导致在生成的映射和代理本身的状态中出现级联错误。此外，减少定位误差的动作可能与探索任务直接不一致。我们提出了一个框架，该框架平衡了探索的效率和减少代理状态不确定性的行动。特别是，我们的主动度量语义SLAM算法方法建立在从原始问题数据中提取的稀疏信息的基础上，使其适用于SWaP约束的机器人。此外，我们将该框架集成在一个完全自主的空中机器人系统中，该系统可以在杂乱的3D环境中实现自主探索。从大量的真实世界实验中，我们表明，通过包括语义环闭合（SLC），我们可以将机器人姿态估计误差在平移中减少90%以上，在偏航中减少约75%，将姿态估计和语义图的不确定性分别减少70%和65%以上。尽管在室内多层勘探的背景下进行了讨论，但我们的系统可用于各种其他应用，如基础设施检查和精密农业，在这些应用中可能无法获得可靠的GPS数据。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.06950v3" target="_blank">2309.06950v3</a>
                              </td>
                              <td>3D Active Metric-Semantic SLAM</td>
                              <td>Yuezhan Tao</td>
                              <td>2023-09-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_06950v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.06950v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08125v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Customizable Perturbation Synthesis for Robust SLAM Benchmarking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08125v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08125v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08125v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Robustness is a crucial factor for the successful deployment of robots in unstructured environments, particularly in the domain of Simultaneous Localization and Mapping (SLAM). Simulation-based benchmarks have emerged as a highly scalable approach for robustness evaluation compared to real-world data collection. However, crafting a challenging and controllable noisy world with diverse perturbations remains relatively under-explored. To this end, we propose a novel, customizable pipeline for noisy data synthesis, aimed at assessing the resilience of multi-modal SLAM models against various perturbations. This pipeline incorporates customizable hardware setups, software components, and perturbed environments. In particular, we introduce comprehensive perturbation taxonomy along with a perturbation composition toolbox, allowing the transformation of clean simulations into challenging noisy environments. Utilizing the pipeline, we instantiate the Robust-SLAM benchmark, which includes diverse perturbation types, to evaluate the risk tolerance of existing advanced multi-modal SLAM models. Our extensive analysis uncovers the susceptibilities of existing SLAM models to real-world disturbance, despite their demonstrated accuracy in standard benchmarks. Our perturbation synthesis toolbox, SLAM robustness evaluation pipeline, and Robust-SLAM benchmark will be made publicly available at https://github.com/Xiaohao-Xu/SLAM-under-Perturbation/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08125v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>鲁棒性是机器人在非结构化环境中成功部署的关键因素，特别是在同步定位和映射（SLAM）领域。与真实世界的数据收集相比，基于仿真的基准测试已成为一种高度可扩展的稳健性评估方法。然而，打造一个具有挑战性和可控性的、具有各种扰动的嘈杂世界的探索相对较少。为此，我们提出了一种新的、可定制的用于噪声数据合成的管道，旨在评估多模态SLAM模型对各种扰动的弹性。该管道包含可定制的硬件设置、软件组件和扰动环境。特别是，我们引入了全面的扰动分类法和扰动合成工具箱，允许将干净的模拟转换为具有挑战性的噪声环境。利用流水线，我们实例化了包括不同扰动类型的鲁棒SLAM基准，以评估现有先进的多模态SLAM模型的风险容限。我们的广泛分析揭示了现有SLAM模型对现实世界扰动的敏感性，尽管它们在标准基准中已经证明了准确性。我们的扰动综合工具箱、SLAM稳健性评估管道和稳健SLAM基准将在https://github.com/Xiaohao-Xu/SLAM-under-Perturbation/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08125v1" target="_blank">2402.08125v1</a>
                              </td>
                              <td>Customizable Perturbation Synthesis for Robust SLAM Benchmarking</td>
                              <td>Xiaohao Xu</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08125v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08125v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/xiaohao-xu/slam-under-perturbation" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07537v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UAV-assisted Visual SLAM Generating Reconstructed 3D Scene Graphs in GPS-denied Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07537v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07537v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07537v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Aerial robots play a vital role in various applications where the situational awareness of the robots concerning the environment is a fundamental demand. As one such use case, drones in GPS-denied environments require equipping with different sensors (e.g., vision sensors) that provide reliable sensing results while performing pose estimation and localization. In this paper, reconstructing the maps of indoor environments alongside generating 3D scene graphs for a high-level representation using a camera mounted on a drone is targeted. Accordingly, an aerial robot equipped with a companion computer and an RGB-D camera was built and employed to be appropriately integrated with a Visual Simultaneous Localization and Mapping (VSLAM) framework proposed by the authors. To enhance the situational awareness of the robot while reconstructing maps, various structural elements, including doors and walls, were labeled with printed fiducial markers, and a dictionary of the topological relations among them was fed to the system. The VSLAM system detects markers and reconstructs the map of the indoor areas enriched with higher-level semantic entities, including corridors and rooms. Another achievement is generating multi-layered vision-based situational graphs containing enhanced hierarchical representations of the indoor environment. In this regard, integrating VSLAM into the employed drone is the primary target of this paper to provide an end-to-end robot application for GPS-denied environments. To show the practicality of the system, various real-world condition experiments have been conducted in indoor scenarios with dissimilar structural layouts. Evaluations show the proposed drone application can perform adequately w.r.t. the ground-truth data and its baseline.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07537v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>空中机器人在各种应用中发挥着至关重要的作用，其中机器人对环境的态势感知是其基本需求。作为一种这样的用例，在拒绝GPS的环境中，无人机需要配备不同的传感器（例如，视觉传感器），这些传感器在执行姿态估计和定位时提供可靠的传感结果。在本文中，目标是使用安装在无人机上的相机重建室内环境的地图，同时生成用于高级表示的3D场景图。因此，构建了一个配备有配套计算机和RGB-D相机的空中机器人，并将其与作者提出的视觉同步定位和映射（VSLAM）框架适当集成。为了在重建地图时增强机器人的态势感知，用打印的基准标记标记包括门和墙在内的各种结构元素，并将它们之间的拓扑关系字典提供给系统。VSLAM系统检测标记并重建富含高级语义实体的室内区域的地图，包括走廊和房间。另一项成就是生成多层基于视觉的情景图，其中包含室内环境的增强层次表示。在这方面，将VSLAM集成到所使用的无人机中是本文的主要目标，以提供用于GPS拒绝环境的端到端机器人应用程序。为了证明该系统的实用性，在不同结构布局的室内场景中进行了各种真实世界条件的实验。评估表明，所提出的无人机应用程序可以充分利用地面实况数据及其基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07537v1" target="_blank">2402.07537v1</a>
                              </td>
                              <td>UAV-assisted Visual SLAM Generating Reconstructed 3D Scene Graphs in GPS-denied Environments</td>
                              <td>Ahmed Radwan</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07537v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07537v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_06951v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Semantic Object-level Modeling for Robust Visual Camera Relocalization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_06951v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_06951v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_06951v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual relocalization is crucial for autonomous visual localization and navigation of mobile robotics. Due to the improvement of CNN-based object detection algorithm, the robustness of visual relocalization is greatly enhanced especially in viewpoints where classical methods fail. However, ellipsoids (quadrics) generated by axis-aligned object detection may limit the accuracy of the object-level representation and degenerate the performance of visual relocalization system. In this paper, we propose a novel method of automatic object-level voxel modeling for accurate ellipsoidal representations of objects. As for visual relocalization, we design a better pose optimization strategy for camera pose recovery, to fully utilize the projection characteristics of 2D fitted ellipses and the 3D accurate ellipsoids. All of these modules are entirely intergrated into visual SLAM system. Experimental results show that our semantic object-level mapping and object-based visual relocalization methods significantly enhance the performance of visual relocalization in terms of robustness to new viewpoints.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_06951v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉重定位是移动机器人自主视觉定位和导航的关键。由于对基于CNN的目标检测算法的改进，视觉重定位的鲁棒性大大增强，尤其是在经典方法失败的情况下。然而，由轴对准对象检测产生的椭球体（二次曲面）可能会限制对象级表示的准确性，并降低视觉重定位系统的性能。在本文中，我们提出了一种新的自动对象级体素建模方法，用于对象的精确椭球表示。在视觉重定位方面，我们设计了一种更好的相机姿态恢复姿态优化策略，以充分利用二维拟合椭圆和三维精确椭圆的投影特性。所有这些模块完全集成到可视化SLAM系统中。实验结果表明，我们的语义对象级映射和基于对象的视觉重定位方法在对新视点的鲁棒性方面显著提高了视觉重定位的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.06951v1" target="_blank">2402.06951v1</a>
                              </td>
                              <td>Semantic Object-level Modeling for Robust Visual Camera Relocalization</td>
                              <td>Yifan Zhu</td>
                              <td>2024-02-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_06951v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.06951v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_04745v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SLAM for Visually Impaired People: a Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_04745v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_04745v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_04745v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent decades, several assistive technologies have been developed to improve the ability of blind and visually impaired individuals to navigate independently and safely. At the same time, simultaneous localization and mapping (SLAM) techniques have become sufficiently robust and efficient to be adopted in developing these assistive technologies. We present the first systematic literature review of 54 recent studies on SLAM-based solutions for blind and visually impaired people, focusing on literature published from 2017 onward. This review explores various localization and mapping techniques employed in this context. We discuss the advantages and limitations of these techniques for blind and visually impaired navigation. Moreover, we examine the major challenges described across studies. We explain how SLAM technology offers the potential to improve the ability of visually impaired individuals to navigate effectively. Finally, we present future opportunities and challenges in this domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_04745v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近几十年来，已经开发了几种辅助技术来提高盲人和视障人士独立安全导航的能力。与此同时，同时定位和映射（SLAM）技术已经变得足够强大和高效，可以用于开发这些辅助技术。我们对最近54项针对盲人和视障人士的基于SLAM的解决方案的研究进行了首次系统的文献综述，重点是2017年以后发表的文献。这篇综述探讨了在这种情况下使用的各种定位和映射技术。我们讨论了这些技术在盲人和视障导航中的优势和局限性。此外，我们还研究了研究中描述的主要挑战。我们解释了SLAM技术如何提供提高视障人士有效导航能力的潜力。最后，我们介绍了这一领域未来的机遇和挑战。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.04745v3" target="_blank">2212.04745v3</a>
                              </td>
                              <td>SLAM for Visually Impaired People: a Survey</td>
                              <td>Marziyeh Bamdad</td>
                              <td>2022-12-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_04745v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.04745v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_06131v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PAS-SLAM: A Visual SLAM System for Planar Ambiguous Scenes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_06131v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_06131v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_06131v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual SLAM (Simultaneous Localization and Mapping) based on planar features has found widespread applications in fields such as environmental structure perception and augmented reality. However, current research faces challenges in accurately localizing and mapping in planar ambiguous scenes, primarily due to the poor accuracy of the employed planar features and data association methods. In this paper, we propose a visual SLAM system based on planar features designed for planar ambiguous scenes, encompassing planar processing, data association, and multi-constraint factor graph optimization. We introduce a planar processing strategy that integrates semantic information with planar features, extracting the edges and vertices of planes to be utilized in tasks such as plane selection, data association, and pose optimization. Next, we present an integrated data association strategy that combines plane parameters, semantic information, projection IoU (Intersection over Union), and non-parametric tests, achieving accurate and robust plane data association in planar ambiguous scenes. Finally, we design a set of multi-constraint factor graphs for camera pose optimization. Qualitative and quantitative experiments conducted on publicly available datasets demonstrate that our proposed system competes effectively in both accuracy and robustness in terms of map construction and camera localization compared to state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_06131v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于平面特征的视觉SLAM在环境结构感知和增强现实等领域得到了广泛的应用。然而，当前的研究在平面模糊场景中准确定位和映射方面面临挑战，主要是由于所使用的平面特征和数据关联方法的准确性较差。在本文中，我们提出了一个基于平面特征的视觉SLAM系统，该系统专为平面模糊场景设计，包括平面处理、数据关联和多约束因子图优化。我们介绍了一种平面处理策略，该策略将语义信息与平面特征相结合，提取平面的边和顶点，用于平面选择、数据关联和姿态优化等任务。接下来，我们提出了一种集成的数据关联策略，该策略结合了平面参数、语义信息、投影IoU（并集上的交集）和非参数测试，在平面模糊场景中实现了准确而稳健的平面数据关联。最后，我们设计了一组用于相机姿态优化的多约束因子图。在公开可用的数据集上进行的定性和定量实验表明，与最先进的方法相比，我们提出的系统在地图构建和相机定位方面的准确性和稳健性方面具有有效的竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.06131v1" target="_blank">2402.06131v1</a>
                              </td>
                              <td>PAS-SLAM: A Visual SLAM System for Planar Ambiguous Scenes</td>
                              <td>Xinggang Hu</td>
                              <td>2024-02-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_06131v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.06131v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03762v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03762v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03762v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03762v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural implicit representations have recently been demonstrated in many fields including Simultaneous Localization And Mapping (SLAM). Current neural SLAM can achieve ideal results in reconstructing bounded scenes, but this relies on the input of RGB-D images. Neural-based SLAM based only on RGB images is unable to reconstruct the scale of the scene accurately, and it also suffers from scale drift due to errors accumulated during tracking. To overcome these limitations, we present MoD-SLAM, a monocular dense mapping method that allows global pose optimization and 3D reconstruction in real-time in unbounded scenes. Optimizing scene reconstruction by monocular depth estimation and using loop closure detection to update camera pose enable detailed and precise reconstruction on large scenes. Compared to previous work, our approach is more robust, scalable and versatile. Our experiments demonstrate that MoD-SLAM has more excellent mapping performance than prior neural SLAM methods, especially in large borderless scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03762v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经隐式表示最近已经在许多领域得到了证明，包括同时定位和映射（SLAM）。目前的神经SLAM在重建有界场景时可以获得理想的结果，但这依赖于RGB-D图像的输入。仅基于RGB图像的基于神经的SLAM无法准确地重建场景的尺度，而且由于跟踪过程中积累的误差，它还存在尺度漂移。为了克服这些限制，我们提出了MoD SLAM，这是一种单目密集映射方法，允许在无界场景中实时进行全局姿态优化和3D重建。通过单目深度估计优化场景重建，并使用闭环检测更新相机姿态，可以在大型场景上进行详细而精确的重建。与以前的工作相比，我们的方法更加健壮、可扩展和通用。我们的实验表明，与现有的神经SLAM方法相比，MoD-SLAM具有更出色的映射性能，尤其是在大型无边界场景中。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03762v2" target="_blank">2402.03762v2</a>
                              </td>
                              <td>MoD-SLAM: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction</td>
                              <td>Heng Zhou</td>
                              <td>2024-02-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03762v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03762v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_14063v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Preferential Multi-Target Search in Indoor Environments using Semantic SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_14063v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_14063v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_14063v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, the demand for service robots capable of executing tasks beyond autonomous navigation has grown. In the future, service robots will be expected to perform complex tasks like 'Set table for dinner'. High-level tasks like these, require, among other capabilities, the ability to retrieve multiple targets. This paper delves into the challenge of locating multiple targets in an environment, termed 'Find my Objects.' We present a novel heuristic designed to facilitate robots in conducting a preferential search for multiple targets in indoor spaces. Our approach involves a Semantic SLAM framework that combines semantic object recognition with geometric data to generate a multi-layered map. We fuse the semantic maps with probabilistic priors for efficient inferencing. Recognizing the challenges introduced by obstacles that might obscure a navigation goal and render standard point-to-point navigation strategies less viable, our methodology offers resilience to such factors. Importantly, our method is adaptable to various object detectors, RGB-D SLAM techniques, and local navigation planners. We demonstrate the 'Find my Objects' task in real-world indoor environments, yielding quantitative results that attest to the effectiveness of our methodology. This strategy can be applied in scenarios where service robots need to locate, grasp, and transport objects, taking into account user preferences. For a brief summary, please refer to our video: https://tinyurl.com/PrefTargetSearch</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_14063v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，对能够执行自主导航以外任务的服务机器人的需求不断增长。未来，服务机器人将被期望执行复杂的任务，如“为晚餐设置餐桌”。像这样的高级任务，除其他功能外，还需要检索多个目标的能力。本文深入探讨了在一个环境中定位多个目标的挑战，称为“查找我的对象”我们提出了一种新的启发式算法，旨在帮助机器人在室内空间中优先搜索多个目标。我们的方法涉及语义SLAM框架，该框架将语义对象识别与几何数据相结合，以生成多层地图。我们将语义图与概率先验相融合，以实现高效推理。认识到障碍物带来的挑战，这些障碍物可能会模糊导航目标并使标准点对点导航策略变得不太可行，我们的方法提供了对这些因素的弹性。重要的是，我们的方法适用于各种物体探测器、RGB-D SLAM技术和本地导航规划者。我们在真实的室内环境中演示了“查找我的对象”任务，产生了定量结果，证明了我们方法的有效性。该策略可应用于服务机器人需要定位、抓取和运输物体的场景，同时考虑用户偏好。有关简要摘要，请参阅我们的视频：https://tinyurl.com/PrefTargetSearch</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.14063v3" target="_blank">2309.14063v3</a>
                              </td>
                              <td>Preferential Multi-Target Search in Indoor Environments using Semantic SLAM</td>
                              <td>Akash Chikhalikar</td>
                              <td>2023-09-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_14063v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.14063v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_05254v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Online and Certifiably Correct Visual Odometry and Mapping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_05254v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_05254v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_05254v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes two new algorithms for certified perception in safety-critical robotic applications. The first is a Certified Visual Odometry algorithm, which uses a RGBD camera with bounded sensor noise to construct a visual odometry estimate with provable error bounds. The second is a Certified Mapping algorithm which, using the same RGBD images, constructs a Signed Distance Field of the obstacle environment, always safely underestimating the distance to the nearest obstacle. This is required to avoid errors due to VO drift. The algorithms are demonstrated in hardware experiments, where we demonstrate both running online at 30FPS. The methods are also compared to state-of-the-art techniques for odometry and mapping.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_05254v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了两种新的安全关键机器人应用认证感知算法。第一种是认证视觉里程计算法，该算法使用具有有界传感器噪声的RGBD相机来构建具有可证明误差边界的视觉里程计估计。第二种是认证映射算法，该算法使用相同的RGBD图像，构建障碍物环境的符号距离场，总是安全地低估到最近障碍物的距离。这是为了避免由于VO漂移而导致的错误。这些算法在硬件实验中进行了演示，我们演示了这两种算法都以30FPS的速度在线运行。还将这些方法与最先进的里程计和地图绘制技术进行了比较。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.05254v1" target="_blank">2402.05254v1</a>
                              </td>
                              <td>Online and Certifiably Correct Visual Odometry and Mapping</td>
                              <td>Devansh R Agrawal</td>
                              <td>2024-02-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_05254v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.05254v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_05003v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Efficient Invariant Kalman Filter for Inertial-based Odometry with Large-sample Environmental Measurements</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_05003v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_05003v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_05003v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A filter for inertial-based odometry is a recursive method used to estimate the pose from measurements of ego-motion and relative pose. Currently, there is no known filter that guarantees the computation of a globally optimal solution for the non-linear measurement model. In this paper, we demonstrate that an innovative filter, with the state being $SE_2(3)$ and the $\sqrt{n}$-\textit{consistent} pose as the initialization, efficiently achieves \textit{asymptotic optimality} in terms of minimum mean square error. This approach is tailored for real-time SLAM and inertial-based odometry applications.   Our first contribution is that we propose an iterative filtering method based on the Gauss-Newton method on Lie groups which is numerically to solve the estimation of states from a priori and non-linear measurements. The filtering stands out due to its iterative mechanism and adaptive initialization. Second, when dealing with environmental measurements of the surroundings, we utilize a $\sqrt{n}$-consistent pose as the initial value for the update step in a single iteration. The solution is closed in form and has computational complexity $O(n)$. Third, we theoretically show that the approach can achieve asymptotic optimality in the sense of minimum mean square error from the a priori and virtual relative pose measurements (see Problem~\ref{prob:new update problem}). Finally, to validate our method, we carry out extensive numerical and experimental evaluations. Our results consistently demonstrate that our approach outperforms other state-of-the-art filter-based methods, including the iterated extended Kalman filter and the invariant extended Kalman filter, in terms of accuracy and running time.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_05003v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于惯性的里程计滤波器是一种递归方法，用于根据自我运动和相对姿态的测量来估计姿态。目前，没有已知的滤波器来保证非线性测量模型的全局最优解的计算。在本文中，我们证明了一个创新的滤波器，以状态为$SE_2（3）$和$\sqrt｛n｝$-\textit｛consistent｝姿态作为初始化，有效地实现了最小均方误差的\textit｛渐近最优性｝。这种方法是为实时SLAM和基于惯性的里程计应用量身定制的。我们的第一个贡献是，我们提出了一种基于李群上的高斯-牛顿方法的迭代滤波方法，该方法在数值上解决了先验和非线性测量的状态估计。滤波因其迭代机制和自适应初始化而脱颖而出。其次，当处理周围环境的环境测量时，我们使用$\sqrt｛n｝$一致姿态作为单个迭代中更新步骤的初始值。该解在形式上是封闭的，并且具有计算复杂度$O（n）$。第三，我们从理论上证明了该方法可以从先验和虚拟相对姿态测量中获得最小均方误差意义上的渐近最优性（见问题~\ref{prob:新更新问题}）。最后，为了验证我们的方法，我们进行了广泛的数值和实验评估。我们的结果一致表明，在精度和运行时间方面，我们的方法优于其他最先进的基于滤波器的方法，包括迭代扩展卡尔曼滤波器和不变扩展卡尔曼滤波器。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.05003v1" target="_blank">2402.05003v1</a>
                              </td>
                              <td>Efficient Invariant Kalman Filter for Inertial-based Odometry with Large-sample Environmental Measurements</td>
                              <td>Xinghan Li</td>
                              <td>2024-02-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_05003v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.05003v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_14641v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adaptive Denoising-Enhanced LiDAR Odometry for Degeneration Resilience in Diverse Terrains</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_14641v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_14641v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_14641v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The flexibility of Simultaneous Localization and Mapping (SLAM) algorithms in various environments has consistently been a significant challenge. To address the issue of LiDAR odometry drift in high-noise settings, integrating clustering methods to filter out unstable features has become an effective module of SLAM frameworks. However, reducing the amount of point cloud data can lead to potential loss of information and possible degeneration. As a result, this research proposes a LiDAR odometry that can dynamically assess the point cloud's reliability. The algorithm aims to improve adaptability in diverse settings by selecting important feature points with sensitivity to the level of environmental degeneration. Firstly, a fast adaptive Euclidean clustering algorithm based on range image is proposed, which, combined with depth clustering, extracts the primary structural points of the environment defined as ambient skeleton points. Then, the environmental degeneration level is computed through the dense normal features of the skeleton points, and the point cloud cleaning is dynamically adjusted accordingly. The algorithm is validated on the KITTI benchmark and real environments, demonstrating higher accuracy and robustness in different environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_14641v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）算法在各种环境中的灵活性一直是一个重大挑战。为了解决高噪声环境下激光雷达测距漂移的问题，集成聚类方法来过滤不稳定的特征已成为SLAM框架的一个有效模块。然而，减少点云数据量可能会导致潜在的信息丢失和可能的退化。因此，本研究提出了一种可以动态评估点云可靠性的激光雷达里程计。该算法旨在通过选择对环境退化程度敏感的重要特征点来提高在不同设置中的适应性。首先，提出了一种基于距离图像的快速自适应欧氏聚类算法，该算法结合深度聚类，提取环境中定义为环境骨架点的主要结构点。然后，通过骨架点的密集法线特征计算环境退化程度，并相应地动态调整点云清理。该算法在KITTI基准测试和实际环境中进行了验证，在不同的环境中表现出更高的准确性和鲁棒性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.14641v2" target="_blank">2309.14641v2</a>
                              </td>
                              <td>Adaptive Denoising-Enhanced LiDAR Odometry for Degeneration Resilience in Diverse Terrains</td>
                              <td>Mazeyu Ji</td>
                              <td>2023-09-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_14641v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.14641v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17907v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and Visual-inertial Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17907v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17907v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17907v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents SubPipe, an underwater dataset for SLAM, object detection, and image segmentation. SubPipe has been recorded using a \gls{LAUV}, operated by OceanScan MST, and carrying a sensor suite including two cameras, a side-scan sonar, and an inertial navigation system, among other sensors. The AUV has been deployed in a pipeline inspection environment with a submarine pipe partially covered by sand. The AUV's pose ground truth is estimated from the navigation sensors. The side-scan sonar and RGB images include object detection and segmentation annotations, respectively. State-of-the-art segmentation, object detection, and SLAM methods are benchmarked on SubPipe to demonstrate the dataset's challenges and opportunities for leveraging computer vision algorithms. To the authors' knowledge, this is the first annotated underwater dataset providing a real pipeline inspection scenario. The dataset and experiments are publicly available online at https://github.com/remaro-network/SubPipe-dataset</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17907v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了用于SLAM、目标检测和图像分割的水下数据集SubPipe。SubPipe是使用由OceanScan MST操作的LAUV进行记录的，并携带一个传感器套件，包括两个摄像头、一个侧扫声纳和一个惯性导航系统以及其他传感器。AUV已经部署在管道检查环境中，海底管道部分被沙子覆盖。AUV的姿态地面实况是由导航传感器估计的。侧扫声纳和RGB图像分别包括物体检测和分割注释。在SubPipe上对最先进的分割、对象检测和SLAM方法进行了基准测试，以展示数据集在利用计算机视觉算法方面的挑战和机遇。据作者所知，这是第一个提供真实管道检查场景的注释水下数据集。该数据集和实验可在线公开获取，网址为https://github.com/remaro-network/SubPipe-dataset</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17907v2" target="_blank">2401.17907v2</a>
                              </td>
                              <td>SubPipe: A Submarine Pipeline Inspection Dataset for Segmentation and Visual-inertial Localization</td>
                              <td>Olaya Álvarez-Tuñón</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17907v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17907v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/remaro-network/subpipe-dataset" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03989v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">YOLOPoint Joint Keypoint and Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03989v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03989v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03989v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Intelligent vehicles of the future must be capable of understanding and navigating safely through their surroundings. Camera-based vehicle systems can use keypoints as well as objects as low- and high-level landmarks for GNSS-independent SLAM and visual odometry. To this end we propose YOLOPoint, a convolutional neural network model that simultaneously detects keypoints and objects in an image by combining YOLOv5 and SuperPoint to create a single forward-pass network that is both real-time capable and accurate. By using a shared backbone and a light-weight network structure, YOLOPoint is able to perform competitively on both the HPatches and KITTI benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03989v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>未来的智能汽车必须能够理解周围环境并安全导航。基于摄像头的车辆系统可以使用关键点以及物体作为低级别和高级别地标，用于GNSS独立SLAM和视觉里程计。为此，我们提出了YOLOPoint，这是一种卷积神经网络模型，通过将YOLOv5和SuperPoint相结合，同时检测图像中的关键点和对象，以创建一个具有实时性和准确性的单前向网络。通过使用共享骨干网和轻量级网络结构，YOLOPoint能够在HPatches和KITTI基准测试上都有竞争力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03989v1" target="_blank">2402.03989v1</a>
                              </td>
                              <td>YOLOPoint Joint Keypoint and Object Detection</td>
                              <td>Anton Backhaus</td>
                              <td>2024-02-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03989v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03989v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/unibwtas/yolopoint" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17826v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PALoc: Advancing SLAM Benchmarking with Prior-Assisted 6-DoF Trajectory Generation and Uncertainty Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17826v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17826v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17826v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accurately generating ground truth (GT) trajectories is essential for Simultaneous Localization and Mapping (SLAM) evaluation, particularly under varying environmental conditions. This study introduces a systematic approach employing a prior map-assisted framework for generating dense six-degree-of-freedom (6-DoF) GT poses for the first time, enhancing the fidelity of both indoor and outdoor SLAM datasets. Our method excels in handling degenerate and stationary conditions frequently encountered in SLAM datasets, thereby increasing robustness and precision. A significant aspect of our approach is the detailed derivation of covariances within the factor graph, enabling an in-depth analysis of pose uncertainty propagation. This analysis crucially contributes to demonstrating specific pose uncertainties and enhancing trajectory reliability from both theoretical and empirical perspectives. Additionally, we provide an open-source toolbox (https://github.com/JokerJohn/Cloud_Map_Evaluation) for map evaluation criteria, facilitating the indirect assessment of overall trajectory precision. Experimental results show at least a 30\% improvement in map accuracy and a 20\% increase in direct trajectory accuracy compared to the Iterative Closest Point (ICP) \cite{sharp2002icp} algorithm across diverse campus environments, with substantially enhanced robustness. Our open-source solution (https://github.com/JokerJohn/PALoc), extensively applied in the FusionPortable\cite{Jiao2022Mar} dataset, is geared towards SLAM benchmark dataset augmentation and represents a significant advancement in SLAM evaluations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17826v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>准确生成地面实况（GT）轨迹对于同时定位和测绘（SLAM）评估至关重要，尤其是在不同的环境条件下。本研究首次引入了一种系统方法，采用先验地图辅助框架生成密集的六自由度（6-DoF）GT姿态，提高了室内和室外SLAM数据集的保真度。我们的方法擅长处理SLAM数据集中经常遇到的退化和平稳条件，从而提高了鲁棒性和精度。我们方法的一个重要方面是在因子图中详细推导协变量，从而能够深入分析姿态不确定性的传播。这一分析从理论和实证角度对证明特定姿态的不确定性和提高轨迹可靠性至关重要。此外，我们还提供了一个开源工具箱(https://github.com/JokerJohn/Cloud_Map_Evaluation)用于地图评估标准，便于间接评估总体轨迹精度。实验结果表明，在不同的校园环境中，与迭代最近点算法相比，地图精度至少提高了30%，直接轨迹精度提高了20%，鲁棒性显著增强。我们的开源解决方案(https://github.com/JokerJohn/PALoc)，广泛应用于FusionPortable\cite{Jia2022Mar}数据集，旨在增强SLAM基准数据集，代表着SLAM评估的重大进步。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17826v2" target="_blank">2401.17826v2</a>
                              </td>
                              <td>PALoc: Advancing SLAM Benchmarking with Prior-Assisted 6-DoF Trajectory Generation and Uncertainty Estimation</td>
                              <td>Xiangcheng Hu</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17826v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17826v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/jokerjohn/paloc" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03246v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03246v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03246v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03246v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaussian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rendering ability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03246v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义理解在密集同时定位和映射（SLAM）中起着至关重要的作用，有助于全面的场景解释。将高斯散射集成到SLAM系统中的最新进展已经证明了其通过使用显式3D高斯表示生成高质量渲染的有效性。在这一进展的基础上，我们提出了SGS-SLAM，这是第一个基于3D高斯的语义密集视觉SLAM系统，它提供了精确的3D语义分割和高保真度重建。具体而言，我们建议在映射过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以提高重建质量。大量实验表明，SGS-SLAM在相机姿态估计、地图重建和语义分割方面提供了最先进的性能，优于现有方法，同时保持了实时渲染能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03246v1" target="_blank">2402.03246v1</a>
                              </td>
                              <td>SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM</td>
                              <td>Mingrui Li</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03246v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03246v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03376v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weighted Conformal LiDAR-Mapping for Structured SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03376v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03376v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03376v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>One of the main challenges in simultaneous localization and mapping (SLAM) is real-time processing. High-computational loads linked to data acquisition and processing complicate this task. This article presents an efficient feature extraction approach for mapping structured environments. The proposed methodology, weighted conformal LiDAR-mapping (WCLM), is based on the extraction of polygonal profiles and propagation of uncertainties from raw measurement data. This is achieved using conformal M bius transformation. The algorithm has been validated experimentally using 2-D data obtained from a low-cost Light Detection and Ranging (LiDAR) range finder. The results obtained suggest that computational efficiency is significantly improved with reference to other state-of-the-art SLAM approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03376v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）的主要挑战之一是实时处理。与数据采集和处理相关的高计算负载使这项任务复杂化。本文提出了一种用于映射结构化环境的高效特征提取方法。所提出的方法，加权共形激光雷达映射（WCLM），是基于从原始测量数据中提取多边形轮廓和传播不确定性。这是使用保角M bius变换实现的。该算法已经使用从低成本的光探测和测距（LiDAR）测距仪获得的二维数据进行了实验验证。所获得的结果表明，与其他最先进的SLAM方法相比，计算效率显著提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03376v1" target="_blank">2402.03376v1</a>
                              </td>
                              <td>Weighted Conformal LiDAR-Mapping for Structured SLAM</td>
                              <td>Natalia Prieto-Fernández</td>
                              <td>2024-02-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03376v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03376v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02020v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuV-SLAM: Fast Neural Multiresolution Voxel Optimization for RGBD Dense SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02020v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02020v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02020v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce NeuV-SLAM, a novel dense simultaneous localization and mapping pipeline based on neural multiresolution voxels, characterized by ultra-fast convergence and incremental expansion capabilities. This pipeline utilizes RGBD images as input to construct multiresolution neural voxels, achieving rapid convergence while maintaining robust incremental scene reconstruction and camera tracking. Central to our methodology is to propose a novel implicit representation, termed VDF that combines the implementation of neural signed distance field (SDF) voxels with an SDF activation strategy. This approach entails the direct optimization of color features and SDF values anchored within the voxels, substantially enhancing the rate of scene convergence. To ensure the acquisition of clear edge delineation, SDF activation is designed, which maintains exemplary scene representation fidelity even under constraints of voxel resolution. Furthermore, in pursuit of advancing rapid incremental expansion with low computational overhead, we developed hashMV, a novel hash-based multiresolution voxel management structure. This architecture is complemented by a strategically designed voxel generation technique that synergizes with a two-dimensional scene prior. Our empirical evaluations, conducted on the Replica and ScanNet Datasets, substantiate NeuV-SLAM's exceptional efficacy in terms of convergence speed, tracking accuracy, scene reconstruction, and rendering quality.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02020v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了NeuV SLAM，这是一种基于神经多分辨率体素的新型密集同时定位和映射管道，具有超快收敛和增量扩展能力。该流水线利用RGBD图像作为输入来构建多分辨率神经体素，实现快速收敛，同时保持稳健的增量场景重建和相机跟踪。我们方法的核心是提出一种新的隐式表示，称为VDF，它将神经符号距离场（SDF）体素的实现与SDF激活策略相结合。这种方法需要对锚定在体素内的颜色特征和SDF值进行直接优化，从而显著提高场景收敛速度。为了确保获得清晰的边缘描绘，设计了SDF激活，即使在体素分辨率的约束下，它也能保持示例场景表示的保真度。此外，为了追求以低计算开销推进快速增量扩展，我们开发了hashMV，这是一种新的基于哈希的多分辨率体素管理结构。该架构由战略性设计的体素生成技术补充，该技术与二维场景先验协同。我们在Replica和ScanNet数据集上进行的经验评估证实了NeuV SLAM在收敛速度、跟踪精度、场景重建和渲染质量方面的卓越功效。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02020v1" target="_blank">2402.02020v1</a>
                              </td>
                              <td>NeuV-SLAM: Fast Neural Multiresolution Voxel Optimization for RGBD Dense SLAM</td>
                              <td>Wenzhi Guo</td>
                              <td>2024-02-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02020v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02020v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01967v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Collaborative Active SLAM: Synchronous and Asynchronous Coordination Among Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01967v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01967v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01967v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In autonomous robotics, a critical challenge lies in developing robust solutions for Active Collaborative SLAM, wherein multiple robots collaboratively explore and map an unknown environment while intelligently coordinating their movements and sensor data acquisitions. In this article, we present two approaches for coordinating a system consisting of multiple robots to perform Active Collaborative SLAM (AC-SLAM) for environmental exploration. Our two coordination approaches, synchronous and asynchronous implement a methodology to prioritize robot goal assignments by the central server. We also present a method to efficiently spread the robots for maximum exploration while keeping SLAM uncertainty low. Both coordination approaches were evaluated through simulation and experiments on publicly available datasets, rendering promising results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01967v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在自主机器人中，一个关键的挑战在于为主动协作SLAM开发强大的解决方案，其中多个机器人协同探索和绘制未知环境的地图，同时智能地协调它们的运动和传感器数据采集。在本文中，我们提出了两种方法来协调由多个机器人组成的系统，以执行用于环境勘探的主动协作SLAM（AC-SLAM）。我们的两种协调方法，同步和异步，实现了一种由中央服务器对机器人目标分配进行优先级排序的方法。我们还提出了一种方法，在保持低SLAM不确定性的同时，有效地分散机器人进行最大限度的探索。通过在公开数据集上进行模拟和实验，对这两种协调方法进行了评估，得出了有希望的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01967v2" target="_blank">2310.01967v2</a>
                              </td>
                              <td>Collaborative Active SLAM: Synchronous and Asynchronous Coordination Among Agents</td>
                              <td>Matteo Maragliano</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01967v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01967v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_00588v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BrainSLAM: SLAM on Neural Population Activity Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_00588v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_00588v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_00588v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments. Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data. We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex. This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors). Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal's location. This is the first demonstration of inference of a spatial map from brain recordings. Our findings expand SLAM to a new modality, enabling a new method of mapping environments and facilitating a better understanding of the role of cognitive maps in navigation and decision making.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_00588v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>同时定位和映射（SLAM）算法通常用于机器人系统中，用于学习新环境的地图。大脑似乎也会学习地图，但其机制尚不清楚，也不清楚如何从神经活动数据中推断出这些地图。我们推出BrainSLAM；一种仅使用从大鼠的三个大脑区域（海马体、前额叶皮层和顶叶皮层）同时记录的群体活动（局部场电位，LFP）数据进行SLAM的方法。该系统使用卷积神经网络（CNN）从大鼠在2D迷宫中导航时记录的神经局部场电位数据的小波标度图中解码速度和熟悉度信息。CNN的输出驱动了一个受RatSLAM启发的架构，为执行路径集成的吸引器网络和执行“环路闭合”（检测先前访问的位置并校正地图混叠错误）的独立系统提供动力。这三个组成部分加在一起可以构建环境的忠实表示，同时跟踪动物的位置。这是从大脑记录中推断空间图的首次演示。我们的发现将SLAM扩展到了一种新的模式，实现了一种绘制环境的新方法，并有助于更好地理解认知地图在导航和决策中的作用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.00588v1" target="_blank">2402.00588v1</a>
                              </td>
                              <td>BrainSLAM: SLAM on Neural Population Activity Data</td>
                              <td>Kipp Freud</td>
                              <td>2024-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_00588v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.00588v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_00438v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The GREENBOT dataset: Multimodal mobile robotic dataset for a typical Mediterranean greenhouse</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_00438v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_00438v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_00438v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces an innovative dataset specifically crafted for challenging agricultural settings (a greenhouse), where achieving precise localization is of paramount importance. The dataset was gathered using a mobile platform equipped with a set of sensors typically used in mobile robots, as it was moved through all the corridors of a typical Mediterranean greenhouse featuring tomato crop. This dataset presents a unique opportunity for constructing detailed 3D models of plants in such indoor-like space, with potential applications such as robotized spraying. For the first time to the best knowledge of authors, a dataset suitable to put at test Simultaneous Localization and Mapping (SLAM) methods is presented in a greenhouse environment, which poses unique challenges. The suitability of the dataset for such goal is assessed by presenting SLAM results with state-of-the-art algorithms. The dataset is available online in \url{https://arm.ual.es/arm-group/dataset-greenhouse-2024/}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_00438v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一个专门为具有挑战性的农业环境（温室）设计的创新数据集，在那里实现精确定位至关重要。数据集是使用一个移动平台收集的，该平台配备了一组移动机器人通常使用的传感器，数据集在以番茄作物为特色的典型地中海温室的所有走廊中移动。该数据集为在这种类似室内的空间中构建植物的详细3D模型提供了一个独特的机会，具有机器人喷涂等潜在应用。据作者所知，首次在温室环境中提出了一个适合测试同步定位和映射（SLAM）方法的数据集，这带来了独特的挑战。通过使用最先进的算法呈现SLAM结果来评估数据集对该目标的适用性。数据集可在线获取，网址为\url{https://arm.ual.es/arm-group/dataset-greenhouse-2024/}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.00438v1" target="_blank">2402.00438v1</a>
                              </td>
                              <td>The GREENBOT dataset: Multimodal mobile robotic dataset for a typical Mediterranean greenhouse</td>
                              <td>Fernando Cañadas-Aránega</td>
                              <td>2024-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_00438v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.00438v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2205_03256v3_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OROS: Online Operation and Orchestration of Collaborative Robots using 5G</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2205_03256v3_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2205_03256v3_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2205_03256v3_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The 5G mobile networks extend the capability for supporting collaborative robot operations in outdoor scenarios. However, the restricted battery life of robots still poses a major obstacle to their effective implementation and utilization in real scenarios. One of the most challenging situations is the execution of mission-critical tasks that require the use of various onboard sensors to perform simultaneous localization and mapping (SLAM) of unexplored environments. Given the time-sensitive nature of these tasks, completing them in the shortest possible time is of the highest importance. In this paper, we analyze the benefits of 5G-enabled collaborative robots by enhancing the intelligence of the robot operation through joint orchestration of Robot Operating System (ROS) and 5G resources for energysaving goals, addressing the problem from both offline and online manners. We propose OROS, a novel orchestration approach that minimizes mission-critical task completion times as well as overall energy consumption of 5G-connected robots by jointly optimizing robotic navigation and sensing together with infrastructure resources. We validate our 5G-enabled collaborative framework by means of Matlab/Simulink, ROS software and Gazebo simulator. Our results show an improvement between 3.65% and 11.98% in exploration task by exploiting 5G orchestration features for battery savings when using 3 robots.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2205_03256v3_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>5G移动网络扩展了在户外场景中支持机器人协同操作的能力。然而，机器人电池寿命有限仍然是其在现实场景中有效实施和利用的主要障碍。最具挑战性的情况之一是执行任务关键型任务，这些任务需要使用各种机载传感器对未探索的环境进行同时定位和测绘（SLAM）。鉴于这些任务的时间敏感性，在尽可能短的时间内完成任务是最重要的。在本文中，我们分析了支持5G的协作机器人的好处，通过机器人操作系统（ROS）和5G资源的联合协调来提高机器人操作的智能性，以实现节能目标，从离线和在线两种方式解决问题。我们提出了OROS，这是一种新的协调方法，通过联合优化机器人导航和传感以及基础设施资源，最大限度地减少任务关键任务的完成时间以及5G连接机器人的总体能耗。我们通过Matlab/Simulink、ROS软件和Gazebo模拟器验证了我们的5G协作框架。我们的结果显示，当使用3个机器人时，通过利用5G协调功能节省电池，探索任务的效率提高了3.65%至11.98%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2205.03256v3" target="_blank">2205.03256v3</a>
                              </td>
                              <td>OROS: Online Operation and Orchestration of Collaborative Robots using 5G</td>
                              <td>Arnau Romero</td>
                              <td>2022-05-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2205_03256v3_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2205.03256v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16719v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16719v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16719v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16719v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot's trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated in hardware using a quadruped robot on various terrains, yielding a 65% improvement on the Root Mean Squared Error compared to our VIO SLAM baseline. Code example: https://github.com/AlexS28/OptiState</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16719v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于腿式机器人的高度动态运动和传感器精度的限制，其状态估计具有挑战性。通过集成卡尔曼滤波、优化和基于学习的模态，我们提出了一种混合解决方案，该解决方案结合本体感觉和外部感觉信息来估计机器人躯干的状态。利用联合编码器和IMU测量，我们的卡尔曼滤波器通过单个刚体模型得到增强，该模型结合了凸模型预测控制优化的地面反作用力控制输出。通过门控递归单元进一步细化了估计，该单元还考虑了应用于深度图像的视觉转换器自动编码器的语义见解和机器人高度。该框架不仅提供了准确的机器人状态估计，包括不确定性评估，而且可以通过学习将传感器测量和模型简化产生的非线性误差降至最低。使用四足机器人在各种地形上对所提出的方法进行了硬件评估，与我们的VIO SLAM基线相比，均方根误差提高了65%。代码示例：https://github.com/AlexS28/OptiState</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16719v2" target="_blank">2401.16719v2</a>
                              </td>
                              <td>OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering</td>
                              <td>Alexander Schperberg</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16719v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16719v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alexs28/optistate" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_10896v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PLVS: A SLAM System with Points, Lines, Volumetric Mapping, and 3D Incremental Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_10896v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_10896v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_10896v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This document presents PLVS: a real-time system that leverages sparse SLAM, volumetric mapping, and 3D unsupervised incremental segmentation. PLVS stands for Points, Lines, Volumetric mapping, and Segmentation. It supports RGB-D and Stereo cameras, which may be optionally equipped with IMUs. The SLAM module is keyframe-based, and extracts and tracks sparse points and line segments as features. Volumetric mapping runs in parallel with respect to the SLAM front-end and generates a 3D reconstruction of the explored environment by fusing point clouds backprojected from keyframes. Different volumetric mapping methods are supported and integrated in PLVS. We use a novel reprojection error to bundle-adjust line segments. This error exploits available depth information to stabilize the position estimates of line segment endpoints. An incremental and geometric-based segmentation method is implemented and integrated for RGB-D cameras in the PLVS framework. We present qualitative and quantitative evaluations of the PLVS framework on some publicly available datasets. The appendix details the adopted stereo line triangulation method and provides a derivation of the Jacobians we used for line error terms. The software is available as open-source.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_10896v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了PLVS：一个利用稀疏SLAM、体积映射和3D无监督增量分割的实时系统。PLVS代表点、线、体积映射和分割。它支持RGB-D和立体声相机，这些相机可以选择配备IMU。SLAM模块基于关键帧，提取并跟踪稀疏点和线段作为特征。体积映射相对于SLAM前端并行运行，并通过融合从关键帧反向投影的点云来生成探索环境的3D重建。PLVS支持并集成了不同的体积映射方法。我们使用一种新的重投影误差来对调整线段进行集束。该误差利用可用的深度信息来稳定线段端点的位置估计。在PLVS框架中，对RGB-D相机实现并集成了一种基于几何的增量分割方法。我们在一些公开的数据集上对PLVS框架进行了定性和定量评估。附录详细介绍了所采用的立体线三角测量方法，并提供了我们用于线误差项的雅可比矩阵的推导。该软件是开源的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.10896v2" target="_blank">2309.10896v2</a>
                              </td>
                              <td>PLVS: A SLAM System with Points, Lines, Volumetric Mapping, and 3D Incremental Segmentation</td>
                              <td>Luigi Freda</td>
                              <td>2023-09-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_10896v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.10896v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/luigifreda/plvs" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17741v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17741v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17741v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17741v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents Haris, an advanced autonomous mobile robot system for tracking the location of vehicles in crowded car parks using license plate recognition. The system employs simultaneous localization and mapping (SLAM) for autonomous navigation and precise mapping of the parking area, eliminating the need for GPS dependency. In addition, the system utilizes a sophisticated framework using computer vision techniques for object detection and automatic license plate recognition (ALPR) for reading and associating license plate numbers with location data. This information is subsequently synchronized with a back-end service and made accessible to users via a user-friendly mobile app, offering effortless vehicle location and alleviating congestion within the parking facility. The proposed system has the potential to improve the management of short-term large outdoor parking areas in crowded places such as sports stadiums. The demo of the robot can be found on https://youtu.be/ZkTCM35fxa0?si=QjggJuN7M1o3oifx.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17741v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了Haris，一种先进的自主移动机器人系统，用于使用车牌识别来跟踪拥挤停车场中的车辆位置。该系统采用同步定位和地图绘制（SLAM）实现停车区的自主导航和精确地图绘制，消除了对GPS的依赖。此外，该系统利用复杂的框架，使用计算机视觉技术进行物体检测和自动车牌识别（ALPR）来读取车牌号并将其与位置数据相关联。这些信息随后与后端服务同步，并通过用户友好的移动应用程序向用户提供访问权限，轻松定位车辆，缓解停车设施内的拥堵。拟议的系统有可能改善体育场馆等拥挤场所的短期大型室外停车场的管理。机器人的演示可以在上找到https://youtu.be/ZkTCM35fxa0?si=QjggJuN7M1o3oifx.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17741v1" target="_blank">2401.17741v1</a>
                              </td>
                              <td>Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance</td>
                              <td>Layth Hamad</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17741v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17741v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17463v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Group Theoretic Metric for Robot State Estimation Leveraging Chebyshev Interpolation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17463v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17463v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17463v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a new metric for robot state estimation based on the recently introduced $\text{SE}_2(3)$ Lie group definition. Our metric is related to prior metrics for SLAM but explicitly takes into account the linear velocity of the state estimate, improving over current pose-based trajectory analysis. This has the benefit of providing a single, quantitative metric to evaluate state estimation algorithms against, while being compatible with existing tools and libraries. Since ground truth data generally consists of pose data from motion capture systems, we also propose an approach to compute the ground truth linear velocity based on polynomial interpolation. Using Chebyshev interpolation and a pseudospectral parameterization, we can accurately estimate the ground truth linear velocity of the trajectory in an optimal fashion with best approximation error. We demonstrate how this approach performs on multiple robotic platforms where accurate state estimation is vital, and compare it to alternative approaches such as finite differences. The pseudospectral parameterization also provides a means of trajectory data compression as an additional benefit. Experimental results show our method provides a valid and accurate means of comparing state estimation systems, which is also easy to interpret and report.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17463v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于最近引入的$\text，我们提出了一种新的机器人状态估计度量{SE}_2（3） $Lie群定义。我们的度量与SLAM的先前度量有关，但明确考虑了状态估计的线速度，改进了当前基于姿态的轨迹分析。这样做的好处是提供了一个单一的定量指标来评估状态估计算法，同时与现有的工具和库兼容。由于地面实况数据通常由运动捕捉系统的姿态数据组成，我们还提出了一种基于多项式插值的地面实况线速度计算方法。使用切比雪夫插值和伪谱参数化，我们可以以最佳的方式以最佳的近似误差准确地估计轨迹的真实线速度。我们展示了这种方法在多个机器人平台上的表现，在这些平台上，准确的状态估计至关重要，并将其与有限差分等替代方法进行了比较。伪谱参数化还提供了一种轨迹数据压缩方法作为额外的好处。实验结果表明，我们的方法为比较状态估计系统提供了一种有效而准确的方法，而且易于解释和报告。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17463v1" target="_blank">2401.17463v1</a>
                              </td>
                              <td>A Group Theoretic Metric for Robot State Estimation Leveraging Chebyshev Interpolation</td>
                              <td>Varun Agrawal</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17463v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17463v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17061v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17061v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17061v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17061v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Omnidirectional and 360{\deg} images are becoming widespread in industry and in consumer society, causing omnidirectional computer vision to gain attention. Their wide field of view allows the gathering of a great amount of information about the environment from only an image. However, the distortion of these images requires the development of specific algorithms for their treatment and interpretation. Moreover, a high number of images is essential for the correct training of computer vision algorithms based on learning. In this paper, we present a tool for generating datasets of omnidirectional images with semantic and depth information. These images are synthesized from a set of captures that are acquired in a realistic virtual environment for Unreal Engine 4 through an interface plugin. We gather a variety of well-known projection models such as equirectangular and cylindrical panoramas, different fish-eye lenses, catadioptric systems, and empiric models. Furthermore, we include in our tool photorealistic non-central-projection systems as non-central panoramas and non-central catadioptric systems. As far as we know, this is the first reported tool for generating photorealistic non-central images in the literature. Moreover, since the omnidirectional images are made virtually, we provide pixel-wise information about semantics and depth as well as perfect knowledge of the calibration parameters of the cameras. This allows the creation of ground-truth information with pixel precision for training learning algorithms and testing 3D vision approaches. To validate the proposed tool, different computer vision algorithms are tested as line extractions from dioptric and catadioptric central images, 3D Layout recovery and SLAM using equirectangular panoramas, and 3D reconstruction from non-central panoramas.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17061v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全方位和360度图像在工业和消费社会中越来越普遍，引起了全方位计算机视觉的关注。它们的宽视场允许仅从图像中收集大量关于环境的信息。然而，这些图像的失真需要开发特定的算法来进行处理和解释。此外，大量的图像对于基于学习的计算机视觉算法的正确训练是必不可少的。在本文中，我们提出了一种用于生成具有语义和深度信息的全向图像数据集的工具。这些图像是从在虚幻引擎4的真实虚拟环境中通过接口插件获取的一组捕获中合成的。我们收集了各种著名的投影模型，如等矩形和圆柱形全景图、不同的鱼眼透镜、折反射系统和经验模型。此外，在我们的工具中，我们将真实感非中心投影系统包括为非中心全景和非中心折反射系统。据我们所知，这是文献中第一个报道的生成真实感非中心图像的工具。此外，由于全向图像是虚拟制作的，我们提供了关于语义和深度的像素信息，以及相机校准参数的完美知识。这允许创建具有像素精度的地面实况信息，用于训练学习算法和测试3D视觉方法。为了验证所提出的工具，测试了不同的计算机视觉算法，如从屈光和折反射中心图像中提取线，使用等矩形全景进行3D布局恢复和SLAM，以及从非中心全景进行3D重建。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17061v1" target="_blank">2401.17061v1</a>
                              </td>
                              <td>OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision</td>
                              <td>Bruno Berenguel-Baeta</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17061v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17061v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sbrunoberenguel/omniscv" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10077v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards a large-scale fused and labeled dataset of human pose while interacting with robots in shared urban areas</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10077v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10077v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10077v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the last decade, Autonomous Delivery Robots (ADRs) have transformed conventional delivery methods, responding to the growing e-commerce demand. However, the readiness of ADRs to navigate safely among pedestrians in shared urban areas remains an open question. We contend that there are crucial research gaps in understanding their interactions with pedestrians in such environments. Human Pose Estimation is a vital stepping stone for various downstream applications, including pose prediction and socially aware robot path-planning. Yet, the absence of an enriched and pose-labeled dataset capturing human-robot interactions in shared urban areas hinders this objective. In this paper, we bridge this gap by repurposing, fusing, and labeling two datasets, MOT17 and NCLT, focused on pedestrian tracking and Simultaneous Localization and Mapping (SLAM), respectively. The resulting unique dataset represents thousands of real-world indoor and outdoor human-robot interaction scenarios. Leveraging YOLOv7, we obtained human pose visual and numeric outputs and provided ground truth poses using manual annotation. To overcome the distance bias present in the traditional MPJPE metric, this study introduces a novel human pose estimation error metric called Mean Scaled Joint Error (MSJE) by incorporating bounding box dimensions into it. Findings demonstrate that YOLOv7 effectively estimates human pose in both datasets. However, it exhibits weaker performance in specific scenarios, like indoor, crowded scenes with a focused light source, where both MPJPE and MSJE are recorded as 10.89 and 25.3, respectively. In contrast, YOLOv7 performs better in single-person estimation (NCLT seq 2) and outdoor scenarios (MOT17 seq1), achieving MSJE values of 5.29 and 3.38, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10077v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的十年里，自动配送机器人（ADR）改变了传统的配送方式，以应对日益增长的电子商务需求。然而，ADR是否准备好在共享城市区域的行人中安全导航仍然是一个悬而未决的问题。我们认为，在理解它们在这种环境中与行人的互动方面存在着关键的研究空白。人类姿势估计是各种下游应用的重要垫脚石，包括姿势预测和具有社会意识的机器人路径规划。然而，缺乏一个丰富的、带有姿势标签的数据集来捕捉共享城市地区的人机互动，阻碍了这一目标的实现。在本文中，我们通过重新调整、融合和标记两个数据集，MOT17和NCLT，分别专注于行人跟踪和同步定位与映射（SLAM），弥合了这一差距。由此产生的独特数据集代表了数千个真实世界的室内和室外人机交互场景。利用YOLOv7，我们获得了人类姿势的视觉和数字输出，并使用手动注释提供了地面实况姿势。为了克服传统MPJPE度量中存在的距离偏差，本研究引入了一种新的人体姿态估计误差度量，称为平均尺度联合误差（MSJE），将边界框维度纳入其中。研究结果表明，YOLOv7在两个数据集中都能有效地估计人体姿态。然而，它在特定场景中表现出较弱的性能，如具有聚焦光源的室内拥挤场景，其中MPJPE和MSJE分别记录为10.89和25.3。相比之下，YOLOv7在单人估计（NCLT seq2）和户外场景（MOT17 seq1）中表现更好，分别达到5.29和3.38的MSJE值。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10077v1" target="_blank">2402.10077v1</a>
                              </td>
                              <td>Towards a large-scale fused and labeled dataset of human pose while interacting with robots in shared urban areas</td>
                              <td>E. Sherafat</td>
                              <td>2024-01-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10077v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10077v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14857v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14857v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14857v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14857v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce an integrated precise LiDAR, Inertial, and Visual (LIV) multi-modal sensor fused mapping system that builds on the differentiable surface splatting to improve the mapping fidelity, quality, and structural accuracy. Notably, this is also a novel form of tightly coupled map for LiDAR-visual-inertial sensor fusion.   This system leverages the complementary characteristics of LiDAR and visual data to capture the geometric structures of large-scale 3D scenes and restore their visual surface information with high fidelity. The initial poses for surface Gaussian scenes are obtained using a LiDAR-inertial system with size-adaptive voxels. Then, we optimized and refined the Gaussians by visual-derived photometric gradients to optimize the quality and density of LiDAR measurements.   Our method is compatible with various types of LiDAR, including solid-state and mechanical LiDAR, supporting both repetitive and non-repetitive scanning modes. bolstering structure construction through LiDAR and facilitating real-time generation of photorealistic renderings across diverse LIV datasets. It showcases notable resilience and versatility in generating real-time photorealistic scenes potentially for digital twins and virtual reality while also holding potential applicability in real-time SLAM and robotics domains.   We release our software and hardware and self-collected datasets on Github\footnote[3]{https://github.com/sheng00125/LIV-GaussMap} to benefit the community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14857v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了一种集成的精确激光雷达、惯性和视觉（LIV）多模态传感器融合测绘系统，该系统建立在可微分表面飞溅的基础上，以提高测绘保真度、质量和结构精度。值得注意的是，这也是一种用于激光雷达视觉惯性传感器融合的新型紧耦合映射。该系统利用激光雷达和视觉数据的互补特性来捕捉大规模3D场景的几何结构，并高保真地恢复其视觉表面信息。表面高斯场景的初始姿态是使用具有尺寸自适应体素的激光雷达惯性系统获得的。然后，我们通过视觉推导的光度梯度优化和细化高斯，以优化激光雷达测量的质量和密度。我们的方法与各种类型的激光雷达兼容，包括固态和机械激光雷达，支持重复和非重复扫描模式。通过激光雷达支持结构构建，并促进在不同的LIV数据集上实时生成真实感渲染图。它在为数字双胞胎和虚拟现实生成实时真实感场景方面表现出了显著的弹性和多功能性，同时在实时SLAM和机器人领域也具有潜在的适用性。我们在Github上发布了我们的软件和硬件以及自行收集的数据集\脚注[3]{https://github.com/sheng00125/LIV-GaussMap}以造福社会。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14857v1" target="_blank">2401.14857v1</a>
                              </td>
                              <td>LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance Field Map Rendering</td>
                              <td>Sheng Hong</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14857v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14857v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13877v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AscDAMs: Advanced SLAM-based channel detection and mapping system</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13877v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13877v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13877v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Obtaining high-resolution, accurate channel topography and deposit conditions is the prior challenge for the study of channelized debris flow. Currently, wide-used mapping technologies including satellite imaging and drone photogrammetry struggle to precisely observe channel interior conditions of mountainous long-deep gullies, particularly those in the Wenchuan Earthquake region. SLAM is an emerging tech for 3D mapping; however, extremely rugged environment in long-deep gullies poses two major challenges even for the state-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation of sensors. These issues result in large deviation and lots of noise for SLAM results. To improve SLAM mapping in such environments, we propose an advanced SLAM-based channel detection and mapping system, namely AscDAMs. It features three main enhancements to post-process SLAM results: (1) The digital orthophoto map aided deviation correction algorithm greatly eliminates the systematic error; (2) The point cloud smoothing algorithm substantially diminishes noises; (3) The cross section extraction algorithm enables the quantitative assessment of channel deposits and their changes. Two field experiments were conducted in Chutou Gully, Wenchuan County in China in February and November 2023, representing observations before and after the rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM results, promoting SLAM for mapping the specially challenging environment. The proposed method compensates for the insufficiencies of existing technologies in detecting debris flow channel interiors including detailed channel morphology, erosion patterns, deposit distinction, volume estimation and change detection. It serves to enhance the study of full-scale debris flow mechanisms, long-term post-seismic evolution, and hazard assessment.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13877v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>获得高分辨率、准确的河道地形和沉积条件是渠化泥石流研究的首要挑战。目前，包括卫星成像和无人机摄影测量在内的广泛使用的测绘技术难以精确观测山区长深沟的河道内部条件，特别是汶川地震地区的深沟。SLAM是一种新兴的3D地图技术；然而，即使对最先进的SLAM来说，长深沟中极其崎岖的环境也带来了两大挑战：（1）非典型特征；（2） 传感器剧烈摆动和振荡。这些问题导致SLAM结果的大偏差和大量噪声。为了改进这种环境中的SLAM映射，我们提出了一种先进的基于SLAM的信道检测和映射系统，即AscDAM。它对后处理SLAM结果有三个主要改进：（1）数字正射影像图辅助偏差校正算法大大消除了系统误差；（2） 点云平滑算法大大减少了噪声；（3） 横截面提取算法能够对河道沉积物及其变化进行定量评估。2023年2月和11月，在中国汶川县楚头沟进行了两次野外实验，代表了雨季前后的观测结果。我们展示了AscDAM极大地提高SLAM结果的能力，促进了SLAM在特殊挑战环境中的映射。所提出的方法弥补了现有技术在检测泥石流通道内部方面的不足，包括详细的通道形态、侵蚀模式、沉积物区分、体积估计和变化检测。它有助于加强对全面泥石流机制、地震后长期演变和灾害评估的研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13877v1" target="_blank">2401.13877v1</a>
                              </td>
                              <td>AscDAMs: Advanced SLAM-based channel detection and mapping system</td>
                              <td>Tengfei Wang</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13877v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13877v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13800v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Object Navigation in real environments using hybrid policies</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13800v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13800v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13800v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments.   In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, which decomposes the problem into two different skills: (1) waypoint navigation is addressed with classical SLAM combined with a symbolic planner, whereas (2) exploration, semantic mapping and goal retrieval are dealt with deep neural networks trained with a combination of supervised learning and RL. We show the advantages of this approach compared to end-to-end methods both in simulation and a real environment and outperform the SOTA for this task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13800v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>导航已经通过SLAM和规划的结合在机器人技术中得到了经典的解决。最近，除了航路点规划之外，还探索了在模拟环境中涉及（视觉）高级推理的重要组成部分的问题，主要通过大规模机器学习来解决，特别是RL、离线RL或模仿学习。这些方法需要代理学习各种技能，如局部规划、映射对象和查询所学习的空间表示。与航路点规划（PointGoal）等更简单的任务相比，对于这些更复杂的任务，目前最先进的模型已经在模拟中进行了全面评估，但据我们所知，还没有在真实环境中进行评估。在这项工作中，我们重点关注sim2real转移。我们针对具有挑战性的多对象导航（Multi-ON）任务，并将其移植到包含原始虚拟Multi-ON对象的真实副本的物理环境中。我们介绍了一种混合导航方法，该方法将问题分解为两种不同的技能：（1）航路点导航是用经典的SLAM与符号规划器相结合来解决的，而（2）探索、语义映射和目标检索是用监督学习和RL相结合来训练的深度神经网络来解决的。我们展示了与端到端方法相比，这种方法在模拟和真实环境中的优势，并在该任务中优于SOTA。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13800v1" target="_blank">2401.13800v1</a>
                              </td>
                              <td>Multi-Object Navigation in real environments using hybrid policies</td>
                              <td>Assem Sadek</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13800v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13800v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_14590v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HeLiPR: Heterogeneous LiDAR Dataset for inter-LiDAR Place Recognition under Spatiotemporal Variations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_14590v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_14590v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_14590v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Place recognition is crucial for robot localization and loop closure in simultaneous localization and mapping (SLAM). Light Detection and Ranging (LiDAR), known for its robust sensing capabilities and measurement consistency even in varying illumination conditions, has become pivotal in various fields, surpassing traditional imaging sensors in certain applications. Among various types of LiDAR, spinning LiDARs are widely used, while non-repetitive scanning patterns have recently been utilized in robotics applications. Some LiDARs provide additional measurements such as reflectivity, Near Infrared (NIR), and velocity from Frequency modulated continuous wave (FMCW) LiDARs. Despite these advances, there is a lack of comprehensive datasets reflecting the broad spectrum of LiDAR configurations for place recognition. To tackle this issue, our paper proposes the HeLiPR dataset, curated especially for place recognition with heterogeneous LiDARs, embodying spatiotemporal variations. To the best of our knowledge, the HeLiPR dataset is the first heterogeneous LiDAR dataset supporting inter-LiDAR place recognition with both non-repetitive and spinning LiDARs, accommodating different field of view (FOV)s and varying numbers of rays. The dataset covers diverse environments, from urban cityscapes to high-dynamic freeways, over a month, enhancing adaptability and robustness across scenarios. Notably, HeLiPR includes trajectories parallel to MulRan sequences, making it valuable for research in heterogeneous LiDAR place recognition and long-term studies. The dataset is accessible at https://sites.google.com/view/heliprdataset .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_14590v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在同步定位与映射（SLAM）中，位置识别对于机器人定位和闭环至关重要。光探测和测距（LiDAR）以其强大的传感能力和测量一致性而闻名，即使在不同的光照条件下也是如此，在各个领域已经成为关键，在某些应用中超过了传统的成像传感器。在各种类型的激光雷达中，旋转激光雷达被广泛使用，而非重复扫描模式最近被用于机器人应用。一些激光雷达提供额外的测量，如反射率、近红外（NIR）和调频连续波（FMCW）激光雷达的速度。尽管取得了这些进展，但缺乏全面的数据集来反映用于位置识别的激光雷达配置的广谱性。为了解决这个问题，我们的论文提出了HeLiPR数据集，该数据集专门用于异构激光雷达的位置识别，体现了时空变化。据我们所知，HeLiPR数据集是第一个支持非重复和旋转激光雷达的激光雷达位置识别的异构激光雷达数据集，可容纳不同的视场和不同数量的光线。该数据集在一个多月的时间里涵盖了从城市景观到高动态高速公路的各种环境，增强了跨场景的适应性和稳健性。值得注意的是，HeLiPR包括与MulRan序列平行的轨迹，这对于异质激光雷达位置识别和长期研究的研究具有价值。数据集可在访问https://sites.google.com/view/heliprdataset .</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.14590v2" target="_blank">2309.14590v2</a>
                              </td>
                              <td>HeLiPR: Heterogeneous LiDAR Dataset for inter-LiDAR Place Recognition under Spatiotemporal Variations</td>
                              <td>Minwoo Jung</td>
                              <td>2023-09-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_14590v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.14590v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_00928v2_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_00928v2_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_00928v2_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_00928v2_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Global registration is a fundamental task that estimates the relative pose between two viewpoints of 3D point clouds. However, there are two issues that degrade the performance of global registration in LiDAR SLAM: one is the sparsity issue and the other is degeneracy. The sparsity issue is caused by the sparse characteristics of the 3D point cloud measurements in a mechanically spinning LiDAR sensor. The degeneracy issue sometimes occurs because the outlier-rejection methods reject too many correspondences, leaving less than three inliers. These two issues have become more severe as the pose discrepancy between the two viewpoints of 3D point clouds becomes greater. To tackle these problems, we propose a robust global registration framework, called \textit{Quatro++}. Extending our previous work that solely focused on the global registration itself, we address the robust global registration in terms of the loop closing in LiDAR SLAM. To this end, ground segmentation is exploited to achieve robust global registration. Through the experiments, we demonstrate that our proposed method shows a higher success rate than the state-of-the-art global registration methods, overcoming the sparsity and degeneracy issues. In addition, we show that ground segmentation significantly helps to increase the success rate for the ground vehicles. Finally, we apply our proposed method to the loop closing module in LiDAR SLAM and confirm that the quality of the loop constraints is improved, showing more precise mapping results. Therefore, the experimental evidence corroborated the suitability of our method as an initial alignment in the loop closing. Our code is available at https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_00928v2_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全局配准是估计三维点云的两个视点之间的相对姿态的基本任务。然而，有两个问题降低了激光雷达SLAM的全局配准性能：一个是稀疏性问题，另一个是退化性问题。稀疏性问题是由机械旋转的激光雷达传感器中的3D点云测量的稀疏特性引起的。退化问题有时会发生，因为异常值拒绝方法拒绝了太多的对应关系，只留下不到三个内部。随着3D点云的两个视点之间的姿态差异变得更大，这两个问题变得更加严重。为了解决这些问题，我们提出了一个强大的全局注册框架，称为\textit｛Quatro++｝。扩展我们之前仅专注于全球注册本身的工作，我们在激光雷达SLAM中解决了闭环方面的稳健全球注册问题。为此，利用地面分割来实现稳健的全局配准。通过实验，我们证明了我们提出的方法比最先进的全局配准方法具有更高的成功率，克服了稀疏性和退化性问题。此外，我们还表明，地面分割显著有助于提高地面车辆的成功率。最后，我们将我们提出的方法应用于激光雷达SLAM中的闭环模块，并证实了环路约束的质量得到了提高，显示出更精确的映射结果。因此，实验证据证实了我们的方法作为闭环初始对准的适用性。我们的代码可在https://quatro-plusplus.github.io.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.00928v2" target="_blank">2311.00928v2</a>
                              </td>
                              <td>Quatro++: Robust Global Registration Exploiting Ground Segmentation for Loop Closing in LiDAR SLAM</td>
                              <td>Hyungtae Lim</td>
                              <td>2023-11-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_00928v2_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.00928v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10857v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10857v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10857v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10857v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep learning algorithms have driven expressive progress in many complex tasks. The loss function is a core component of deep learning techniques, guiding the learning process of neural networks. This paper contributes by introducing a consistency loss for visual odometry with deep learning-based approaches. The motion consistency loss explores repeated motions that appear in consecutive overlapped video clips. Experimental results show that our approach increased the performance of a model on the KITTI odometry benchmark.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10857v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>深度学习算法推动了许多复杂任务的表达进度。损失函数是深度学习技术的核心组成部分，指导神经网络的学习过程。本文通过引入基于深度学习的视觉里程计方法的一致性损失来做出贡献。运动一致性损失探索出现在连续重叠视频剪辑中的重复运动。实验结果表明，我们的方法提高了模型在KITTI里程计基准上的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10857v1" target="_blank">2401.10857v1</a>
                              </td>
                              <td>Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning</td>
                              <td>André O. Françani</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10857v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10857v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10560v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">360ORB-SLAM: A Visual SLAM System for Panoramic Images with Depth Completion Network</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10560v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10560v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10560v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To enhance the performance and effect of AR/VR applications and visual assistance and inspection systems, visual simultaneous localization and mapping (vSLAM) is a fundamental task in computer vision and robotics. However, traditional vSLAM systems are limited by the camera's narrow field-of-view, resulting in challenges such as sparse feature distribution and lack of dense depth information. To overcome these limitations, this paper proposes a 360ORB-SLAM system for panoramic images that combines with a depth completion network. The system extracts feature points from the panoramic image, utilizes a panoramic triangulation module to generate sparse depth information, and employs a depth completion network to obtain a dense panoramic depth map. Experimental results on our novel panoramic dataset constructed based on Carla demonstrate that the proposed method achieves superior scale accuracy compared to existing monocular SLAM methods and effectively addresses the challenges of feature association and scale ambiguity. The integration of the depth completion network enhances system stability and mitigates the impact of dynamic elements on SLAM performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10560v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了提高AR/VR应用程序以及视觉辅助和检测系统的性能和效果，视觉同步定位和映射（vSLAM）是计算机视觉和机器人技术中的一项基本任务。然而，传统的vSLAM系统受到相机狭窄视场的限制，导致了特征分布稀疏和缺乏密集深度信息等挑战。为了克服这些限制，本文提出了一种360ORB-SLAM全景图像系统，该系统与深度补全网络相结合。该系统从全景图像中提取特征点，利用全景三角测量模块生成稀疏的深度信息，并利用深度补全网络获得密集的全景深度图。在基于Carla构建的新全景数据集上的实验结果表明，与现有的单目SLAM方法相比，该方法实现了优越的尺度精度，并有效地解决了特征关联和尺度模糊的挑战。深度完井网络的集成增强了系统稳定性，并减轻了动态元素对SLAM性能的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10560v1" target="_blank">2401.10560v1</a>
                              </td>
                              <td>360ORB-SLAM: A Visual SLAM System for Panoramic Images with Depth Completion Network</td>
                              <td>Yichen Chen</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10560v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10560v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09388v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of Quadruped Robot</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09388v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09388v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09388v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces CognitiveDog, a pioneering development of quadruped robot with Large Multi-modal Model (LMM) that is capable of not only communicating with humans verbally but also physically interacting with the environment through object manipulation. The system was realized on Unitree Go1 robot-dog equipped with a custom gripper and demonstrated autonomous decision-making capabilities, independently determining the most appropriate actions and interactions with various objects to fulfill user-defined tasks. These tasks do not necessarily include direct instructions, challenging the robot to comprehend and execute them based on natural language input and environmental cues. The paper delves into the intricacies of this system, dataset characteristics, and the software architecture. Key to this development is the robot's proficiency in navigating space using Visual-SLAM, effectively manipulating and transporting objects, and providing insightful natural language commentary during task execution. Experimental results highlight the robot's advanced task comprehension and adaptability, underscoring its potential in real-world applications. The dataset used to fine-tune the robot-dog behavior generation model is provided at the following link: huggingface.co/datasets/ArtemLykov/CognitiveDog_dataset</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09388v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了CognitiveDog，这是一种具有大型多模态模型（LMM）的四足机器人的开创性发展，它不仅能够与人类进行言语交流，而且能够通过物体操纵与环境进行物理交互。该系统是在配备了自定义夹持器的Unitree Go1机器狗上实现的，并展示了自主决策能力，独立确定最合适的动作以及与各种对象的交互，以完成用户定义的任务。这些任务不一定包括直接指令，挑战机器人根据自然语言输入和环境线索理解和执行指令。本文深入研究了该系统的复杂性、数据集特性和软件体系结构。这一开发的关键是机器人熟练地使用Visual SLAM在空间中导航，有效地操纵和运输物体，并在任务执行过程中提供富有洞察力的自然语言评论。实验结果突出了机器人先进的任务理解能力和适应性，突出了其在现实世界应用中的潜力。用于微调机器狗行为生成模型的数据集在以下链接中提供：huggingface.co/datasets/ArtemLykov/CognitiveDog_dataset</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09388v1" target="_blank">2401.09388v1</a>
                              </td>
                              <td>CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of Quadruped Robot</td>
                              <td>Artem Lykov</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09388v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09388v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09331v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-Based Visual Odometry on Non-Holonomic Ground Vehicles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09331v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09331v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09331v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the promise of superior performance under challenging conditions, event-based motion estimation remains a hard problem owing to the difficulty of extracting and tracking stable features from event streams. In order to robustify the estimation, it is generally believed that fusion with other sensors is a requirement. In this work, we demonstrate reliable, purely event-based visual odometry on planar ground vehicles by employing the constrained non-holonomic motion model of Ackermann steering platforms. We extend single feature n-linearities for regular frame-based cameras to the case of quasi time-continuous event-tracks, and achieve a polynomial form via variable degree Taylor expansions. Robust averaging over multiple event tracks is simply achieved via histogram voting. As demonstrated on both simulated and real data, our algorithm achieves accurate and robust estimates of the vehicle's instantaneous rotational velocity, and thus results that are comparable to the delta rotations obtained by frame-based sensors under normal conditions. We furthermore significantly outperform the more traditional alternatives in challenging illumination scenarios. The code is available at \url{https://github.com/gowanting/NHEVO}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09331v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在具有挑战性的条件下有望获得卓越的性能，但由于难以从事件流中提取和跟踪稳定特征，基于事件的运动估计仍然是一个难题。为了使估计具有鲁棒性，通常认为需要与其他传感器进行融合。在这项工作中，我们通过使用阿克曼转向平台的约束非完整运动模型，在平面地面车辆上演示了可靠的、纯基于事件的视觉里程计。我们将基于常规帧的相机的单特征n线性扩展到准时间连续事件轨迹的情况，并通过变阶泰勒展开实现多项式形式。通过直方图投票可以简单地实现对多个事件轨迹的稳健平均。如模拟数据和真实数据所示，我们的算法实现了对车辆瞬时转速的准确和稳健估计，从而获得了与正常条件下基于帧的传感器获得的德尔塔旋转相当的结果。此外，在具有挑战性的照明场景中，我们显著优于更传统的替代方案。代码位于\url{https://github.com/gowanting/NHEVO}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09331v1" target="_blank">2401.09331v1</a>
                              </td>
                              <td>Event-Based Visual Odometry on Non-Holonomic Ground Vehicles</td>
                              <td>Wanting Xu</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09331v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09331v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/gowanting/nhevo" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09322v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FIT-SLAM -- Fisher Information and Traversability estimation-based Active SLAM for exploration in 3D environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09322v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09322v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09322v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Active visual SLAM finds a wide array of applications in GNSS-Denied sub-terrain environments and outdoor environments for ground robots. To achieve robust localization and mapping accuracy, it is imperative to incorporate the perception considerations in the goal selection and path planning towards the goal during an exploration mission. Through this work, we propose FIT-SLAM (Fisher Information and Traversability estimation-based Active SLAM), a new exploration method tailored for unmanned ground vehicles (UGVs) to explore 3D environments. This approach is devised with the dual objectives of sustaining an efficient exploration rate while optimizing SLAM accuracy. Initially, an estimation of a global traversability map is conducted, which accounts for the environmental constraints pertaining to traversability. Subsequently, we propose a goal candidate selection approach along with a path planning method towards this goal that takes into account the information provided by the landmarks used by the SLAM backend to achieve robust localization and successful path execution . The entire algorithm is tested and evaluated first in a simulated 3D world, followed by a real-world environment and is compared to pre-existing exploration methods. The results obtained during this evaluation demonstrate a significant increase in the exploration rate while effectively minimizing the localization covariance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09322v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>主动视觉SLAM在拒绝全球导航卫星系统的地下环境和地面机器人的户外环境中有着广泛的应用。为了实现稳健的定位和测绘精度，在探索任务期间，必须将感知考虑因素纳入目标选择和目标路径规划中。通过这项工作，我们提出了FIT-SLAM（基于Fisher信息和遍历性估计的主动SLAM），这是一种为无人地面飞行器（UGV）探索3D环境量身定制的新探索方法。这种方法的设计具有双重目标，即在优化SLAM精度的同时保持有效的勘探速率。最初，对全球可穿越性地图进行估计，该地图考虑了与可穿越性相关的环境约束。随后，我们提出了一种目标候选选择方法以及实现该目标的路径规划方法，该方法考虑了SLAM后端使用的地标提供的信息，以实现稳健的定位和成功的路径执行。首先在模拟的3D世界中测试和评估整个算法，然后在真实世界环境中进行测试和评估，并将其与预先存在的探索方法进行比较。在该评估过程中获得的结果表明，在有效地最小化定位协方差的同时，勘探率显著提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09322v1" target="_blank">2401.09322v1</a>
                              </td>
                              <td>FIT-SLAM -- Fisher Information and Traversability estimation-based Active SLAM for exploration in 3D environments</td>
                              <td>Suchetan Saravanan</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09322v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09322v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09160v1_0">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DK-SLAM: Monocular Visual SLAM with Deep Keypoints Adaptive Learning, Tracking and Loop-Closing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09160v1_0_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09160v1_0_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09160v1_0_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unreliable feature extraction and matching in handcrafted features undermine the performance of visual SLAM in complex real-world scenarios. While learned local features, leveraging CNNs, demonstrate proficiency in capturing high-level information and excel in matching benchmarks, they encounter challenges in continuous motion scenes, resulting in poor generalization and impacting loop detection accuracy. To address these issues, we present DK-SLAM, a monocular visual SLAM system with adaptive deep local features. MAML optimizes the training of these features, and we introduce a coarse-to-fine feature tracking approach. Initially, a direct method approximates the relative pose between consecutive frames, followed by a feature matching method for refined pose estimation. To counter cumulative positioning errors, a novel online learning binary feature-based online loop closure module identifies loop nodes within a sequence. Experimental results underscore DK-SLAM's efficacy, outperforms representative SLAM solutions, such as ORB-SLAM3 on publicly available datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09160v1_0_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>手工特征中不可靠的特征提取和匹配会破坏视觉SLAM在复杂现实世界场景中的性能。虽然利用细胞神经网络学习局部特征，证明了它们在捕捉高级信息方面的熟练程度和在匹配基准方面的出色表现，但它们在连续运动场景中遇到了挑战，导致泛化能力差，并影响环路检测精度。为了解决这些问题，我们提出了DK-SLAM，一种具有自适应深度局部特征的单目视觉SLAM系统。MAML优化了这些特征的训练，并引入了一种从粗到细的特征跟踪方法。首先，直接方法近似连续帧之间的相对姿态，然后是用于精细姿态估计的特征匹配方法。为了应对累积的定位误差，一种新颖的基于二进制特征的在线学习回路闭合模块识别序列中的回路节点。实验结果强调了DK-SLAM的功效，在公开可用的数据集上优于代表性的SLAM解决方案，如ORB-SLAM3。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09160v1" target="_blank">2401.09160v1</a>
                              </td>
                              <td>DK-SLAM: Monocular Visual SLAM with Deep Keypoints Adaptive Learning, Tracking and Loop-Closing</td>
                              <td>Hao Qu</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09160v1_0"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09160v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SFM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_12025v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_12025v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_12025v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_12025v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice. Lastly, we outline recommendations for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_12025v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着基础模型的出现，自然语言处理（NLP）领域最近发生了变革，特别是大语言模型（LLM），它彻底改变了基于文本的NLP。这一范式已经扩展到其他模式，包括语音，研究人员正在积极探索将语音基础模型（SFM）和LLM组合成能够处理多模式任务的单一统一模型。在这些任务中，本文主要研究语音到文本的翻译。通过审查已发表的关于该主题的论文，我们对迄今为止提出的架构解决方案和培训策略提出了统一的看法，强调了它们之间的异同。基于这一检查，我们不仅组织了经验教训，还展示了不同的设置和评估方法如何阻碍为每个建筑构建块和培训选择确定性能最佳的解决方案。最后，我们概述了关于该主题的未来工作的建议，旨在更好地了解ST的SFM+LLM解决方案的优势和劣势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.12025v1" target="_blank">2402.12025v1</a>
                              </td>
                              <td>Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?</td>
                              <td>Marco Gaido</td>
                              <td>2024-02-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_12025v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.12025v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11431v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Robust Error-Resistant View Selection Method for 3D Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11431v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11431v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11431v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To address the issue of increased triangulation uncertainty caused by selecting views with small camera baselines in Structure from Motion (SFM) view selection, this paper proposes a robust error-resistant view selection method. The method utilizes a triangulation-based computation to obtain an error-resistant model, which is then used to construct an error-resistant matrix. The sorting results of each row in the error-resistant matrix determine the candidate view set for each view. By traversing the candidate view sets of all views and completing the missing views based on the error-resistant matrix, the integrity of 3D reconstruction is ensured. Experimental comparisons between this method and the exhaustive method with the highest accuracy in the COLMAP program are conducted in terms of average reprojection error and absolute trajectory error in the reconstruction results. The proposed method demonstrates an average reduction of 29.40% in reprojection error accuracy and 5.07% in absolute trajectory error on the TUM dataset and DTU dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11431v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了解决在运动结构（SFM）视图选择中选择具有小相机基线的视图导致三角测量不确定性增加的问题，本文提出了一种稳健的抗误差视图选择方法。该方法利用基于三角测量的计算来获得抗误差模型，然后用于构建抗误差矩阵。抗误差矩阵中每行的排序结果确定每个视图的候选视图集。通过遍历所有视图的候选视图集，并基于容错矩阵完成缺失的视图，确保了三维重建的完整性。根据重建结果中的平均重投影误差和绝对轨迹误差，将该方法与COLMAP程序中精度最高的穷举方法进行了实验比较。所提出的方法在TUM数据集和DTU数据集上的重投影误差精度平均降低了29.40%，绝对轨迹误差平均降低了5.07%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11431v1" target="_blank">2402.11431v1</a>
                              </td>
                              <td>A Robust Error-Resistant View Selection Method for 3D Reconstruction</td>
                              <td>Shaojie Zhang</td>
                              <td>2024-02-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11431v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11431v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11287v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dense Matchers for Dense Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11287v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11287v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11287v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Optical flow is a useful input for various applications, including 3D reconstruction, pose estimation, tracking, and structure-from-motion. Despite its utility, the field of dense long-term tracking, especially over wide baselines, has not been extensively explored. This paper extends the concept of combining multiple optical flows over logarithmically spaced intervals as proposed by MFT. We demonstrate the compatibility of MFT with different optical flow networks, yielding results that surpass their individual performance. Moreover, we present a simple yet effective combination of these networks within the MFT framework. This approach proves to be competitive with more sophisticated, non-causal methods in terms of position prediction accuracy, highlighting the potential of MFT in enhancing long-term tracking applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11287v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>光流是各种应用的有用输入，包括3D重建、姿态估计、跟踪和运动结构。尽管它很有用，但密集的长期跟踪领域，特别是在宽基线上，尚未得到广泛探索。本文扩展了MFT提出的在对数间隔上组合多个光流的概念。我们展示了MFT与不同光流网络的兼容性，产生的结果超过了它们各自的性能。此外，我们在MFT框架内提出了这些网络的简单而有效的组合。在位置预测精度方面，这种方法被证明与更复杂的非因果方法具有竞争力，突出了MFT在增强长期跟踪应用方面的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11287v1" target="_blank">2402.11287v1</a>
                              </td>
                              <td>Dense Matchers for Dense Tracking</td>
                              <td>Tomáš Jelínek</td>
                              <td>2024-02-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11287v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11287v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2209_03910v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and Feature-metric Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2209_03910v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2209_03910v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2209_03910v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present PixTrack, a vision based object pose tracking framework using novel view synthesis and deep feature-metric alignment. We follow an SfM-based relocalization paradigm where we use a Neural Radiance Field to canonically represent the tracked object. Our evaluations demonstrate that our method produces highly accurate, robust, and jitter-free 6DoF pose estimates of objects in both monocular RGB images and RGB-D images without the need of any data annotation or trajectory smoothing. Our method is also computationally efficient making it easy to have multi-object tracking with no alteration to our algorithm through simple CPU multiprocessing. Our code is available at: https://github.com/GiantAI/pixtrack</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2209_03910v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了PixTrack，这是一种基于视觉的物体姿态跟踪框架，使用了新颖的视图合成和深度特征度量对齐。我们遵循基于SfM的重定位范式，其中我们使用神经辐射场来规范地表示被跟踪的对象。我们的评估表明，我们的方法对单目RGB图像和RGB-D图像中的物体进行了高度准确、稳健和无抖动的6DoF姿态估计，而无需任何数据注释或轨迹平滑。我们的方法在计算上也很高效，通过简单的CPU多处理，可以很容易地进行多对象跟踪，而不会改变我们的算法。我们的代码位于：https://github.com/GiantAI/pixtrack</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2209.03910v2" target="_blank">2209.03910v2</a>
                              </td>
                              <td>PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and Feature-metric Alignment</td>
                              <td>Prajwal Chidananda</td>
                              <td>2022-09-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2209_03910v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2209.03910v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/giantai/pixtrack" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_11883v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On-the-Fly SfM: What you capture is What you get</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_11883v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_11883v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_11883v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the last decades, ample achievements have been made on Structure from motion (SfM). However, the vast majority of them basically work in an offline manner, i.e., images are firstly captured and then fed together into a SfM pipeline for obtaining poses and sparse point cloud. In this work, on the contrary, we present an on-the-fly SfM: running online SfM while image capturing, the newly taken On-the-Fly image is online estimated with the corresponding pose and points, i.e., what you capture is what you get. Specifically, our approach firstly employs a vocabulary tree that is unsupervised trained using learning-based global features for fast image retrieval of newly fly-in image. Then, a robust feature matching mechanism with least squares (LSM) is presented to improve image registration performance. Finally, via investigating the influence of newly fly-in image's connected neighboring images, an efficient hierarchical weighted local bundle adjustment (BA) is used for optimization. Extensive experimental results demonstrate that on-the-fly SfM can meet the goal of robustly registering the images while capturing in an online way.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_11883v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几十年里，运动结构（SfM）取得了丰硕的成果。然而，它们中的绝大多数基本上是以离线方式工作的，即首先捕获图像，然后将其一起输入到SfM管道中，以获得姿态和稀疏点云。相反，在这项工作中，我们提出了一个动态SfM：在图像捕捉的同时运行在线SfM，新拍摄的动态图像是用相应的姿势和点进行在线估计的，即，你捕捉到的就是你得到的。具体来说，我们的方法首先使用了一个词汇树，该词汇树使用基于学习的全局特征进行无监督训练，用于新飞行图像的快速图像检索。然后，提出了一种鲁棒的最小二乘特征匹配机制来提高图像配准性能。最后，通过研究新飞入图像的连接相邻图像的影响，使用有效的分层加权局部束平差（BA）进行优化。大量的实验结果表明，动态SfM可以实现在以在线方式拍摄的同时稳健地配准图像的目标。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.11883v2" target="_blank">2309.11883v2</a>
                              </td>
                              <td>On-the-Fly SfM: What you capture is What you get</td>
                              <td>Zongqian Zhan</td>
                              <td>2023-09-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_11883v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.11883v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/RayShark0605/On_the_fly_SfM" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17245v4_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17245v4_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17245v4_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17245v4_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting come with a substantial storage overhead caused by growing the SfM points to millions, often demanding gigabyte-level disk space for a single unbounded scene, posing significant scalability challenges and hindering the splatting efficiency.   To address this challenge, we introduce LightGaussian, a novel method designed to transform 3D Gaussians into a more efficient and compact format. Drawing inspiration from the concept of Network Pruning, LightGaussian identifies Gaussians that are insignificant in contributing to the scene reconstruction and adopts a pruning and recovery process, effectively reducing redundancy in Gaussian counts while preserving visual effects. Additionally, LightGaussian employs distillation and pseudo-view augmentation to distill spherical harmonics to a lower degree, allowing knowledge transfer to more compact representations while maintaining reflectance. Furthermore, we propose a hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in lower bitwidth representations with minimal accuracy losses.   In summary, LightGaussian achieves an averaged compression rate over 15x while boosting the FPS from 139 to 215, enabling an efficient representation of complex scenes on Mip-NeRF 360, Tank and Temple datasets.   Project website: https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17245v4_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用基于点的技术进行实时神经渲染的最新进展为3D表示的广泛采用铺平了道路。然而，像3D高斯飞溅这样的基础方法会带来大量的存储开销，这是由于SfM点增长到数百万，通常需要千兆字节级别的磁盘空间才能用于单个无边界场景，这对可扩展性提出了重大挑战，并阻碍了飞溅效率。为了应对这一挑战，我们引入了LightGaussian，这是一种新的方法，旨在将3D高斯变换为更高效、更紧凑的格式。LightGaussian从网络修剪的概念中汲取灵感，识别出对场景重建贡献不大的高斯，并采用修剪和恢复过程，有效地减少了高斯计数的冗余，同时保留了视觉效果。此外，LightGaussian采用蒸馏和伪视图增强来提取较低程度的球面谐波，允许将知识转移到更紧凑的表示中，同时保持反射率。此外，我们提出了一种混合方案，VecTree量化，来量化所有属性，从而以最小的精度损失获得较低的位宽表示。总之，LightGaussian实现了超过15倍的平均压缩率，同时将FPS从139提高到215，从而能够在Mip-NeRF 360、Tank和Temple数据集上高效地表示复杂场景。项目网站：https://lightgaussian.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17245v4" target="_blank">2311.17245v4</a>
                              </td>
                              <td>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17245v4_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17245v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/VITA-Group/LightGaussian" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_10109v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10109v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10109v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10109v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Low-light image enhancement is a crucial visual task, and many unsupervised methods tend to overlook the degradation of visible information in low-light scenes, which adversely affects the fusion of complementary information and hinders the generation of satisfactory results. To address this, our study introduces "Enlighten-Your-Voice", a multimodal enhancement framework that innovatively enriches user interaction through voice and textual commands. This approach does not merely signify a technical leap but also represents a paradigm shift in user engagement. Our model is equipped with a Dual Collaborative Attention Module (DCAM) that meticulously caters to distinct content and color discrepancies, thereby facilitating nuanced enhancements. Complementarily, we introduce a Semantic Feature Fusion (SFM) plug-and-play module that synergizes semantic context with low-light enhancement operations, sharpening the algorithm's efficacy. Crucially, "Enlighten-Your-Voice" showcases remarkable generalization in unsupervised zero-shot scenarios. The source code can be accessed from https://github.com/zhangbaijin/Enlighten-Your-Voice</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10109v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>弱光图像增强是一项至关重要的视觉任务，许多无监督方法往往忽略了弱光场景中可见信息的退化，这对互补信息的融合产生了不利影响，并阻碍了令人满意的结果的产生。为了解决这一问题，我们的研究引入了“启发你的声音”，这是一个多模式增强框架，通过语音和文本命令创新地丰富了用户交互。这种方法不仅意味着技术上的飞跃，而且代表着用户参与度的范式转变。我们的模型配备了双协作注意力模块（DCAM），该模块可精心处理不同的内容和颜色差异，从而促进细微的增强。作为补充，我们引入了一个语义特征融合（SFM）即插即用模块，该模块将语义上下文与弱光增强操作协同，提高了算法的有效性。至关重要的是，“启蒙青年之声”在无监督的零样本场景中展示了非凡的泛化能力。源代码可以从https://github.com/zhangbaijin/Enlighten-Your-Voice</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10109v2" target="_blank">2312.10109v2</a>
                              </td>
                              <td>Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image Enhancement</td>
                              <td>Xiaofeng Zhang</td>
                              <td>2023-12-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10109v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10109v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhangbaijin/enlighten-your-voice" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17592v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Local Feature Matching Using Deep Learning: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17592v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17592v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17592v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local feature matching enjoys wide-ranging applications in the realm of computer vision, encompassing domains such as image retrieval, 3D reconstruction, and object recognition. However, challenges persist in improving the accuracy and robustness of matching due to factors like viewpoint and lighting variations. In recent years, the introduction of deep learning models has sparked widespread exploration into local feature matching techniques. The objective of this endeavor is to furnish a comprehensive overview of local feature matching methods. These methods are categorized into two key segments based on the presence of detectors. The Detector-based category encompasses models inclusive of Detect-then-Describe, Joint Detection and Description, Describe-then-Detect, as well as Graph Based techniques. In contrast, the Detector-free category comprises CNN Based, Transformer Based, and Patch Based methods. Our study extends beyond methodological analysis, incorporating evaluations of prevalent datasets and metrics to facilitate a quantitative comparison of state-of-the-art techniques. The paper also explores the practical application of local feature matching in diverse domains such as Structure from Motion, Remote Sensing Image Registration, and Medical Image Registration, underscoring its versatility and significance across various fields. Ultimately, we endeavor to outline the current challenges faced in this domain and furnish future research directions, thereby serving as a reference for researchers involved in local feature matching and its interconnected domains.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17592v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部特征匹配在计算机视觉领域有着广泛的应用，包括图像检索、三维重建和物体识别等领域。然而，由于视点和照明变化等因素，在提高匹配的准确性和稳健性方面仍然存在挑战。近年来，深度学习模型的引入引发了对局部特征匹配技术的广泛探索。这项工作的目的是提供局部特征匹配方法的全面概述。根据探测器的存在，这些方法分为两个关键部分。基于检测器的类别包括检测然后描述、联合检测和描述、描述然后检测以及基于图的技术等模型。相比之下，无检测器类别包括基于CNN、基于转换器和基于补丁的方法。我们的研究超越了方法分析，结合了对流行数据集和指标的评估，以促进对最先进技术的定量比较。本文还探讨了局部特征匹配在运动结构、遥感图像配准和医学图像配准等不同领域的实际应用，强调了其在各个领域的通用性和意义。最终，我们努力概述该领域当前面临的挑战，并提供未来的研究方向，从而为参与局部特征匹配及其互连领域的研究人员提供参考。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17592v1" target="_blank">2401.17592v1</a>
                              </td>
                              <td>Local Feature Matching Using Deep Learning: A Survey</td>
                              <td>Shibiao Xu</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17592v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17592v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07250v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07250v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07250v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07250v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The localization of objects is a crucial task in various applications such as robotics, virtual and augmented reality, and the transportation of goods in warehouses. Recent advances in deep learning have enabled the localization using monocular visual cameras. While structure from motion (SfM) predicts the absolute pose from a point cloud, absolute pose regression (APR) methods learn a semantic understanding of the environment through neural networks. However, both fields face challenges caused by the environment such as motion blur, lighting changes, repetitive patterns, and feature-less structures. This study aims to address these challenges by incorporating additional information and regularizing the absolute pose using relative pose regression (RPR) methods. RPR methods suffer under different challenges, i.e., motion blur. The optical flow between consecutive images is computed using the Lucas-Kanade algorithm, and the relative pose is predicted using an auxiliary small recurrent convolutional network. The fusion of absolute and relative poses is a complex task due to the mismatch between the global and local coordinate systems. State-of-the-art methods fusing absolute and relative poses use pose graph optimization (PGO) to regularize the absolute pose predictions using relative poses. In this work, we propose recurrent fusion networks to optimally align absolute and relative pose predictions to improve the absolute pose prediction. We evaluate eight different recurrent units and construct a simulation environment to pre-train the APR and RPR networks for better generalized training. Additionally, we record a large database of different scenarios in a challenging large-scale indoor environment that mimics a warehouse with transportation robots. We conduct hyperparameter searches and experiments to show the effectiveness of our recurrent fusion method compared to PGO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07250v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在机器人、虚拟和增强现实以及仓库中的货物运输等各种应用中，物体的定位是一项至关重要的任务。深度学习的最新进展使得能够使用单目视觉相机进行定位。虽然运动结构（SfM）从点云预测绝对姿态，但绝对姿态回归（APR）方法通过神经网络学习对环境的语义理解。然而，这两个领域都面临着环境带来的挑战，如运动模糊、照明变化、重复模式和无特征结构。本研究旨在通过结合额外信息和使用相对姿态回归（RPR）方法规范绝对姿态来应对这些挑战。RPR方法面临不同的挑战，即运动模糊。使用Lucas Kanade算法计算连续图像之间的光流，并使用辅助的小型递归卷积网络预测相对姿态。由于全局坐标系和局部坐标系之间的不匹配，绝对姿态和相对姿态的融合是一项复杂的任务。融合绝对姿态和相对姿态的现有技术方法使用姿态图优化（PGO）来使用相对姿态正则化绝对姿态预测。在这项工作中，我们提出了递归融合网络来优化绝对和相对姿态预测，以改进绝对姿态预测。我们评估了八个不同的递归单元，并构建了一个模拟环境来预训练APR和RPR网络，以便更好地进行广义训练。此外，我们在具有挑战性的大型室内环境中记录了不同场景的大型数据库，该环境模拟了带有运输机器人的仓库。我们进行了超参数搜索和实验，以显示与PGO相比，我们的递归融合方法的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07250v3" target="_blank">2304.07250v3</a>
                              </td>
                              <td>Fusing Structure from Motion and Simulation-Augmented Pose Regression from Optical Flow for Challenging Indoor Environments</td>
                              <td>Felix Ott</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07250v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07250v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_15667v4_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_15667v4_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_15667v4_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_15667v4_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_15667v4_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>相机姿态估计是一个长期存在的计算机视觉问题，迄今为止，它通常依赖于经典的方法，如手工关键点匹配、RANSAC和束调整。在本文中，我们建议在概率扩散框架内公式化运动结构（SfM）问题，对给定输入图像的相机姿态的条件分布进行建模。这种对老问题的新颖看法有几个优点。（i） 扩散框架的性质反映了束调整的迭代过程。（ii）该公式允许来自核极几何的几何约束的无缝集成。（iii）它在典型的困难场景中表现出色，例如具有宽基线的稀疏视图。（iv）该方法可以预测任意数量的图像的内在和外在。我们在两个真实世界的数据集上证明了我们的方法PoseDiffusion显著优于经典的SfM管道和学习的方法。最后，我们观察到，我们的方法可以在不需要进一步训练的情况下在数据集之间进行推广。项目页面：https://posediffusion.github.io/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.15667v4" target="_blank">2306.15667v4</a>
                              </td>
                              <td>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-06-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_15667v4_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.15667v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14289v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Speech foundation models on intelligibility prediction for hearing-impaired listeners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14289v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14289v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14289v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Speech foundation models (SFMs) have been benchmarked on many speech processing tasks, often achieving state-of-the-art performance with minimal adaptation. However, the SFM paradigm has been significantly less explored for applications of interest to the speech perception community. In this paper we present a systematic evaluation of 10 SFMs on one such application: Speech intelligibility prediction. We focus on the non-intrusive setup of the Clarity Prediction Challenge 2 (CPC2), where the task is to predict the percentage of words correctly perceived by hearing-impaired listeners from speech-in-noise recordings. We propose a simple method that learns a lightweight specialized prediction head on top of frozen SFMs to approach the problem. Our results reveal statistically significant differences in performance across SFMs. Our method resulted in the winning submission in the CPC2, demonstrating its promise for speech perception applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14289v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语音基础模型（SFM）已经在许多语音处理任务上进行了基准测试，通常以最小的适应度实现最先进的性能。然而，对于语音感知社区感兴趣的应用，SFM范式的探索明显较少。在本文中，我们对10个SFM的一个应用进行了系统评估：语音可懂度预测。我们专注于清晰度预测挑战2（CPC2）的非侵入性设置，其中的任务是预测听障听众从噪声记录中的语音中正确感知的单词的百分比。我们提出了一种简单的方法，在冻结的SFM上学习轻量级的专业预测头来解决这个问题。我们的研究结果显示，SFM之间的性能存在统计学上的显著差异。我们的方法在CPC2中获胜，证明了它在语音感知应用中的前景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14289v1" target="_blank">2401.14289v1</a>
                              </td>
                              <td>Speech foundation models on intelligibility prediction for hearing-impaired listeners</td>
                              <td>Santiago Cuervo</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14289v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14289v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11711v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11711v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11711v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11711v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) have garnered considerable attention as a paradigm for novel view synthesis by learning scene representations from discrete observations. Nevertheless, NeRF exhibit pronounced performance degradation when confronted with sparse view inputs, consequently curtailing its further applicability. In this work, we introduce Hierarchical Geometric, Semantic, and Photometric Guided NeRF (HG3-NeRF), a novel methodology that can address the aforementioned limitation and enhance consistency of geometry, semantic content, and appearance across different views. We propose Hierarchical Geometric Guidance (HGG) to incorporate the attachment of Structure from Motion (SfM), namely sparse depth prior, into the scene representations. Different from direct depth supervision, HGG samples volume points from local-to-global geometric regions, mitigating the misalignment caused by inherent bias in the depth prior. Furthermore, we draw inspiration from notable variations in semantic consistency observed across images of different resolutions and propose Hierarchical Semantic Guidance (HSG) to learn the coarse-to-fine semantic content, which corresponds to the coarse-to-fine scene representations. Experimental results demonstrate that HG3-NeRF can outperform other state-of-the-art methods on different standard benchmarks and achieve high-fidelity synthesis results for sparse view inputs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11711v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）作为一种通过从离散观测中学习场景表示的新型视图合成范式，已经引起了人们的极大关注。然而，当面对稀疏视图输入时，NeRF表现出明显的性能退化，从而限制了其进一步的适用性。在这项工作中，我们介绍了层次几何、语义和光度引导的NeRF（HG3-NeRF），这是一种新的方法，可以解决上述限制，并增强不同视图中几何、语义内容和外观的一致性。我们提出了分层几何制导（HGG），将运动结构的附加（SfM），即稀疏深度先验，纳入场景表示中。与直接深度监督不同，HGG从局部几何区域到全局几何区域对体积点进行采样，减轻了深度先验中固有偏差引起的偏差。此外，我们从不同分辨率的图像中观察到的语义一致性的显著变化中获得了灵感，并提出了层次语义引导（HSG）来学习粗到细的语义内容，该内容对应于粗到细场景表示。实验结果表明，HG3-NeRF可以在不同的标准基准上优于其他最先进的方法，并实现稀疏视图输入的高保真度合成结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11711v1" target="_blank">2401.11711v1</a>
                              </td>
                              <td>HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs</td>
                              <td>Zelin Gao</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11711v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11711v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10886v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SCENES: Subpixel Correspondence Estimation With Epipolar Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10886v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10886v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10886v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Extracting point correspondences from two or more views of a scene is a fundamental computer vision problem with particular importance for relative camera pose estimation and structure-from-motion. Existing local feature matching approaches, trained with correspondence supervision on large-scale datasets, obtain highly-accurate matches on the test sets. However, they do not generalise well to new datasets with different characteristics to those they were trained on, unlike classic feature extractors. Instead, they require finetuning, which assumes that ground-truth correspondences or ground-truth camera poses and 3D structure are available. We relax this assumption by removing the requirement of 3D structure, e.g., depth maps or point clouds, and only require camera pose information, which can be obtained from odometry. We do so by replacing correspondence losses with epipolar losses, which encourage putative matches to lie on the associated epipolar line. While weaker than correspondence supervision, we observe that this cue is sufficient for finetuning existing models on new data. We then further relax the assumption of known camera poses by using pose estimates in a novel bootstrapping approach. We evaluate on highly challenging datasets, including an indoor drone dataset and an outdoor smartphone camera dataset, and obtain state-of-the-art results without strong supervision.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10886v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从场景的两个或多个视图中提取点对应关系是一个基本的计算机视觉问题，对于从运动中估计相对相机姿态和结构具有特别重要的意义。现有的局部特征匹配方法，在大规模数据集上进行对应监督训练，可以在测试集上获得高度准确的匹配。然而，与经典的特征提取器不同，它们不能很好地推广到与训练对象具有不同特征的新数据集。相反，它们需要微调，这假设地面实况对应关系或地面实况摄像机姿态和3D结构是可用的。我们通过取消3D结构（例如深度图或点云）的要求来放松这一假设，并且只需要相机姿态信息，这些信息可以从里程计中获得。我们通过用核极损失代替对应损失来做到这一点，这鼓励假定的匹配位于相关的核极线上。虽然弱于对应监督，但我们观察到，这一提示足以在新数据上微调现有模型。然后，我们通过在一种新颖的自举方法中使用姿势估计，进一步放宽了已知相机姿势的假设。我们在极具挑战性的数据集上进行评估，包括室内无人机数据集和室外智能手机相机数据集，并在没有强有力监督的情况下获得最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10886v1" target="_blank">2401.10886v1</a>
                              </td>
                              <td>SCENES: Subpixel Correspondence Estimation With Epipolar Supervision</td>
                              <td>Dominik A. Kloepfer</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10886v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10886v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_09252v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_09252v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_09252v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_09252v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper provides a comprehensive survey on pioneer and state-of-the-art 3D scene geometry estimation methodologies based on single, two, or multiple images captured under the omnidirectional optics. We first revisit the basic concepts of the spherical camera model, and review the most common acquisition technologies and representation formats suitable for omnidirectional (also called 360$^\circ$, spherical or panoramic) images and videos. We then survey monocular layout and depth inference approaches, highlighting the recent advances in learning-based solutions suited for spherical data. The classical stereo matching is then revised on the spherical domain, where methodologies for detecting and describing sparse and dense features become crucial. The stereo matching concepts are then extrapolated for multiple view camera setups, categorizing them among light fields, multi-view stereo, and structure from motion (or visual simultaneous localization and mapping). We also compile and discuss commonly adopted datasets and figures of merit indicated for each purpose and list recent results for completeness. We conclude this paper by pointing out current and future trends.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_09252v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文对基于在全向光学系统下捕获的单个、两个或多个图像的先驱和最先进的3D场景几何估计方法进行了全面的综述。我们首先回顾了球面相机模型的基本概念，并回顾了适用于全向（也称为360$^\circ$，球面或全景）图像和视频的最常见的采集技术和表示格式。然后，我们调查了单目布局和深度推理方法，重点介绍了适用于球形数据的基于学习的解决方案的最新进展。然后在球面域上修改经典的立体匹配，其中检测和描述稀疏和密集特征的方法变得至关重要。然后，将立体匹配概念外推到多视图相机设置中，将其分类为光场、多视图立体和运动结构（或视觉同时定位和映射）。我们还汇编和讨论了常用的数据集和针对每种目的指出的优缺点，并列出了最新的结果以确保完整性。最后，我们指出了当前和未来的趋势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.09252v1" target="_blank">2401.09252v1</a>
                              </td>
                              <td>3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey</td>
                              <td>Thiago Lopes Trugillo da Silveira</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_09252v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.09252v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08937v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08937v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08937v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08937v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces ``confidence": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08937v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在给定一组2D图像的情况下，神经辐射场（NeRF）在新视图合成（NVS）中表现出显著的性能。然而，NeRF训练需要每个输入视图的精确相机姿势，通常通过运动结构（SfM）管道获得。最近的作品试图放松这种限制，但它们仍然经常依赖于可以改进的体面的初始姿势。在这里，我们旨在消除姿势初始化的要求。我们提出了增量置信（ICON），这是一种从2D视频帧中训练NeRF的优化过程。ICON仅假设相机运动平滑，以估计姿势的初始猜测。此外，ICON引入了“置信度”：一种用于动态重加权梯度的模型质量自适应度量。ICON依赖于高置信度姿势来学习NeRF，并依赖于高置信度3D结构（由NeRF编码）来学习姿势。我们表明，与使用SfM姿势的方法相比，ICON在没有预先初始化姿势的情况下，在CO3D和HO3D中都实现了卓越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08937v1" target="_blank">2401.08937v1</a>
                              </td>
                              <td>ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization</td>
                              <td>Weiyao Wang</td>
                              <td>2024-01-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08937v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08937v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08043v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08043v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08043v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08043v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision-based localization is a cost-effective and thus attractive solution for many intelligent mobile platforms. However, its accuracy and especially robustness still suffer from low illumination conditions, illumination changes, and aggressive motion. Event-based cameras are bio-inspired visual sensors that perform well in HDR conditions and have high temporal resolution, and thus provide an interesting alternative in such challenging scenarios. While purely event-based solutions currently do not yet produce satisfying mapping results, the present work demonstrates the feasibility of purely event-based tracking if an alternative sensor is permitted for mapping. The method relies on geometric 3D-2D registration of semi-dense maps and events, and achieves highly reliable and accurate cross-modal tracking results. Practically relevant scenarios are given by depth camera-supported tracking or map-based localization with a semi-dense map prior created by a regular image-based visual SLAM or structure-from-motion system. Conventional edge-based 3D-2D alignment is extended by a novel polarity-aware registration that makes use of signed time-surface maps (STSM) obtained from event streams. We furthermore introduce a novel culling strategy for occluded points. Both modifications increase the speed of the tracker and its robustness against occlusions or large view-point variations. The approach is validated on many real datasets covering the above-mentioned challenging conditions, and compared against similar solutions realised with regular cameras.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08043v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于视觉的定位是一种具有成本效益的解决方案，因此对许多智能移动平台具有吸引力。然而，其准确性和特别是鲁棒性仍然受到低照明条件、照明变化和侵略性运动的影响。基于事件的相机是受生物启发的视觉传感器，在HDR条件下表现良好，具有高时间分辨率，因此在这种具有挑战性的场景中提供了一种有趣的替代方案。虽然纯基于事件的解决方案目前还没有产生令人满意的映射结果，但目前的工作证明了如果允许替代传感器进行映射，则纯基于事件跟踪的可行性。该方法依赖于半密集地图和事件的几何3D-2D配准，并实现了高度可靠和准确的跨模态跟踪结果。实际相关场景由深度相机支持的跟踪或基于地图的定位给出，其中半密集地图由基于常规图像的视觉SLAM或来自运动系统的结构预先创建。传统的基于边缘的3D-2D对准通过利用从事件流获得的带符号时间表面图（STSM）的新颖的极性感知配准来扩展。此外，我们还介绍了一种新的遮挡点剔除策略。这两种修改都提高了跟踪器的速度及其对遮挡或大视点变化的鲁棒性。该方法在涵盖上述挑战性条件的许多真实数据集上进行了验证，并与使用常规相机实现的类似解决方案进行了比较。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08043v1" target="_blank">2401.08043v1</a>
                              </td>
                              <td>Cross-Modal Semi-Dense 6-DoF Tracking of an Event Camera in Challenging Conditions</td>
                              <td>Yi-Fan Zuo</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08043v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08043v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zyfff/canny-evt" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_08422v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_08422v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_08422v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_08422v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Tunnel construction using the drill-and-blast method requires the 3D measurement of the excavation front to evaluate underbreak locations. Considering the inspection and measurement task's safety, cost, and efficiency, deploying lightweight autonomous robots, such as unmanned aerial vehicles (UAV), becomes more necessary and popular. Most of the previous works use a prior map for inspection viewpoint determination and do not consider dynamic obstacles. To maximally increase the level of autonomy, this paper proposes a vision-based UAV inspection framework for dynamic tunnel environments without using a prior map. Our approach utilizes a hierarchical planning scheme, decomposing the inspection problem into different levels. The high-level decision maker first determines the task for the robot and generates the target point. Then, the mid-level path planner finds the waypoint path and optimizes the collision-free static trajectory. Finally, the static trajectory will be fed into the low-level local planner to avoid dynamic obstacles and navigate to the target point. Besides, our framework contains a novel dynamic map module that can simultaneously track dynamic obstacles and represent static obstacles based on an RGB-D camera. After inspection, the Structure-from-Motion (SfM) pipeline is applied to generate the 3D shape of the target. To our best knowledge, this is the first time autonomous inspection has been realized in unknown and dynamic tunnel environments. Our flight experiments in a real tunnel prove that our method can autonomously inspect the tunnel excavation front surface. Our software is available on GitHub as an open-source ROS package.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_08422v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用钻爆法进行隧道施工需要对开挖前沿进行三维测量，以评估欠挖位置。考虑到检测和测量任务的安全性、成本和效率，部署无人机等轻型自主机器人变得更加必要和流行。以前的大多数工作都使用先验图来确定检查视点，并且没有考虑动态障碍物。为了最大限度地提高自主性水平，本文提出了一种基于视觉的无人机动态隧道环境检测框架，无需使用先验地图。我们的方法使用分层规划方案，将检查问题分解为不同的级别。高级决策者首先确定机器人的任务并生成目标点。然后，中级路径规划器找到航路点路径并优化无碰撞静态轨迹。最后，静态轨迹将被输入到低级别的局部规划器中，以避开动态障碍并导航到目标点。此外，我们的框架包含一个新的动态地图模块，该模块可以基于RGB-D相机同时跟踪动态障碍物和表示静态障碍物。检查后，应用运动结构（SfM）管道生成目标的3D形状。据我们所知，这是首次在未知和动态的隧道环境中实现自主检测。我们在实际隧道中的飞行实验证明，我们的方法可以自主检测隧道开挖前表面。我们的软件在GitHub上作为开源ROS包提供。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.08422v3" target="_blank">2301.08422v3</a>
                              </td>
                              <td>A vision-based autonomous UAV inspection framework for unknown tunnel construction sites with dynamic obstacles</td>
                              <td>Zhefan Xu</td>
                              <td>2023-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_08422v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.08422v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhefan-xu/cerlab-uav-autonomy" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03930v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Photometric Correction for Infrared Sensors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03930v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03930v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03930v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Infrared thermography has been widely used in several domains to capture and measure temperature distributions across surfaces and objects. This methodology can be further expanded to 3D applications if the spatial distribution of the temperature distribution is available. Structure from Motion (SfM) is a photometric range imaging technique that makes it possible to obtain 3D renderings from a cloud of 2D images. To explore the possibility of 3D reconstruction via SfM from infrared images, this article proposes a photometric correction model for infrared sensors based on temperature constancy. Photometric correction is accomplished by estimating the scene irradiance as the values from the solution to a differential equation for microbolometer pixel excitation with unknown coefficients and initial conditions. The model was integrated into an SfM framework and experimental evaluations demonstrate the contribution of the photometric correction for improving the estimates of both the camera motion and the scene structure. Further, experiments show that the reconstruction quality from the corrected infrared imagery achieves performance on par with state-of-the-art reconstruction using RGB sensors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03930v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>红外热成像已广泛应用于多个领域，用于捕捉和测量表面和物体的温度分布。如果温度分布的空间分布可用，则该方法可以进一步扩展到3D应用。运动结构（SfM）是一种光度范围成像技术，可以从2D图像云中获得3D渲染。为了探索通过SfM从红外图像中进行三维重建的可能性，本文提出了一种基于温度恒定性的红外传感器光度校正模型。光度校正是通过将场景辐照度估计为具有未知系数和初始条件的微测辐射热计像素激发的微分方程的解的值来实现的。该模型被集成到SfM框架中，实验评估证明了光度校正对改善相机运动和场景结构估计的贡献。此外，实验表明，校正后的红外图像的重建质量达到了与使用RGB传感器的最先进重建相当的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03930v2" target="_blank">2304.03930v2</a>
                              </td>
                              <td>Photometric Correction for Infrared Sensors</td>
                              <td>Jincheng Zhang</td>
                              <td>2023-04-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03930v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03930v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05236v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05236v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05236v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05236v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Our world is full of identical objects (\emphe.g., cans of coke, cars of same model). These duplicates, when seen together, provide additional and strong cues for us to effectively reason about 3D. Inspired by this observation, we introduce Structure from Duplicates (SfD), a novel inverse graphics framework that reconstructs geometry, material, and illumination from a single image containing multiple identical objects. SfD begins by identifying multiple instances of an object within an image, and then jointly estimates the 6DoF pose for all instances.An inverse graphics pipeline is subsequently employed to jointly reason about the shape, material of the object, and the environment light, while adhering to the shared geometry and material constraint across instances. Our primary contributions involve utilizing object duplicates as a robust prior for single-image inverse graphics and proposing an in-plane rotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object pose estimation. By leveraging multi-view cues from a single image, SfD generates more realistic and detailed 3D reconstructions, significantly outperforming existing single image reconstruction models and multi-view reconstruction approaches with a similar or greater number of observations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05236v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们的世界充满了相同的物体（例如，可乐罐、同一型号的汽车）。当这些重复出现在一起时，为我们有效地推理3D提供了额外而有力的线索。受这一观察结果的启发，我们引入了“重复结构”（SfD），这是一种新颖的逆图形框架，可以从包含多个相同对象的单个图像中重建几何体、材料和照明。SfD首先识别图像中对象的多个实例，然后联合估计所有实例的6DoF姿势。随后使用反向图形管道来联合推理对象的形状、材质和环境光，同时遵守实例之间的共享几何图形和材质约束。我们的主要贡献包括利用对象副本作为单图像逆图形的鲁棒先验，并提出用于联合6-DoF对象姿态估计的平面内旋转鲁棒运动结构（SfM）公式。通过利用来自单个图像的多视图线索，SfD生成了更真实、更详细的3D重建，显著优于具有相似或更多观测值的现有单个图像重建模型和多视图重建方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05236v1" target="_blank">2401.05236v1</a>
                              </td>
                              <td>Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects</td>
                              <td>Tianhang Cheng</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05236v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05236v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tianhang-cheng/sfd" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_03450v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Classification of Critical Configurations for any Number of Projective Views</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_03450v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_03450v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_03450v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure from motion is the process of recovering information about cameras and 3D scene from a set of images. Generally, in a noise-free setting, all information can be uniquely recovered if enough images and image points are provided. There are, however, certain cases where unique recovery is impossible, even in theory; these are called critical configurations. We use a recently developed algebraic approach to classify all critical configurations for any number of projective cameras. We show that they form well-known algebraic varieties, such as quadric surfaces and curves of degree at most 4. This paper also improves upon earlier results both by finding previously unknown critical configurations and by showing that some configurations previously believed to be critical are in fact not.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_03450v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构是从一组图像中恢复有关相机和3D场景的信息的过程。通常，在无噪声设置中，如果提供了足够的图像和图像点，则可以唯一地恢复所有信息。然而，在某些情况下，即使在理论上，也不可能进行独特的恢复；这些被称为关键配置。我们使用最近开发的代数方法对任意数量的投影相机的所有关键配置进行分类。我们证明了它们形成了众所周知的代数变体，如二次曲面和次数最多为4的曲线。本文还改进了早期的结果，发现了以前未知的关键构型，并表明一些以前认为是关键的构型实际上不是。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.03450v1" target="_blank">2401.03450v1</a>
                              </td>
                              <td>A Classification of Critical Configurations for any Number of Projective Views</td>
                              <td>Martin Bråtelund</td>
                              <td>2024-01-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_03450v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.03450v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mabraate/critical-configurations" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_11153v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Research on Multilingual Natural Scene Text Detection Algorithm</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_11153v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_11153v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_11153v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Natural scene text detection is a significant challenge in computer vision, with tremendous potential applications in multilingual, diverse, and complex text scenarios. We propose a multilingual text detection model to address the issues of low accuracy and high difficulty in detecting multilingual text in natural scenes. In response to the challenges posed by multilingual text images with multiple character sets and various font styles, we introduce the SFM Swin Transformer feature extraction network to enhance the model's robustness in detecting characters and fonts across different languages. Dealing with the considerable variation in text scales and complex arrangements in natural scene text images, we present the AS-HRFPN feature fusion network by incorporating an Adaptive Spatial Feature Fusion module and a Spatial Pyramid Pooling module. The feature fusion network improvements enhance the model's ability to detect text sizes and orientations. Addressing diverse backgrounds and font variations in multilingual scene text images is a challenge for existing methods. Limited local receptive fields hinder detection performance. To overcome this, we propose a Global Semantic Segmentation Branch, extracting and preserving global features for more effective text detection, aligning with the need for comprehensive information. In this study, we collected and built a real-world multilingual natural scene text image dataset and conducted comprehensive experiments and analyses. The experimental results demonstrate that the proposed algorithm achieves an F-measure of 85.02\%, which is 4.71\% higher than the baseline model. We also conducted extensive cross-dataset validation on MSRA-TD500, ICDAR2017MLT, and ICDAR2015 datasets to verify the generality of our approach. The code and dataset can be found at https://github.com/wangmelon/CEMLT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_11153v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自然场景文本检测是计算机视觉中的一个重大挑战，在多语言、多样化和复杂的文本场景中具有巨大的潜在应用。我们提出了一种多语言文本检测模型，以解决在自然场景中检测多语言文本的准确性低和难度高的问题。为了应对具有多个字符集和各种字体样式的多语言文本图像带来的挑战，我们引入了SFM Swin Transformer特征提取网络，以增强模型在检测不同语言的字符和字体时的鲁棒性。针对自然场景文本图像中文本尺度的巨大变化和复杂排列，我们结合自适应空间特征融合模块和空间金字塔池模块，提出了AS-HRFPN特征融合网络。特征融合网络的改进增强了模型检测文本大小和方向的能力。解决多语言场景文本图像中的不同背景和字体变化是对现有方法的挑战。有限的局部感受野阻碍了检测性能。为了克服这一点，我们提出了一个全局语义分割分支，提取并保留全局特征，以实现更有效的文本检测，从而满足对全面信息的需求。在本研究中，我们收集并构建了一个真实世界的多语言自然场景文本图像数据集，并进行了全面的实验和分析。实验结果表明，该算法的F测度为85.02%，比基线模型高4.71%。我们还对MSRA-TD500、ICDAR2017MLT和ICDAR2015数据集进行了广泛的跨数据集验证，以验证我们方法的通用性。代码和数据集位于https://github.com/wangmelon/CEMLT.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.11153v2" target="_blank">2312.11153v2</a>
                              </td>
                              <td>Research on Multilingual Natural Scene Text Detection Algorithm</td>
                              <td>Tao Wang</td>
                              <td>2023-12-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_11153v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.11153v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_09012v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_09012v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_09012v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_09012v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large-scale visual localization systems continue to rely on 3D point clouds built from image collections using structure-from-motion. While the 3D points in these models are represented using local image features, directly matching a query image's local features against the point cloud is challenging due to the scale of the nearest-neighbor search problem. Many recent approaches to visual localization have thus proposed a hybrid method, where first a global (per image) embedding is used to retrieve a small subset of database images, and local features of the query are matched only against those. It seems to have become common belief that global embeddings are critical for said image-retrieval in visual localization, despite the significant downside of having to compute two feature types for each query image. In this paper, we take a step back from this assumption and propose Constrained Approximate Nearest Neighbors (CANN), a joint solution of k-nearest-neighbors across both the geometry and appearance space using only local features. We first derive the theoretical foundation for k-nearest-neighbor retrieval across multiple metrics and then showcase how CANN improves visual localization. Our experiments on public localization benchmarks demonstrate that our method significantly outperforms both state-of-the-art global feature-based retrieval and approaches using local feature aggregation schemes. Moreover, it is an order of magnitude faster in both index and query time than feature aggregation schemes for these datasets. Code: \url{https://github.com/google-research/google-research/tree/master/cann}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_09012v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大规模视觉定位系统继续依赖于使用来自运动的结构从图像集合构建的3D点云。虽然这些模型中的3D点是使用局部图像特征来表示的，但由于最近邻搜索问题的规模，将查询图像的局部特征与点云直接匹配是具有挑战性的。因此，许多最近的视觉定位方法提出了一种混合方法，其中首先使用全局（每图像）嵌入来检索数据库图像的一小部分，并且仅针对查询的局部特征进行匹配。人们似乎已经普遍认为，全局嵌入对于视觉定位中的所述图像检索至关重要，尽管必须为每个查询图像计算两种特征类型是显著的缺点。在本文中，我们从这一假设后退一步，提出了约束近似最近邻（CANN），这是一种仅使用局部特征在几何和外观空间上的k个最近邻的联合解。我们首先推导了跨多个度量的k近邻检索的理论基础，然后展示了CANN如何改进视觉定位。我们在公共定位基准上的实验表明，我们的方法显著优于最先进的基于全局特征的检索和使用局部特征聚合方案的方法。此外，它在索引和查询时间上都比这些数据集的特征聚合方案快一个数量级。代码：\url{https://github.com/google-research/google-research/tree/master/cann}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.09012v3" target="_blank">2306.09012v3</a>
                              </td>
                              <td>Yes, we CANN: Constrained Approximate Nearest Neighbors for local feature-based visual localization</td>
                              <td>Dror Aiger</td>
                              <td>2023-06-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_09012v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.09012v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/google-research" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03704v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pose-Free Generalizable Rendering Transformer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03704v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03704v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03704v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the field of novel-view synthesis, the necessity of knowing camera poses (e.g., via Structure from Motion) before rendering has been a common practice. However, the consistent acquisition of accurate camera poses remains elusive, and errors in pose extraction can adversely impact the view synthesis process. To address this challenge, we introduce PF-GRT, a new Pose-Free framework for Generalizable Rendering Transformer, eliminating the need for pre-computed camera poses and instead leveraging feature-matching learned directly from data. PF-GRT is parameterized using a local relative coordinate system, where one of the source images is set as the origin. An OmniView Transformer is designed for fusing multi-view cues under the pose-free setting, where unposed-view fusion and origin-centric aggregation are performed. The 3D point feature along target ray is sampled by projecting onto the selected origin plane. The final pixel intensities are modulated and decoded using another Transformer. PF-GRT demonstrates an impressive ability to generalize to new scenes that were not encountered during the training phase, without the need of pre-computing camera poses. Our experiments with zero-shot rendering on the LLFF, RealEstate-10k, Shiny, and Blender datasets reveal that it produces superior quality in generating photo-realistic images. Moreover, it demonstrates robustness against noise in test camera poses. Code is available at https://zhiwenfan.github.io/PF-GRT/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03704v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在新视图合成领域，在渲染之前了解相机姿势（例如，通过运动结构）的必要性已经成为一种常见的做法。然而，准确的相机姿势的一致获取仍然难以捉摸，姿势提取中的错误可能会对视图合成过程产生不利影响。为了应对这一挑战，我们引入了PF-GRT，这是一种用于通用渲染转换器的新的无姿势框架，无需预先计算相机姿势，而是利用直接从数据中学习的特征匹配。PF-GRT使用局部相对坐标系进行参数化，其中一个源图像被设置为原点。OmniView Transformer设计用于在无姿势设置下融合多视图线索，其中执行未融合的视图融合和以原点为中心的聚合。通过投影到选定的原点平面上，对沿目标射线的三维点特征进行采样。使用另一个Transformer对最终像素强度进行调制和解码。PF-GRT展示了一种令人印象深刻的能力，可以推广到训练阶段没有遇到的新场景，而无需预先计算相机姿势。我们在LLFF、RealEstate-10k、Shiny和Blender数据集上进行的零样本渲染实验表明，它在生成照片真实感图像时产生了卓越的质量。此外，它还展示了在测试相机姿态时对噪声的鲁棒性。代码位于https://zhiwenfan.github.io/PF-GRT/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03704v3" target="_blank">2310.03704v3</a>
                              </td>
                              <td>Pose-Free Generalizable Rendering Transformer</td>
                              <td>Zhiwen Fan</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03704v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03704v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhiwenfan/PF-GRT" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhiwenfan/DragView" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_15471v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Residual Learning for Image Point Descriptors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_15471v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_15471v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_15471v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Local image feature descriptors have had a tremendous impact on the development and application of computer vision methods. It is therefore unsurprising that significant efforts are being made for learning-based image point descriptors. However, the advantage of learned methods over handcrafted methods in real applications is subtle and more nuanced than expected. Moreover, handcrafted descriptors such as SIFT and SURF still perform better point localization in Structure-from-Motion (SfM) compared to many learned counterparts. In this paper, we propose a very simple and effective approach to learning local image descriptors by using a hand-crafted detector and descriptor. Specifically, we choose to learn only the descriptors, supported by handcrafted descriptors while discarding the point localization head. We optimize the final descriptor by leveraging the knowledge already present in the handcrafted descriptor. Such an approach of optimization allows us to discard learning knowledge already present in non-differentiable functions such as the hand-crafted descriptors and only learn the residual knowledge in the main network branch. This offers 50X convergence speed compared to the standard baseline architecture of SuperPoint while at inference the combined descriptor provides superior performance over the learned and hand-crafted descriptors. This is done with minor increase in the computations over the baseline learned descriptor. Our approach has potential applications in ensemble learning and learning with non-differentiable functions. We perform experiments in matching, camera localization and Structure-from-Motion in order to showcase the advantages of our approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_15471v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>局部图像特征描述符对计算机视觉方法的发展和应用产生了巨大的影响。因此，对基于学习的图像点描述符做出重大努力并不令人惊讶。然而，在实际应用中，学习方法相对于手工制作方法的优势是微妙的，而且比预期的更微妙。此外，与许多学习的描述符相比，手工制作的描述符（如SIFT和SURF）在运动结构（SfM）中仍然执行更好的点定位。在本文中，我们提出了一种非常简单有效的方法，通过使用手工制作的检测器和描述符来学习局部图像描述符。具体来说，我们选择只学习描述符，由手工制作的描述符支持，同时丢弃点定位头。我们通过利用手工制作的描述符中已经存在的知识来优化最终描述符。这种优化方法允许我们丢弃已经存在于不可微函数中的学习知识，例如手工制作的描述符，并且只学习主网络分支中的剩余知识。与SuperPoint的标准基线架构相比，这提供了50倍的收敛速度，而在推理时，组合描述符提供了优于学习和手工制作的描述符的性能。这是在计算量比基线学习描述符略有增加的情况下完成的。我们的方法在集成学习和具有不可微函数的学习中具有潜在的应用。我们在匹配、相机定位和运动结构方面进行了实验，以展示我们方法的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.15471v1" target="_blank">2312.15471v1</a>
                              </td>
                              <td>Residual Learning for Image Point Descriptors</td>
                              <td>Rashik Shrestha</td>
                              <td>2023-12-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_15471v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.15471v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_13977v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_13977v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_13977v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_13977v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, neural implicit functions have demonstrated remarkable results in the field of multi-view reconstruction. However, most existing methods are tailored for dense views and exhibit unsatisfactory performance when dealing with sparse views. Several latest methods have been proposed for generalizing implicit reconstruction to address the sparse view reconstruction task, but they still suffer from high training costs and are merely valid under carefully selected perspectives. In this paper, we propose a novel sparse view reconstruction framework that leverages on-surface priors to achieve highly faithful surface reconstruction. Specifically, we design several constraints on global geometry alignment and local geometry refinement for jointly optimizing coarse shapes and fine details. To achieve this, we train a neural network to learn a global implicit field from the on-surface points obtained from SfM and then leverage it as a coarse geometric constraint. To exploit local geometric consistency, we project on-surface points onto seen and unseen views, treating the consistent loss of projected features as a fine geometric constraint. The experimental results with DTU and BlendedMVS datasets in two prevalent sparse settings demonstrate significant improvements over the state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_13977v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，神经隐函数在多视图重建领域取得了显著的成果。然而，大多数现有的方法都是为密集视图量身定制的，并且在处理稀疏视图时表现出不令人满意的性能。已经提出了几种最新的方法来推广隐式重建以解决稀疏视图重建任务，但它们仍然存在较高的训练成本，并且仅在精心选择的视角下有效。在本文中，我们提出了一种新的稀疏视图重建框架，该框架利用表面先验来实现高度忠实的表面重建。具体而言，我们设计了全局几何对齐和局部几何细化的几个约束条件，以共同优化粗略形状和精细细节。为了实现这一点，我们训练神经网络从SfM获得的表面点学习全局隐式场，然后将其作为粗略的几何约束。为了利用局部几何一致性，我们将表面上的点投影到可见和不可见的视图上，将投影特征的一致丢失视为精细的几何约束。DTU和BlendedMVS数据集在两种流行的稀疏设置中的实验结果表明，与最先进的方法相比，有了显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.13977v2" target="_blank">2312.13977v2</a>
                              </td>
                              <td>NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views</td>
                              <td>Han Huang</td>
                              <td>2023-12-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_13977v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.13977v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_10529v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Transformers in Unsupervised Structure-from-Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_10529v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_10529v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_10529v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformers have revolutionized deep learning based computer vision with improved performance as well as robustness to natural corruptions and adversarial attacks. Transformers are used predominantly for 2D vision tasks, including image classification, semantic segmentation, and object detection. However, robots and advanced driver assistance systems also require 3D scene understanding for decision making by extracting structure-from-motion (SfM). We propose a robust transformer-based monocular SfM method that learns to predict monocular pixel-wise depth, ego vehicle's translation and rotation, as well as camera's focal length and principal point, simultaneously. With experiments on KITTI and DDAD datasets, we demonstrate how to adapt different vision transformers and compare them against contemporary CNN-based methods. Our study shows that transformer-based architecture, though lower in run-time efficiency, achieves comparable performance while being more robust against natural corruptions, as well as untargeted and targeted attacks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_10529v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transformers通过提高性能以及对自然腐蚀和对抗性攻击的鲁棒性，彻底改变了基于深度学习的计算机视觉。转换器主要用于2D视觉任务，包括图像分类、语义分割和对象检测。然而，机器人和先进的驾驶员辅助系统也需要3D场景理解，以便通过从运动中提取结构（SfM）来进行决策。我们提出了一种基于稳健变换器的单目SfM方法，该方法可以同时学习预测单目像素深度、自我车辆的平移和旋转以及相机的焦距和主点。通过在KITTI和DDAD数据集上的实验，我们展示了如何适应不同的视觉变换器，并将其与当代基于CNN的方法进行比较。我们的研究表明，基于转换器的体系结构虽然运行时效率较低，但可以实现相当的性能，同时对自然损坏以及无目标和有针对性的攻击更具鲁棒性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.10529v1" target="_blank">2312.10529v1</a>
                              </td>
                              <td>Transformers in Unsupervised Structure-from-Motion</td>
                              <td>Hemang Chawla</td>
                              <td>2023-12-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_10529v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.10529v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/NeurAI-Lab/MT-SfMLearner" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/neurai-lab/mt-sfmlearner" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08863v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08863v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08863v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08863v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the reconstruction of high-fidelity 3D head models from static portrait image has made great progress. However, most methods require multi-view or multi-illumination information, which therefore put forward high requirements for data acquisition. In this paper, we study the reconstruction of high-fidelity 3D head models from arbitrary monocular videos. Non-rigid structure from motion (NRSFM) methods have been widely used to solve such problems according to the two-dimensional correspondence between different frames. However, the inaccurate correspondence caused by high-complex hair structures and various facial expression changes would heavily influence the reconstruction accuracy. To tackle these problems, we propose a prior-guided dynamic implicit neural network. Specifically, we design a two-part dynamic deformation field to transform the current frame space to the canonical one. We further model the head geometry in the canonical space with a learnable signed distance field (SDF) and optimize it using the volumetric rendering with the guidance of two-main head priors to improve the reconstruction accuracy and robustness. Extensive ablation studies and comparisons with state-of-the-art methods demonstrate the effectiveness and robustness of our proposed method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08863v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，从静态人像图像重建高保真三维头部模型取得了很大进展。然而，大多数方法都需要多视图或多照明信息，因此对数据采集提出了很高的要求。在本文中，我们研究了从任意单目视频中重建高保真3D头部模型。根据不同框架之间的二维对应关系，非刚性运动结构（NRSFM）方法已被广泛用于解决这些问题。然而，高度复杂的头发结构和各种面部表情变化导致的不准确对应将严重影响重建的准确性。为了解决这些问题，我们提出了一种先验引导的动态隐式神经网络。具体来说，我们设计了一个由两部分组成的动态变形场，将当前的帧空间转换为规范的帧空间。我们进一步用可学习的符号距离场（SDF）在正则空间中对头部几何结构进行建模，并在两个主要头部先验的指导下使用体积渲染对其进行优化，以提高重建精度和鲁棒性。广泛的消融研究和与最先进方法的比较证明了我们提出的方法的有效性和稳健性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08863v1" target="_blank">2312.08863v1</a>
                              </td>
                              <td>HeadRecon: High-Fidelity 3D Head Reconstruction from Monocular Video</td>
                              <td>Xueying Wang</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08863v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08863v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08760v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08760v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08760v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08760v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Neural Radiance Fields (NeRF) have demonstrated impressive performance in novel view synthesis. However, NeRF and most of its variants still rely on traditional complex pipelines to provide extrinsic and intrinsic camera parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF, directly treat camera parameters as learnable and estimate them through differential volume rendering. However, these methods work for forward-looking scenes with slight motions and fail to tackle the rotation scenario in practice. To overcome this limitation, we propose a novel \underline{c}amera parameter \underline{f}ree neural radiance field (CF-NeRF), which incrementally reconstructs 3D representations and recovers the camera parameters inspired by incremental structure from motion (SfM). Given a sequence of images, CF-NeRF estimates the camera parameters of images one by one and reconstructs the scene through initialization, implicit localization, and implicit optimization. To evaluate our method, we use a challenging real-world dataset NeRFBuster which provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF is robust to camera rotation and achieves state-of-the-art results without providing prior information and constraints.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08760v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>神经辐射场（NeRF）在新的视图合成中表现出了令人印象深刻的性能。然而，NeRF及其大多数变体仍然依赖于传统的复杂管道来提供外在和内在的相机参数，如COLMAP。最近的工作，如NeRFmm、BARF和L2G NeRF，直接将相机参数视为可学习的，并通过差分体绘制进行估计。然而，这些方法适用于有轻微运动的前瞻性场景，但在实践中无法解决旋转场景。为了克服这一限制，我们提出了一种新颖的下划线{c}amera参数\下划线{f}ree神经辐射场（CF NeRF），其增量重建3D表示并从运动中恢复受增量结构启发的相机参数（SfM）。给定一系列图像，CF-NeRF逐个估计图像的相机参数，并通过初始化、隐式定位和隐式优化重建场景。为了评估我们的方法，我们使用了一个具有挑战性的真实世界数据集NeRFBuster，该数据集提供了复杂轨迹下的12个场景。结果表明，CF-NeRF对相机旋转具有鲁棒性，并且在不提供先验信息和约束的情况下实现了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08760v1" target="_blank">2312.08760v1</a>
                              </td>
                              <td>CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning</td>
                              <td>Qingsong Yan</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08760v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08760v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_07504v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">COLMAP-Free 3D Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_07504v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_07504v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_07504v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses. To relax this constraint, multiple efforts have been made to train Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the implicit representations of NeRFs provide extra challenges to optimize the 3D structure and camera poses at the same time. On the other hand, the recently proposed 3D Gaussian Splatting provides new opportunities given its explicit point cloud representations. This paper leverages both the explicit geometric representation and the continuity of the input video stream to perform novel view synthesis without any SfM preprocessing. We process the input frames in a sequential manner and progressively grow the 3D Gaussians set by taking one input frame at a time, without the need to pre-compute the camera poses. Our method significantly improves over previous approaches in view synthesis and camera pose estimation under large motion changes. Our project page is https://oasisyang.github.io/colmap-free-3dgs</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_07504v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然神经渲染在场景重建和新颖的视图合成方面取得了令人印象深刻的进展，但它在很大程度上依赖于精确预计算的相机姿态。为了放松这一限制，已经做出了多项努力来训练神经辐射场（NeRF），而不需要预处理相机姿势。然而，NeRF的隐式表示为同时优化3D结构和相机姿态提供了额外的挑战。另一方面，鉴于其明确的点云表示，最近提出的3D高斯飞溅提供了新的机会。本文利用输入视频流的显式几何表示和连续性来执行新的视图合成，而无需任何SfM预处理。我们以顺序的方式处理输入帧，并通过一次获取一个输入帧来逐步增长3D高斯集，而无需预先计算相机姿势。在大的运动变化下，我们的方法在视图合成和相机姿态估计方面比以前的方法有了显著的改进。我们的项目页面是https://oasisyang.github.io/colmap-free-3dgs</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.07504v1" target="_blank">2312.07504v1</a>
                              </td>
                              <td>COLMAP-Free 3D Gaussian Splatting</td>
                              <td>Yang Fu</td>
                              <td>2023-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_07504v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.07504v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06865v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Keypoint-based Stereophotoclinometry for Characterizing and Navigating Small Bodies: A Factor Graph Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06865v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06865v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06865v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes the incorporation of techniques from stereophotoclinometry (SPC) into a keypoint-based structure-from-motion (SfM) system to estimate the surface normal and albedo at detected landmarks to improve autonomous surface and shape characterization of small celestial bodies from in-situ imagery. In contrast to the current state-of-the-practice method for small body shape reconstruction, i.e., SPC, which relies on human-in-the-loop verification and high-fidelity a priori information to achieve accurate results, we forego the expensive maplet estimation step and instead leverage dense keypoint measurements and correspondences from an autonomous keypoint detection and matching method based on deep learning to provide the necessary photogrammetric constraints. Moreover, we develop a factor graph-based approach allowing for simultaneous optimization of the spacecraft's pose, landmark positions, Sun-relative direction, and surface normals and albedos via fusion of Sun sensor measurements and image keypoint measurements. The proposed framework is validated on real imagery of the Cornelia crater on Asteroid 4 Vesta, along with pose estimation and mapping comparison against an SPC reconstruction, where we demonstrate precise alignment to the SPC solution without relying on any a priori camera pose and topography information or humans-in-the-loop</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06865v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文建议将立体摄影测斜（SPC）技术纳入基于关键点的运动结构（SfM）系统，以估计探测到的地标的表面法线和反照率，从而从原位图像中改进小型天体的自主表面和形状特征。与小体型重建实践方法（即SPC）的现状不同，SPC依赖于人在环验证和高保真度先验信息来实现准确的结果，我们放弃了昂贵的maplet估计步骤，而是利用基于深度学习的自主关键点检测和匹配方法的密集关键点测量和对应关系来提供必要的摄影测量约束。此外，我们开发了一种基于因子图的方法，通过融合太阳传感器测量和图像关键点测量，可以同时优化航天器的姿态、地标位置、太阳相对方向以及表面法线和反照率。所提出的框架在小行星4灶神星上科妮利亚陨石坑的真实图像上得到了验证，同时还进行了姿态估计和与SPC重建的映射比较，在SPC重建中，我们展示了与SPC解决方案的精确对准，而不依赖于任何先验的相机姿态和地形信息或环中的人类</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06865v1" target="_blank">2312.06865v1</a>
                              </td>
                              <td>Keypoint-based Stereophotoclinometry for Characterizing and Navigating Small Bodies: A Factor Graph Approach</td>
                              <td>Travis Driver</td>
                              <td>2023-12-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06865v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06865v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06741v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Gaussian Splatting SLAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06741v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06741v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06741v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the first application of 3D Gaussian Splatting to incremental 3D reconstruction using a single moving monocular or RGB-D camera. Our Simultaneous Localisation and Mapping (SLAM) method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion (SfM) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full SLAM system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation, but also reconstruction of tiny and even transparent objects.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06741v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们首次将3D高斯散射应用于使用单个移动单目或RGB-D相机的增量3D重建。我们的同步定位和映射（SLAM）方法以3帧/秒的速度实时运行，利用高斯作为唯一的3D表示，统一了所需的表示，以实现准确、高效的跟踪、映射和高质量渲染。需要进行多项创新，才能从实时相机中以高保真度连续重建3D场景。首先，为了超越最初的3DGS算法，该算法需要来自离线运动结构（SfM）系统的精确姿态，我们使用针对3D高斯的直接优化来制定3DGS的相机跟踪，并表明这能够实现快速而稳健的跟踪，并具有广泛的收敛范围。其次，通过利用高斯的显式性质，我们引入了几何验证和正则化来处理增量三维密集重建中出现的模糊性。最后，我们介绍了一个完整的SLAM系统，它不仅在新的视图合成和轨迹估计方面取得了最先进的结果，而且还重建了微小甚至透明的物体。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06741v1" target="_blank">2312.06741v1</a>
                              </td>
                              <td>Gaussian Splatting SLAM</td>
                              <td>Hidenobu Matsuki</td>
                              <td>2023-12-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06741v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06741v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_08479v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_08479v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_08479v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_08479v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Keypoint detection is a pivotal step in 3D reconstruction, whereby sets of (up to) K points are detected in each view of a scene. Crucially, the detected points need to be consistent between views, i.e., correspond to the same 3D point in the scene. One of the main challenges with keypoint detection is the formulation of the learning objective. Previous learning-based methods typically jointly learn descriptors with keypoints, and treat the keypoint detection as a binary classification task on mutual nearest neighbours. However, basing keypoint detection on descriptor nearest neighbours is a proxy task, which is not guaranteed to produce 3D-consistent keypoints. Furthermore, this ties the keypoints to a specific descriptor, complicating downstream usage. In this work, we instead learn keypoints directly from 3D consistency. To this end, we train the detector to detect tracks from large-scale SfM. As these points are often overly sparse, we derive a semi-supervised two-view detection objective to expand this set to a desired number of detections. To train a descriptor, we maximize the mutual nearest neighbour objective over the keypoints with a separate network. Results show that our approach, DeDoDe, achieves significant gains on multiple geometry benchmarks. Code is provided at https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_08479v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>关键点检测是3D重建中的关键步骤，通过该步骤可以在场景的每个视图中检测到（最多）K个点的集合。至关重要的是，检测到的点需要在视图之间保持一致，即对应于场景中的同一3D点。关键点检测的主要挑战之一是学习目标的制定。以前基于学习的方法通常将描述符与关键点联合学习，并将关键点检测视为对相互最近邻居的二元分类任务。然而，基于描述符最近邻居的关键点检测是一项代理任务，不能保证产生3D一致的关键点。此外，这将关键点与特定描述符联系在一起，使下游使用变得复杂。在这项工作中，我们直接从3D一致性中学习关键点。为此，我们训练检测器来检测大规模SfM中的轨道。由于这些点往往过于稀疏，我们推导出一个半监督的双视图检测目标，以将该集扩展到所需的检测数量。为了训练描述符，我们使用单独的网络在关键点上最大化相互最近邻目标。结果表明，我们的方法DeDoDe在多个几何基准上实现了显著的增益。代码提供于https://github.com/Parskatt/DeDoDe</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.08479v3" target="_blank">2308.08479v3</a>
                              </td>
                              <td>DeDoDe: Detect, Don't Describe -- Describe, Don't Detect for Local Feature Matching</td>
                              <td>Johan Edstedt</td>
                              <td>2023-08-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_08479v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.08479v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/parskatt/dedode" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_05889v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SuperPrimitive: Scene Reconstruction at a Primitive Level</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_05889v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_05889v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_05889v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Joint camera pose and dense geometry estimation from a set of images or a monocular video remains a challenging problem due to its computational complexity and inherent visual ambiguities. Most dense incremental reconstruction systems operate directly on image pixels and solve for their 3D positions using multi-view geometry cues. Such pixel-level approaches suffer from ambiguities or violations of multi-view consistency (e.g. caused by textureless or specular surfaces).   We address this issue with a new image representation which we call a SuperPrimitive. SuperPrimitives are obtained by splitting images into semantically correlated local regions and enhancing them with estimated surface normal directions, both of which are predicted by state-of-the-art single image neural networks. This provides a local geometry estimate per SuperPrimitive, while their relative positions are adjusted based on multi-view observations.   We demonstrate the versatility of our new representation by addressing three 3D reconstruction tasks: depth completion, few-view structure from motion, and monocular dense visual odometry.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_05889v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于其计算复杂性和固有的视觉模糊性，从一组图像或单目视频中进行联合相机姿态和密集几何估计仍然是一个具有挑战性的问题。大多数密集增量重建系统直接对图像像素进行操作，并使用多视图几何提示来求解其3D位置。这种像素级方法存在模糊性或违反多视图一致性的问题（例如，由无纹理或镜面引起）。我们用一种新的图像表示来解决这个问题，我们称之为超原始。超基元是通过将图像分割成语义相关的局部区域并用估计的表面法线方向对其进行增强来获得的，这两者都是由最先进的单图像神经网络预测的。这提供了每个SuperPrimitive的局部几何体估计，而它们的相对位置是基于多视图观测进行调整的。我们通过解决三个3D重建任务来展示我们新表示的多功能性：深度完成、运动中的少量视图结构和单目密集视觉里程计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.05889v1" target="_blank">2312.05889v1</a>
                              </td>
                              <td>SuperPrimitive: Scene Reconstruction at a Primitive Level</td>
                              <td>Kirill Mazur</td>
                              <td>2023-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_05889v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.05889v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_04563v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Visual Geometry Grounded Deep Structure From Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_04563v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_04563v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_04563v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images. Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment. Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance specific elements (e.g., keypoint matching), but are still based on the original, non-differentiable pipeline. Instead, we propose a new deep pipeline VGGSfM, where each component is fully differentiable and thus can be trained in an end-to-end manner. To this end, we introduce new mechanisms and simplifications. First, we build on recent advances in deep 2D point tracking to extract reliable pixel-accurate tracks, which eliminates the need for chaining pairwise matches. Furthermore, we recover all cameras simultaneously based on the image and track features instead of gradually registering cameras. Finally, we optimise the cameras and triangulate 3D points via a differentiable bundle adjustment layer. We attain state-of-the-art performance on three popular datasets, CO3D, IMC Phototourism, and ETH3D.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_04563v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>运动结构（SfM）是计算机视觉界的一个长期问题，其目的是从一组不受约束的2D图像中重建场景的相机姿态和3D结构。经典框架通过检测和匹配关键点、配准图像、三角测量3D点和进行束调整，以增量的方式解决了这个问题。最近的研究工作主要围绕着利用深度学习技术的力量来增强特定元素（例如，关键点匹配），但仍基于原始的、不可微分的管道。相反，我们提出了一种新的深度流水线VGGSfM，其中每个组件都是完全可微的，因此可以以端到端的方式进行训练。为此，我们引入了新的机制和简化。首先，我们在深度2D点跟踪的最新进展的基础上提取可靠的像素精确轨迹，这消除了对成对匹配进行链接的需要。此外，我们根据图像和跟踪特征同时恢复所有相机，而不是逐渐注册相机。最后，我们优化相机，并通过可微分束调整层对3D点进行三角测量。我们在三个流行的数据集上获得了最先进的性能，即CO3D、IMC Phototourism和ETH3D。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.04563v1" target="_blank">2312.04563v1</a>
                              </td>
                              <td>Visual Geometry Grounded Deep Structure From Motion</td>
                              <td>Jianyuan Wang</td>
                              <td>2023-12-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_04563v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.04563v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_15984v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Structure-from-Motion with Graph Attention Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_15984v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_15984v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed model outperforms competing learning-based methods, and challenges COLMAP while having lower runtime.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_15984v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们通过使用图注意力网络来解决从运动中学习结构（SfM）的问题。SfM是一个经典的计算机视觉问题，通过迭代最小化重投影误差来解决，称为束调整（BA），从良好的初始化开始。为了获得对BA足够好的初始化，传统方法依赖于一系列子问题（如成对姿态估计、姿态平均或三角测量），这些子问题提供了一个初始解决方案，然后可以使用BA进行细化。在这项工作中，我们通过学习一个模型来替换这些子问题，该模型将在多个视图中检测到的2D关键点作为输入，并输出相应的相机姿态和3D关键点坐标。我们的模型利用图神经网络来学习SfM特定的基元，并表明它可以用于新的和看不见的序列的重建的快速推理。实验结果表明，所提出的模型优于竞争的基于学习的方法，并在具有较低运行时间的同时挑战了COLMAP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.15984v2" target="_blank">2308.15984v2</a>
                              </td>
                              <td>Learning Structure-from-Motion with Graph Attention Networks</td>
                              <td>Lucas Brynte</td>
                              <td>2023-08-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_15984v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.15984v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_00451v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_00451v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_00451v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Novel view synthesis from limited observations remains an important and persistent task. However, high efficiency in existing NeRF-based few-shot view synthesis is often compromised to obtain an accurate 3D representation. To address this challenge, we propose a few-shot view synthesis framework based on 3D Gaussian Splatting that enables real-time and photo-realistic view synthesis with as few as three training views. The proposed method, dubbed FSGS, handles the extremely sparse initialized SfM points with a thoughtfully designed Gaussian Unpooling process. Our method iteratively distributes new Gaussians around the most representative locations, subsequently infilling local details in vacant areas. We also integrate a large-scale pre-trained monocular depth estimator within the Gaussians optimization process, leveraging online augmented views to guide the geometric optimization towards an optimal solution. Starting from sparse points observed from limited input viewpoints, our FSGS can accurately grow into unseen regions, comprehensively covering the scene and boosting the rendering quality of novel views. Overall, FSGS achieves state-of-the-art performance in both accuracy and rendering efficiency across diverse datasets, including LLFF, Mip-NeRF360, and Blender. Project website: https://zehaozhu.github.io/FSGS/.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_00451v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从有限的观测中合成新的观点仍然是一项重要而持久的任务。然而，为了获得准确的3D表示，现有的基于NeRF的少镜头视图合成中的高效率经常受到损害。为了应对这一挑战，我们提出了一种基于3D高斯散射的多镜头视图合成框架，该框架能够在只有三个训练视图的情况下进行实时和照片逼真的视图合成。所提出的方法被称为FSGS，通过精心设计的高斯去极化过程来处理极稀疏的初始化SfM点。我们的方法迭代地将新的高斯分布在最具代表性的位置周围，随后在空置区域填充局部细节。我们还在Gaussians优化过程中集成了一个大规模的预训练单目深度估计器，利用在线增强视图来引导几何优化走向最优解。从有限输入视点观察到的稀疏点开始，我们的FSGS可以准确地生长到看不见的区域，全面覆盖场景，提高新视图的渲染质量。总体而言，FSGS在各种数据集（包括LLFF、Mip-NeRF360和Blender）的准确性和渲染效率方面都达到了最先进的性能。项目网站：https://zehaozhu.github.io/FSGS/.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.00451v1" target="_blank">2312.00451v1</a>
                              </td>
                              <td>FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting</td>
                              <td>Zehao Zhu</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_00451v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.00451v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18801v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Distributed Global Structure-from-Motion with a Deep Front-End</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18801v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18801v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While initial approaches to Structure-from-Motion (SfM) revolved around both global and incremental methods, most recent applications rely on incremental systems to estimate camera poses due to their superior robustness. Though there has been tremendous progress in SfM `front-ends' powered by deep models learned from data, the state-of-the-art (incremental) SfM pipelines still rely on classical SIFT features, developed in 2004. In this work, we investigate whether leveraging the developments in feature extraction and matching helps global SfM perform on par with the SOTA incremental SfM approach (COLMAP). To do so, we design a modular SfM framework that allows us to easily combine developments in different stages of the SfM pipeline. Our experiments show that while developments in deep-learning based two-view correspondence estimation do translate to improvements in point density for scenes reconstructed with global SfM, none of them outperform SIFT when comparing with incremental SfM results on a range of datasets. Our SfM system is designed from the ground up to leverage distributed computation, enabling us to parallelize computation on multiple machines and scale to large scenes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18801v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虽然最初的运动结构（SfM）方法围绕着全局和增量方法，但由于其优越的鲁棒性，最近的应用依赖于增量系统来估计相机姿态。尽管通过从数据中学习的深度模型在SfM“前端”方面取得了巨大进展，但最先进的（增量）SfM管道仍然依赖于2004年开发的经典SIFT特征。在这项工作中，我们研究了利用特征提取和匹配的发展是否有助于全局SfM与SOTA增量SfM方法（COLMAP）不相上下。为此，我们设计了一个模块化的SfM框架，使我们能够轻松地将SfM管道不同阶段的开发结合起来。我们的实验表明，虽然基于深度学习的两视图对应性估计的发展确实转化为用全局SfM重建的场景的点密度的提高，但与一系列数据集上的增量SfM结果相比，它们都没有优于SIFT。我们的SfM系统是从头开始设计的，以利用分布式计算，使我们能够在多台机器上并行计算并扩展到大型场景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18801v1" target="_blank">2311.18801v1</a>
                              </td>
                              <td>Distributed Global Structure-from-Motion with a Deep Front-End</td>
                              <td>Ayush Baid</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18801v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18801v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/borglab/gtsfm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11702v3_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11702v3_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11702v3_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain limited to small scenes memorized during training, and thus hardly scale to realistic datasets and scenarios. In this paper, we propose a generalized SCR model trained once to be deployed in new test scenes, regardless of their scale, without any finetuning. Instead of encoding the scene coordinates into the network weights, our model takes as input a database image with some sparse 2D pixel to 3D coordinate annotations, extracted from e.g. off-the-shelf Structure-from-Motion or RGB-D data, and a query image for which are predicted a dense 3D coordinate map and its confidence, based on cross-attention. At test time, we rely on existing off-the-shelf image retrieval systems and fuse the predictions from a shortlist of relevant database images w.r.t. the query. Afterwards camera pose is obtained using standard Perspective-n-Point (PnP). Starting from selfsupervised CroCo pretrained weights, we train our model on diverse datasets to ensure generalizabilty across various scenarios, and significantly outperform other scene regression approaches, including scene-specific models, on multiple visual localization benchmarks. Finally, we show that the database representation of images and their 2D-3D annotations can be highly compressed with negligible loss of localization performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11702v3_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>场景坐标回归（SCR），即预测给定图像的每个像素的3D坐标，最近显示出了很有前途的潜力。然而，现有的方法仍然局限于训练期间记忆的小场景，因此很难扩展到真实的数据集和场景。在本文中，我们提出了一个经过一次训练的广义SCR模型，该模型将部署在新的测试场景中，无论其规模如何，而无需任何微调。我们的模型不是将场景坐标编码到网络权重中，而是将具有一些稀疏的2D像素到3D坐标注释的数据库图像作为输入，该数据库图像是从例如现成的运动结构或RGB-D数据中提取的，以及查询图像，基于交叉关注，对其预测密集的3D坐标图及其置信度。在测试时，我们依赖现有的现成图像检索系统，并将相关数据库图像的短名单中的预测与查询相融合。然后，使用标准透视n-Point（PnP）来获得相机姿势。从自监督CroCo预训练的权重开始，我们在不同的数据集上训练我们的模型，以确保在各种场景中的可推广性，并在多个视觉定位基准上显著优于其他场景回归方法，包括场景特定模型。最后，我们证明了图像的数据库表示及其2D-3D注释可以被高度压缩，而定位性能的损失可以忽略不计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11702v3" target="_blank">2307.11702v3</a>
                              </td>
                              <td>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</td>
                              <td>Jerome Revaud</td>
                              <td>2023-07-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11702v3_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11702v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11808v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robot Hand-Eye Calibration using Structure-from-Motion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11808v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11808v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we propose a new flexible method for hand-eye calibration. The vast majority of existing hand-eye calibration techniques requires a calibration rig which is used in conjunction with camera pose estimation methods. Instead, we combine structure-from-motion with known robot motions and we show that the solution can be obtained in linear form. The latter solves for both the hand-eye parameters and for the unknown scale factor inherent with structure-from-motion methods. The algebraic analysis that is made possible with such a linear formulation allows to investigate not only the well known case of general screw motions but also such singular motions as pure translations, pure rotations, and planar motions. In essence, the robot-mounted camera looks to an unknown rigid layout, tracks points over an image sequence and estimates the camera-to-robot relationship. Such a self calibration process is relevant for unmanned vehicles, robots working in remote places, and so forth. We conduct a large number of experiments which validate the quality of the method by comparing it with existing ones.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11808v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出了一种新的灵活的手眼校准方法。绝大多数现有的手眼校准技术需要与相机姿态估计方法结合使用的校准装置。相反，我们将运动中的结构与已知的机器人运动相结合，证明了该解可以以线性形式获得。后者同时求解手眼参数和运动中结构方法固有的未知比例因子。用这种线性公式进行代数分析不仅可以研究一般螺杆运动的已知情况，还可以研究纯平移、纯旋转和平面运动等奇异运动。本质上，安装在机器人上的相机观察未知的刚性布局，跟踪图像序列上的点，并估计相机与机器人的关系。这种自校准过程与无人车、在偏远地区工作的机器人等相关。我们进行了大量的实验，通过与现有方法的比较验证了该方法的质量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11808v2" target="_blank">2311.11808v2</a>
                              </td>
                              <td>Robot Hand-Eye Calibration using Structure-from-Motion</td>
                              <td>Nicolas Andreff</td>
                              <td>2023-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11808v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11808v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_11171v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_11171v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_11171v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Triangulation algorithms often aim to minimize the reprojection ($L_2$) error, but this only provides the maximum likelihood estimate when there are no errors in the camera parameters or camera poses. Although recent advancements have yielded techniques to estimate camera parameters accounting for 3D point uncertainties, most structure from motion (SfM) pipelines still use older triangulation algorithms. This work leverages recent discoveries to provide a fast, scalable, and statistically optimal way to triangulate called LOSTU. Results show that LOSTU consistently produces lower 3D reconstruction errors than conventional $L_2$ triangulation methods -- often allowing LOSTU to successfully triangulate more points. Moreover, in addition to providing a better 3D reconstruction, LOSTU can be substantially faster than Levenberg-Marquardt (or similar) optimization schemes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_11171v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三角测量算法通常旨在最小化重投影（$L_2$）误差，但这仅在相机参数或相机姿态没有误差时提供最大似然估计。尽管最近的进步已经产生了估计相机参数的技术，考虑到3D点的不确定性，但大多数运动结构（SfM）管道仍然使用旧的三角测量算法。这项工作利用最近的发现，提供了一种快速、可扩展和统计优化的三角测量方法，称为LOSTU。结果表明，与传统的$L_2$三角测量方法相比，LOSTU始终产生较低的三维重建误差——通常允许LOSTU成功地对更多的点进行三角测量。此外，除了提供更好的3D重建外，LOSTU可以比Levenberg-Marquardt（或类似）优化方案快得多。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.11171v1" target="_blank">2311.11171v1</a>
                              </td>
                              <td>LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation</td>
                              <td>Sébastien Henry</td>
                              <td>2023-11-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_11171v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.11171v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_10582v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_10582v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_10582v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human motion trajectory prediction is a very important functionality for human-robot collaboration, specifically in accompanying, guiding, or approaching tasks, but also in social robotics, self-driving vehicles, or security systems. In this paper, a novel trajectory prediction model, Social Force Generative Adversarial Network (SoFGAN), is proposed. SoFGAN uses a Generative Adversarial Network (GAN) and Social Force Model (SFM) to generate different plausible people trajectories reducing collisions in a scene. Furthermore, a Conditional Variational Autoencoder (CVAE) module is added to emphasize the destination learning. We show that our method is more accurate in making predictions in UCY or BIWI datasets than most of the current state-of-the-art models and also reduces collisions in comparison to other approaches. Through real-life experiments, we demonstrate that the model can be used in real-time without GPU's to perform good quality predictions with a low computational cost.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_10582v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类运动轨迹预测是人机协作的一个非常重要的功能，特别是在伴随、引导或接近任务时，也在社交机器人、自动驾驶车辆或安全系统中。本文提出了一种新的轨迹预测模型——社会力量生成对抗网络（SoFGAN）。SoFGAN使用生成对抗网络（GAN）和社会力量模型（SFM）来生成不同的看似合理的人的轨迹，从而减少场景中的碰撞。此外，增加了条件变分自动编码器（CVAE）模块，以强调目的地学习。我们表明，与当前大多数最先进的模型相比，我们的方法在UCY或BIWI数据集中进行预测时更准确，并且与其他方法相比，还减少了碰撞。通过真实的实验，我们证明了该模型可以在没有GPU的情况下实时使用，以低计算成本执行高质量的预测。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.10582v1" target="_blank">2311.10582v1</a>
                              </td>
                              <td>Human motion trajectory prediction using the Social Force Model for real-time and low computational cost applications</td>
                              <td>Oscar Gil</td>
                              <td>2023-11-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_10582v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.10582v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_03426v2_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_03426v2_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_03426v2_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Given a convex function $f$ on $\mathbb{R}^n$ with an integer minimizer, we show how to find an exact minimizer of $f$ using $O(n^2 \log n)$ calls to a separation oracle and $O(n^4 \log n)$ time. The previous best polynomial time algorithm for this problem given in [Jiang, SODA 2021, JACM 2022] achieves $O(n^2\log\log n/\log n)$ oracle complexity. However, the overall runtime of Jiang's algorithm is at least $\widetilde{\Omega}(n^8)$, due to expensive sub-routines such as the Lenstra-Lenstra-Lov\'asz (LLL) algorithm [Lenstra, Lenstra, Lov\'asz, Math. Ann. 1982] and random walk based cutting plane method [Bertsimas, Vempala, JACM 2004]. Our significant speedup is obtained by a nontrivial combination of a faster version of the LLL algorithm due to [Neumaier, Stehl\'e, ISSAC 2016] that gives similar guarantees, the volumetric center cutting plane method (CPM) by [Vaidya, FOCS 1989] and its fast implementation given in [Jiang, Lee, Song, Wong, STOC 2020].   For the special case of submodular function minimization (SFM), our result implies a strongly polynomial time algorithm for this problem using $O(n^3 \log n)$ calls to an evaluation oracle and $O(n^4 \log n)$ additional arithmetic operations. Both the oracle complexity and the number of arithmetic operations of our more general algorithm are better than the previous best-known runtime algorithms for this specific problem given in [Lee, Sidford, Wong, FOCS 2015] and [Dadush, V\'egh, Zambelli, SODA 2018, MOR 2021].</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_03426v2_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定$\mathbb｛R｝^n$上具有整数极小值的凸函数$f$，我们展示了如何使用对分离预言机的$O（n^2\logn）$调用和$O（n ^4\logn）$时间来找到$f$的精确极小值。[Jiang，SODA 2021，JACM 2022]中给出的该问题的先前最佳多项式时间算法实现了$O（n^2 \log\logn/\logn）$oracle复杂度。然而，由于昂贵的子程序，如Lenstra-Lenstra-Lov'asz（LLL）算法[Lenstra，Lenstra，Lov'asz，Math.Ann.1982]和基于随机行走的切割平面方法[Bertsimas，Vempala，JACM 2004]，姜算法的总体运行时间至少为$\widetilde｛\Omega｝（n^8）$。我们的显著加速是通过[Neumaier，Stehl\'e，ISSAC 2016]的LLL算法的更快版本、[Vaidya，FOCS 1989]的体积中心切割平面法（CPM）及其在[Jiang，Lee，Song，Wong，STOC 2020]中给出的快速实现的非平凡组合而获得的。对于子模函数最小化（SFM）的特殊情况，我们的结果暗示了该问题的强多项式时间算法，使用$O（n^3\logn）$对评估预言机的调用和$O（n ^4\logn。对于这一特定问题，我们更通用的算法的预言机复杂性和算术运算次数都优于[Lee，Sidford，Wong，FOCS 2015]和[Dadush，V\'egh，Zambelli，SODA 2018，MOR 2021]中给出的以前最著名的运行时算法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.03426v2" target="_blank">2304.03426v2</a>
                              </td>
                              <td>Convex Minimization with Integer Minima in $\widetilde O(n^4)$ Time</td>
                              <td>Haotian Jiang</td>
                              <td>2023-04-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_03426v2_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.03426v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_06137v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_06137v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_06137v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised monocular depth estimation methods aim to be used in critical applications such as autonomous vehicles for environment analysis. To circumvent the potential imperfections of these approaches, a quantification of the prediction confidence is crucial to guide decision-making systems that rely on depth estimation. In this paper, we propose MonoProb, a new unsupervised monocular depth estimation method that returns an interpretable uncertainty, which means that the uncertainty reflects the expected error of the network in its depth predictions. We rethink the stereo or the structure-from-motion paradigms used to train unsupervised monocular depth models as a probabilistic problem. Within a single forward pass inference, this model provides a depth prediction and a measure of its confidence, without increasing the inference time. We then improve the performance on depth and uncertainty with a novel self-distillation loss for which a student is supervised by a pseudo ground truth that is a probability distribution on depth output by a teacher. To quantify the performance of our models we design new metrics that, unlike traditional ones, measure the absolute performance of uncertainty predictions. Our experiments highlight enhancements achieved by our method on standard depth and uncertainty metrics as well as on our tailored metrics. https://github.com/CEA-LIST/MonoProb</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_06137v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督单目深度估计方法旨在用于关键应用，如用于环境分析的自动驾驶汽车。为了避免这些方法的潜在缺陷，预测置信度的量化对于指导依赖深度估计的决策系统至关重要。在本文中，我们提出了MonoProb，这是一种新的无监督单目深度估计方法，它返回可解释的不确定性，这意味着不确定性反映了网络在深度预测中的预期误差。我们将用于训练无监督单目深度模型的运动范式中的立体或结构重新思考为一个概率问题。在单次前向推理中，该模型提供深度预测及其置信度的测量，而不增加推理时间。然后，我们通过一种新颖的自蒸馏损失来提高深度和不确定性方面的性能，对于这种损失，学生受到伪基本真理的监督，伪基本真理是教师输出的深度上的概率分布。为了量化我们模型的性能，我们设计了新的指标，与传统指标不同，这些指标衡量不确定性预测的绝对性能。我们的实验强调了我们的方法在标准深度和不确定性指标以及我们定制的指标上实现的增强。https://github.com/CEA-LIST/MonoProb</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.06137v1" target="_blank">2311.06137v1</a>
                              </td>
                              <td>MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty</td>
                              <td>Rémi Marsal</td>
                              <td>2023-11-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_06137v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.06137v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cea-list/monoprob" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_05323v1_1">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Spatial Attention-based Distribution Integration Network for Human Pose Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_05323v1_1_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_05323v1_1_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, human pose estimation has made significant progress through the implementation of deep learning techniques. However, these techniques still face limitations when confronted with challenging scenarios, including occlusion, diverse appearances, variations in illumination, and overlap. To cope with such drawbacks, we present the Spatial Attention-based Distribution Integration Network (SADI-NET) to improve the accuracy of localization in such situations. Our network consists of three efficient models: the receptive fortified module (RFM), spatial fusion module (SFM), and distribution learning module (DLM). Building upon the classic HourglassNet architecture, we replace the basic block with our proposed RFM. The RFM incorporates a dilated residual block and attention mechanism to expand receptive fields while enhancing sensitivity to spatial information. In addition, the SFM incorporates multi-scale characteristics by employing both global and local attention mechanisms. Furthermore, the DLM, inspired by residual log-likelihood estimation (RLE), reconfigures a predicted heatmap using a trainable distribution weight. For the purpose of determining the efficacy of our model, we conducted extensive experiments on the MPII and LSP benchmarks. Particularly, our model obtained a remarkable $92.10\%$ percent accuracy on the MPII test dataset, demonstrating significant improvements over existing models and establishing state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_05323v1_1_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，通过深度学习技术的实施，人体姿态估计取得了重大进展。然而，当面临具有挑战性的场景时，这些技术仍然面临限制，包括遮挡、不同的外观、照明的变化和重叠。为了解决这些缺点，我们提出了基于空间注意力的分布集成网络（SADI-NET）来提高这种情况下的定位精度。我们的网络由三个有效的模型组成：接受强化模块（RFM）、空间融合模块（SFM）和分布学习模块（DLM）。在经典HourglassNet架构的基础上，我们用我们提出的RFM取代了基本块。RFM结合了扩张的残差块和注意力机制，以扩大感受野，同时增强对空间信息的敏感性。此外，通过采用全局和局部注意力机制，SFM融合了多尺度特征。此外，受残差对数似然估计（RLE）的启发，DLM使用可训练分布权重重新配置预测热图。为了确定我们的模型的有效性，我们在MPII和LSP基准上进行了广泛的实验。特别是，我们的模型在MPII测试数据集上获得了92.10%$%的显著准确率，证明了与现有模型相比的显著改进，并建立了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.05323v1" target="_blank">2311.05323v1</a>
                              </td>
                              <td>Spatial Attention-based Distribution Integration Network for Human Pose Estimation</td>
                              <td>Sihan Gao</td>
                              <td>2023-11-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_05323v1_1"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.05323v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="LLM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_13254v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13254v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13254v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13254v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13254v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了CounterCurate，这是一个全面提高对比和生成多模式模型的视觉-语言组合推理能力的框架。特别是，我们发现了两个未被充分探索的关键问题：忽视基于物理的推理（计数和位置理解），以及使用高效的文本和图像生成模型进行语义反事实微调的潜力。我们的工作开创了一种解决这些差距的方法。我们首先关注CLIP和LLaVA等多模式模型在基于物理的组合推理中的近乎偶然的性能。然后，我们使用基础图像生成模型GLIGEN应用简单的数据扩充来生成微调数据，从而显著提高性能：在我们新策划的Flickr30k Positions基准上，CLIP和LLaVA分别提高了33%和37%。此外，我们还利用了高性能文本生成和图像生成模型的功能，特别是GPT-4V和DALLE-3，以策划具有挑战性的语义反事实，从而进一步增强在SugarCrepe等基准上的合成推理能力，其中CounterCurate的性能优于GPT-4V。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13254v1" target="_blank">2402.13254v1</a>
                              </td>
                              <td>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</td>
                              <td>Jianrui Zhang</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13254v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13254v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13253v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BiMediX: Bilingual Medical Mixture of Experts LLM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13253v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13253v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13253v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we introduce BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering. We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs. Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual instruction set covering 1.3 Million diverse medical interactions, resulting in over 632 million healthcare specialized tokens for instruction tuning. Our BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and maintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively, computed across multiple medical evaluation benchmarks in English, while operating at 8-times faster inference. Moreover, our BiMediX outperforms the generic Arabic-English bilingual LLM, Jais-30B, by average absolute gains of 10% on our Arabic medical benchmark and 15% on bilingual evaluations across multiple datasets. Our project page with source code and trained model is available at https://github.com/mbzuai-oryx/BiMediX .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13253v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们介绍了BiMediX，这是第一个双语医学专家混合LLM，旨在实现英语和阿拉伯语的无缝交互。我们的模型有助于用英语和阿拉伯语进行广泛的医疗互动，包括多回合聊天以询问患者症状和病史等其他细节、多项选择题回答和开放式问题回答。我们提出了一个半自动化的英语到阿拉伯语翻译管道，并经过人工改进，以确保高质量的翻译。我们还介绍了阿拉伯医学LLM的综合评估基准。此外，我们引入了BiMed1.3M，这是一个广泛的阿英双语教学集，涵盖了130万种不同的医疗互动，产生了超过6.32亿个用于教学调整的医疗保健专业代币。我们的BiMed1.3M数据集包括250k个合成的多回合医患聊天，并保持1:2的阿拉伯语与英语比例。我们的模型在多个英语医学评估基准上计算的平均绝对增益分别为2.5%和4.1%，优于最先进的Med42和Meditron，同时推理速度快8倍。此外，我们的BiMediX在多个数据集的阿拉伯语医学基准和双语评估中的平均绝对收益分别为10%和15%，优于通用的阿拉伯语-英语双语LLM Jais-30B。我们的项目页面包含源代码和经过训练的模型，可在https://github.com/mbzuai-oryx/BiMediX .</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13253v1" target="_blank">2402.13253v1</a>
                              </td>
                              <td>BiMediX: Bilingual Medical Mixture of Experts LLM</td>
                              <td>Sara Pieri</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13253v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13253v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13249v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13249v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13249v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13249v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13249v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，在对事实一致性或幻觉评估的研究推动下，单文档新闻摘要在忠实性方面取得了实质性进展。我们询问这些进步是否会延续到其他文本摘要领域。我们提出了一个新的基于主题的对话摘要评估基准，由不同规模的LLM生成。我们提供了对这些摘要的事实一致性的二元句子级人类注释，以及对事实不一致的句子的详细解释。我们的分析表明，无论模型的大小，现有的LLM都会在对话领域产生大量的事实错误。另一方面，当LLM，包括GPT-4，作为二元事实评估器时，它们的表现很差，并且可以被主流的最先进的专业事实评估指标所超越。最后，我们用精心策划的错误分类法对幻觉类型进行了分析。我们发现，在模型生成的摘要中存在不同的错误和错误分布，并且与基于LLM的评估器相比，基于非LLM的度量可以更好地捕捉所有错误类型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13249v1" target="_blank">2402.13249v1</a>
                              </td>
                              <td>TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization</td>
                              <td>Liyan Tang</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13249v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13249v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/amazon-science/tofueval" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13234v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unlocking Insights: Semantic Search in Jupyter Notebooks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13234v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13234v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13234v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semantic search, a process aimed at delivering highly relevant search results by comprehending the searcher's intent and the contextual meaning of terms within a searchable dataspace, plays a pivotal role in information retrieval. In this paper, we investigate the application of large language models to enhance semantic search capabilities, specifically tailored for the domain of Jupyter Notebooks. Our objective is to retrieve generated outputs, such as figures or tables, associated functions and methods, and other pertinent information.   We demonstrate a semantic search framework that achieves a comprehensive semantic understanding of the entire notebook's contents, enabling it to effectively handle various types of user queries. Key components of this framework include:   1). A data preprocessor is designed to handle diverse types of cells within Jupyter Notebooks, encompassing both markdown and code cells. 2). An innovative methodology is devised to address token size limitations that arise with code-type cells. We implement a finer-grained approach to data input, transitioning from the cell level to the function level, effectively resolving these issues.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13234v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语义搜索是一个旨在通过理解搜索者的意图和可搜索数据空间中术语的上下文含义来提供高度相关的搜索结果的过程，在信息检索中发挥着关键作用。在本文中，我们研究了大型语言模型的应用，以增强专门为Jupyter Notebooks领域量身定制的语义搜索能力。我们的目标是检索生成的输出，如图形或表格、相关函数和方法以及其他相关信息。我们展示了一个语义搜索框架，该框架实现了对整个笔记本内容的全面语义理解，使其能够有效地处理各种类型的用户查询。该框架的关键组成部分包括：1）。数据预处理器设计用于处理Jupyter Notebooks中的各种类型的单元格，包括标记单元格和代码单元格。2.设计了一种创新的方法来解决代码类型单元格中出现的令牌大小限制。我们实现了一种更细粒度的数据输入方法，从单元级别过渡到功能级别，有效地解决了这些问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13234v1" target="_blank">2402.13234v1</a>
                              </td>
                              <td>Unlocking Insights: Semantic Search in Jupyter Notebooks</td>
                              <td>Lan Li</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13234v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13234v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13232v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Touch, Vision, and Language Dataset for Multimodal Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13232v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13232v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13232v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild vision-touch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-vision-language (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) touch-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human-labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark. Code and data: https://tactile-vlm.github.io.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13232v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>触摸是人类一种重要的感知方式，但它尚未被纳入多模式生成语言模型。这在一定程度上是由于难以获得触觉数据的自然语言标签，以及将触觉读数与视觉观察和语言描述相结合的复杂性。作为弥合这一差距的一步，这项工作引入了一个新的44K野生视觉触摸对数据集，其中英语标签由人类注释（10%），文本伪标签来自GPT-4V（90%）。我们使用该数据集来训练用于开放词汇分类的视觉-语言对齐触觉编码器和用于使用训练的编码器生成文本的触摸-视觉语言（TVL）模型。结果表明，与在任何一对模式上训练的现有模型相比，通过结合触摸，TVL模型提高了（+29%的分类准确率）触摸视觉语言对齐。尽管只有一小部分数据集是人为标记的，但在新的触摸视觉理解基准上，TVL模型比GPT-4V（+12%）和开源视觉语言模型（+32%）提高了视觉触觉理解。代码和数据：https://tactile-vlm.github.io.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13232v1" target="_blank">2402.13232v1</a>
                              </td>
                              <td>A Touch, Vision, and Language Dataset for Multimodal Alignment</td>
                              <td>Letian Fu</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13232v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13232v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13231v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Investigating Cultural Alignment of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13231v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13231v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13231v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13231v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>长期以来，语言与文化之间错综复杂的关系一直是语言人类学研究的主题。作为人类集体知识库而推广的大型语言模型（LLM）提出了一个关键问题：这些模型是否真正概括了不同文化所采用的多样知识？我们的研究表明，这些模型在两个维度上表现出更大的文化一致性——首先，当用特定文化的主导语言提示时，其次，当用该文化所使用的语言的精细混合进行预训练时。我们通过模拟社会学调查来量化文化一致性，将模型反应与实际调查参与者的反应进行比较作为参考。具体而言，我们复制了在埃及和美国不同地区进行的一项调查，通过提示LLM使用阿拉伯语和英语的不同预训练数据混合物，以及真实受访者的人物角色和调查问题。进一步的分析表明，对于代表性不足的人物角色和文化敏感的话题，比如那些探索社会价值观的话题，错位变得更加明显。最后，我们介绍了人类学提示，这是一种利用人类学推理来增强文化一致性的新方法。我们的研究强调了建立一个更平衡的多语言预训练数据集的必要性，以更好地代表人类经验的多样性和不同文化的多样性，这对跨语言迁移的主题有很多影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13231v1" target="_blank">2402.13231v1</a>
                              </td>
                              <td>Investigating Cultural Alignment of Large Language Models</td>
                              <td>Badr AlKhamissi</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13231v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13231v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/bkhmsi/cultural-trends" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_15006v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_15006v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_15006v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_15006v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This study critically evaluates the efficacy of prompting methods in enhancing the mathematical reasoning capability of large language models (LLMs). The investigation uses three prescriptive prompting methods - simple, persona, and conversational prompting - known for their effectiveness in enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and MMLU datasets, encompassing a broad spectrum of mathematical challenges. A grading script adapted to each dataset is used to determine the effectiveness of these prompting interventions in enhancing the model's mathematical analysis power. Contrary to expectations, our empirical analysis reveals that none of the investigated methods consistently improves over ChatGPT-3.5's baseline performance, with some causing significant degradation. Our findings suggest that prompting strategies do not necessarily generalize to new domains, in this study failing to enhance mathematical performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_15006v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本研究批判性地评估了提示方法在增强大型语言模型（LLM）数学推理能力方面的功效。该调查使用了三种规定性提示方法——简单提示、人物提示和会话提示——以其在增强LLM的语言任务方面的有效性而闻名。我们在OpenAI的LLM聊天机器人ChatGPT-3.5上对来自MATH、GSM8K和MMLU数据集的大量问题集进行了这项分析，包括广泛的数学挑战。使用适用于每个数据集的评分脚本来确定这些提示干预措施在增强模型数学分析能力方面的有效性。与预期相反，我们的实证分析显示，没有一种研究方法能持续改善ChatGPT-3.5的基线性能，其中一些方法会导致显著退化。我们的研究结果表明，提示策略不一定推广到新的领域，在这项研究中未能提高数学性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.15006v2" target="_blank">2312.15006v2</a>
                              </td>
                              <td>Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities</td>
                              <td>Yuhao Chen</td>
                              <td>2023-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_15006v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.15006v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13228v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13228v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13228v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13228v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. By fine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which achieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly 2\% better than any other open-source model on the HuggingFace Open LLM Leaderboard and becomes the first open-source LLM to surpass an average accuracy of 80\%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13228v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>直接偏好优化（DPO）在显著提高大型语言模型（LLM）在推理、总结和对齐等下游任务上的性能方面是有效的。DPO使用首选和分散数据对，对选择一个响应而不是另一个响应的相对概率进行建模。在这项工作中，我们首先从理论上证明，只要优选类和不优选类之间的相对概率增加，标准DPO损失就可以导致模型对优选示例的可能性降低。然后，我们根据经验表明，当在常见数据集上微调LLM时，尤其是在完成对之间的编辑距离较低的数据集上，会出现这种现象。利用这些见解，我们设计了DPO Positive（DPOP），这是一种新的损失函数和训练程序，可以避免这种失败模式。令人惊讶的是，我们还发现，在各种数据集和下游任务中，包括完成之间具有高编辑距离的数据集，DPOP显著优于DPO。通过对DPOP进行微调，我们创建并发布了Smaug-34B和Smaug-72B，它们实现了最先进的开源性能。值得注意的是，Smaug-72B比HuggingFace open LLM排行榜上的任何其他开源模型都要好近2%，成为第一个平均准确率超过80%的开源LLM。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13228v1" target="_blank">2402.13228v1</a>
                              </td>
                              <td>Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive</td>
                              <td>Arka Pal</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13228v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13228v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10184v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10184v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10184v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10184v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling, and meanwhile propose new, generalizable methods of analysis that have wider applications, including potentially shedding light on goal misgeneralization. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Based on this framework, we introduce a new method to model generalization in the reward modeling stage of RLHF, the induced Bayesian network (IBN). Drawing from random graph theory and causal analysis, it enables empirically grounded derivation of generalization error bounds, a key improvement over classical methods of generalization analysis. An insight from our analysis is the superiority of the tree-based information structure in reward modeling, compared to chain-based baselines in conventional RLHF methods. We derive that in complex contexts with limited data, the tree-based reward model (RM) induces up to $\Theta(\log n/\log\log n)$ times less variance than chain-based RM where $n$ is the dataset size. As validation, we demonstrate that on three NLP tasks, the tree-based RM achieves 65% win rate on average against chain-based baselines. Looking ahead, we hope to extend the IBN analysis to help understand the phenomenon of goal misgeneralization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10184v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从人类反馈中强化学习（RLHF）存在三重困境：高度多样化的上下文之间的不兼容性、低标记成本和可靠的对齐性能。在这里，我们的目标是通过在奖励建模过程中设计数据集信息结构来缓解这种不兼容性，同时提出了具有更广泛应用的新的、可推广的分析方法，包括可能揭示目标泛化错误。具体来说，我们首先重新审视了RLHF过程，并提出了一个理论框架，将其描述为文本分布上的自动编码过程。我们的框架形式化了RLHF的目标，即确保人类偏好和大型语言模型（LLM）行为之间的分布一致性。基于该框架，我们引入了一种在RLHF的奖励建模阶段进行建模泛化的新方法，即诱导贝叶斯网络（IBN）。它借鉴了随机图理论和因果分析，能够根据经验推导出泛化误差界，这是对经典泛化分析方法的关键改进。从我们的分析中可以看出，与传统RLHF方法中基于链的基线相比，基于树的信息结构在奖励建模中具有优势。我们得出，在数据有限的复杂环境中，基于树的奖励模型（RM）比基于链的RM（其中$n$是数据集大小）引起的方差小$\Theta（\logn/\log\logn）$倍。作为验证，我们证明了在三个NLP任务上，基于树的RM相对于基于链的基线平均实现了65%的胜率。展望未来，我们希望扩展IBN分析，以帮助理解目标泛化错误的现象。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10184v3" target="_blank">2402.10184v3</a>
                              </td>
                              <td>Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective</td>
                              <td>Tianyi Qiu</td>
                              <td>2024-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10184v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10184v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13225v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13225v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13225v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13225v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis. Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality. Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge. In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts. Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs. Manual evaluations show that RiskCalcs tools achieve an accuracy of over 80% on three quality metrics. At inference time, AgentMD can automatically select and apply the relevant RiskCalcs tools given any patient description. On the newly established RiskQA benchmark, AgentMD significantly outperforms chain-of-thought prompting with GPT-4 (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD to real-world clinical notes for analyzing both population-level and risk-level patient characteristics. In summary, our study illustrates the utility of language agents augmented with clinical calculators for healthcare analytics and patient care.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13225v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>临床计算器通过为各种目的（如预后）提供准确的循证预测，在医疗保健中发挥着至关重要的作用。然而，它们的广泛使用经常受到可用性挑战、传播不力和功能受限的阻碍。使用大量临床计算器增强大型语言模型提供了克服这些障碍和提高工作流程效率的机会，但手动管理过程的可扩展性带来了重大挑战。作为回应，我们介绍了AgentMD，这是一种新型的语言代理，能够在各种临床环境中管理和应用临床计算器。使用已发表的文献，AgentMD自动策划了2164个不同的临床计算器集合，这些计算器具有可执行功能和结构化文档，统称为RiskCalcs。手动评估显示，RiskCalcs工具在三个质量指标上的准确率超过80%。在推断时，AgentMD可以在给定任何患者描述的情况下自动选择并应用相关的RiskCalcs工具。在新建立的RiskQA基准上，AgentMD显著优于GPT-4的思维链提示（准确率分别为87.7%和40.9%）。此外，我们还将AgentMD应用于真实世界的临床笔记，以分析人群水平和风险水平的患者特征。总之，我们的研究说明了使用临床计算器增强语言代理在医疗分析和患者护理中的效用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13225v1" target="_blank">2402.13225v1</a>
                              </td>
                              <td>AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning</td>
                              <td>Qiao Jin</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13225v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13225v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13222v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13222v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13222v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13222v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language. However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language. Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs. Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English. In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem. The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models. Through our results and review of related works, we argue for the need to develop code models for languages other than English.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13222v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，大型语言模型（LLM）变得越来越强大，并且能够通过自然语言中的适当指令来解决大量任务。然而，绝大多数测试套件都假设指令是用英语编写的，英语是事实上的提示语言。即使对于最先进的LLM来说，代码智能和问题解决仍然是一项艰巨的任务。目前，没有数据集来衡量英语以外语言的代码生成模型的泛化能力。在这项工作中，我们展示了RoCode，这是一个有竞争力的编程数据集，由2642个用罗马尼亚语编写的问题、11k个用C、C++和Python编写的解决方案以及每个问题的全面测试套件组成。RoCode的目的是为评估在罗马尼亚语/多语言文本上训练的语言模型的代码智能提供一个基准，并为预训练的罗马尼亚语模型提供一个微调集。通过我们的研究结果和对相关工作的回顾，我们认为有必要为英语以外的语言开发代码模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13222v1" target="_blank">2402.13222v1</a>
                              </td>
                              <td>RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian</td>
                              <td>Adrian Cosma</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13222v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13222v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cosmaadrian/rocode" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13220v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13220v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13220v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13220v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 5% to 35%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13220v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多模式大型语言模型（MLLMs）的显著进步并没有使其免受挑战，特别是在处理提示中的欺骗性信息的背景下，从而在这种条件下产生幻觉反应。为了定量评估这种脆弱性，我们提出了MAD Bench，这是一个精心策划的基准，包含850个测试样本，分为6类，如不存在的物体、物体数量、空间关系和视觉混乱。我们对流行的MLLMs进行了全面分析，从GPT-4V、Gemini Pro到开源模型，如LLaVA-1.5和CogVLM。根据经验，我们观察到GPT-4V与其他模型之间存在显著的性能差距；以及以前稳健的指令调优模型，如LRV指令和LLaVA RLHF，在这个新的基准上并不有效。虽然GPT-4V在MAD平台上实现了75.02%的准确率，但我们实验中任何其他模型的准确率都在5%到35%之间。我们进一步提出了一种补救措施，在欺骗性提示中增加一段，以鼓励模型在回答问题之前三思而后行。令人惊讶的是，这种简单的方法甚至可以将精度提高一倍；然而，绝对数字仍然太低，不能令人满意。我们希望MAD Bench可以作为一个有价值的基准，促进进一步的研究，以增强模型对欺骗性提示的抵御能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13220v1" target="_blank">2402.13220v1</a>
                              </td>
                              <td>How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts</td>
                              <td>Yusu Qian</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13220v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13220v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06717v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Privacy Issues in Large Language Models: A Survey</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06717v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06717v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06717v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we have missed some of your work please contact us, as we will attempt to keep this survey relatively up to date. We are maintaining a repository with the list of papers covered in this survey and any relevant code that was publicly available at https://github.com/safr-ml-lab/survey-llm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06717v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这是对人工智能研究活跃领域的首次调查，重点关注大型语言模型（LLM）中的隐私问题。具体而言，我们专注于红队建模的工作，以突出隐私风险，试图将隐私纳入训练或推理过程，使经过训练的模型能够有效删除数据，以符合现有的隐私法规，并试图缓解版权问题。我们的重点是总结开发算法、证明定理和进行实证评估的技术研究。虽然有大量的法律和政策工作从不同的角度应对这些挑战，但这并不是我们调查的重点。尽管如此，这些工作以及最近的法律发展确实为这些技术问题的形式化提供了信息，因此我们在第1节中简要讨论了这些问题。虽然我们已经尽了最大努力将所有相关工作包括在内，但由于这项研究的快速发展，我们可能错过了最近的一些工作。如果我们错过了您的一些工作，请与我们联系，因为我们将努力使这项调查保持相对最新。我们正在维护一个存储库，其中包含本调查中涵盖的论文列表以及公开的任何相关代码https://github.com/safr-ml-lab/survey-llm.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06717v3" target="_blank">2312.06717v3</a>
                              </td>
                              <td>Privacy Issues in Large Language Models: A Survey</td>
                              <td>Seth Neel</td>
                              <td>2023-12-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06717v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06717v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/safr-ml-lab/survey-llm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13213v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13213v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13213v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13213v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&A task. For the six LLMs with the best Q&A performance, the AUROC derived from the MSP was better than random chance with p < 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of softmax probabilities and find similar (but not identical) results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13213v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型（LLM）在许多任务中表现出色，但过度自信仍然是一个问题。我们假设，在多项选择问答任务中，与正确答案相比，错误答案的最大软最大概率（MSP）较小。我们在10个开源LLM和5个数据集上对这一假设进行了全面评估，并在原始问答任务中表现良好的模型中为我们的假设找到了有力的证据。对于问答性能最好的六个LLM，在59/60个实例中，MSP得出的AUROC优于随机机会，p＜10^{-4}。在这六个LLM中，平均AUROC在60%到69%之间。利用这些发现，我们提出了一个多项选择问答任务，可以选择弃权，并表明可以通过基于初始模型响应的MSP选择性弃权来提高性能。我们还使用前softmax logits而不是softmax概率进行了相同的实验，并发现了相似（但不完全相同）的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13213v1" target="_blank">2402.13213v1</a>
                              </td>
                              <td>Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A</td>
                              <td>Benjamin Plaut</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13213v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13213v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/bplaut/softmax-probs-predict-llm-correctness" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13212v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Soft Self-Consistency Improves Language Model Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13212v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13212v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13212v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, Soft-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that Soft-SC can be applied to both open-source and black-box models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13212v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>通过对多个解决方案进行采样和评分以选择最终答案，可以改进大型语言模型（LLM）的生成。目前的“抽样选择”方法，如自我一致性（SC），依赖于多数投票来获得答案。然而，当任务有许多不同且有效的答案时，通过投票进行选择需要大量样本。这使得SC对于涉及按顺序生成多个动作（答案）的交互式任务来说过于昂贵。在确定多数投票无法在此类任务中提供一致的收益后，我们展示了如何通过软化评分标准来提高成功率。我们引入了软自洽性（Soft SC），它用根据模型可能性计算的连续得分取代了SC的不连续得分，即使在行动稀疏分布的情况下也可以进行选择。软SC提高了长期交互式任务的性能和效率，需要SC一半的样本才能获得相当或更好的性能。在固定数量的样本中，Soft SC在编写bash程序方面的绝对成功率比SC提高了1.3%，在网上购物（WebShop）方面提高了6.6%，在交互式家庭游戏（ALFWorld）方面增加了4.7%。最后，我们展示了Soft SC可以应用于开源和黑盒模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13212v1" target="_blank">2402.13212v1</a>
                              </td>
                              <td>Soft Self-Consistency Improves Language Model Agents</td>
                              <td>Han Wang</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13212v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13212v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/hannight/soft_self_consistency" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13211v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13211v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13211v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13211v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13211v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>情绪支持会话（ESC）是一项旨在通过日常会话缓解个人情绪困扰的任务。鉴于其固有的复杂性和非直观性，ESConv数据集结合了支持策略，以促进生成适当的响应。最近，尽管大型语言模型（LLM）具有非凡的会话能力，但先前的研究表明，它们往往难以提供有用的情感支持。因此，这项工作最初分析了LLM在ESConv上的结果，揭示了在选择正确策略方面的挑战以及对特定策略的显著偏好。受此启发，我们探讨了LLM中固有偏好对提供情感支持的影响，因此，我们观察到，对特定策略表现出高度偏好会阻碍有效的情感支持，从而增强其预测适当策略的稳健性。此外，我们进行了一项方法学研究，以深入了解LLM作为熟练情感支持者的必要方法。我们的研究结果强调，（1）对特定策略的低偏好阻碍了情感支持的进展，（2）外部援助有助于减少偏好偏见，（3）LLM本身无法成为良好的情感支持者。这些见解为未来的研究提供了有希望的途径，以提高LLM的情商。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13211v1" target="_blank">2402.13211v1</a>
                              </td>
                              <td>Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation</td>
                              <td>Dongjin Kang</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13211v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13211v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13210v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bayesian Reward Models for LLM Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13210v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13210v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13210v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback). However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimization in best-of-n sampling.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13210v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了确保大型语言模型（LLM）的响应是有益和无毒的，我们通常会根据人类偏好数据对奖励模型进行微调。然后，我们选择具有高回报的政策响应（最佳抽样），或进一步优化政策以产生具有高回报（从人类反馈中强化学习）的响应。然而，这个过程很容易受到奖励过度优化或黑客攻击的影响，在这种情况下，由于奖励模型中的错误而不是真正的偏好，所选择的响应具有高奖励。这是特别有问题的，因为提示或响应与训练数据不同。应该可以通过训练贝叶斯奖励模型来缓解这些问题，该模型进一步从训练数据分布中发出更高的不确定性信号。因此，我们使用拉普拉斯-LoRA（Yang et al.，2024）训练了贝叶斯奖励模型，并发现由此产生的不确定性估计可以成功地缓解最佳抽样中的奖励过度优化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13210v1" target="_blank">2402.13210v1</a>
                              </td>
                              <td>Bayesian Reward Models for LLM Alignment</td>
                              <td>Adam X. Yang</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13210v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13210v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13184v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13184v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13184v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13184v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this study, we introduce "CosmoAgent," an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLMs with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, crucial for preventing interstellar conflicts. We have also released the code and datasets to enable further academic investigation into this interesting area of research. The code is available at https://github.com/agiresearch/AlienAgent.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13184v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项研究中，我们介绍了“宇宙代理”，这是一个创新的人工智能框架，利用大型语言模型（LLM）来模拟人类和地外文明之间的复杂互动，特别强调斯蒂芬·霍金关于不要随意向宇宙中发送无线电信号的警告建议。目标是评估和平共处的可行性，同时考虑可能威胁善意文明的潜在风险。利用数学模型和状态转换矩阵，我们的方法定量评估了文明的发展轨迹，为未来在增长和饱和的关键点的决策提供了见解。此外，该论文承认宇宙中潜在生活条件的巨大多样性，这可能会在不同文明之间培养独特的宇宙学、伦理准则和世界观。认识到当前LLM设计中固有的以地球为中心的偏见，我们提出了一个新颖的概念，即使用具有不同伦理范式的LLM，并模拟具有不同道德原则的实体之间的互动。这项创新研究为理解复杂的文明间动态提供了一种新的方式，拓展了我们的视野，同时开创了解决冲突的新战略，这对防止星际冲突至关重要。我们还发布了代码和数据集，以便对这一有趣的研究领域进行进一步的学术调查。代码位于https://github.com/agiresearch/AlienAgent.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13184v1" target="_blank">2402.13184v1</a>
                              </td>
                              <td>What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents</td>
                              <td>Mingyu Jin</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13184v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13184v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13178v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Benchmarking Retrieval-Augmented Generation for Medicine</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13178v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13178v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13178v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the "lost-in-the-middle" effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13178v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型（LLM）在广泛的医学问答（QA）任务中取得了最先进的性能，但它们仍然面临幻觉和过时知识的挑战。检索增广生成（RAG）是一种很有前途的解决方案，已被广泛采用。然而，RAG系统可能涉及多个灵活的组件，并且缺乏关于用于各种医疗目的的最佳RAG设置的最佳实践。为了系统地评估这些系统，我们提出了医学信息检索增强生成评估（MIRAGE），这是第一个此类基准，包括来自五个医学QA数据集的7663个问题。使用MIRAGE，我们通过本工作中引入的MedRAG工具包，在41种不同语料库、检索器和骨干LLM的组合上进行了超过1.8万亿个提示令牌的大规模实验。总体而言，MedRAG通过思想链提示将六种不同LLM的准确性提高了18%，将GPT-3.5和Mixtral的性能提升到GPT-4水平。我们的结果表明，各种医学语料库和检索器的组合实现了最佳的性能。此外，我们还发现了医学RAG中的对数线性缩放特性和“中间丢失”效应。我们相信，我们的综合评估可以作为实施医学RAG系统的实用指南。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13178v1" target="_blank">2402.13178v1</a>
                              </td>
                              <td>Benchmarking Retrieval-Augmented Generation for Medicine</td>
                              <td>Guangzhi Xiong</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13178v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13178v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02073v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02073v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02073v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02073v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries. Within this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02073v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在利用上下文中提供的新颖信息方面具有令人印象深刻的能力。然而，这种上下文基础的机制仍然未知，尤其是在上下文信息与存储在参数中的事实知识相矛盾的情况下，LLM也擅长回忆这些事实知识。支持上下文信息对于检索增强生成方法至关重要，该方法用最新信息丰富上下文，希望基础能够纠正过时或嘈杂的存储知识。我们提出了一种使用Fakepedia研究基础能力的新方法，Fakepeda是一个反事实文本数据集，旨在与模型的内部参数知识相冲突。我们使用Fakepedia对各种LLM进行基准测试，然后在回答Fakepeda查询时，基于我们的掩蔽分组因果追踪（MGCT）对LLM组件进行因果中介分析。在该分析中，我们确定了接地和未接地响应之间的不同计算模式。我们最后证明，仅通过计算分析就可以区分接地响应和未接地响应。我们的研究结果，以及关于事实回忆机制的现有研究结果，为基础和事实回忆机制如何在LLM中相互作用提供了一个连贯的叙述。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02073v2" target="_blank">2312.02073v2</a>
                              </td>
                              <td>A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia</td>
                              <td>Giovanni Monea</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02073v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02073v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/epfl-dlab/llm-grounding-analysis" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09766v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09766v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09766v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09766v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in an reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more dependable evaluation protocols in the future.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09766v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对生成的文本内容的自动评估在NLP领域提出了一个持续的挑战。考虑到现代语言模型（LMs）在不同NLP任务中令人印象深刻的能力，在创建用于自动评估生成任务的创新评估指标时使用这些模型的趋势越来越大。本文研究了一个关键问题：语言模型驱动的评估指标是否固有地倾向于由相同的底层语言模型生成的文本？具体而言，我们评估了突出的基于LM的评估指标（如BARTScore、T5Score和GPTScore）是否在摘要任务的背景下对其各自的潜在LM表现出有利的偏见。我们的发现揭示了一种潜在的偏见，尤其是当以无参考的方式使用此类评估指标而不利用黄金摘要时。这些结果强调，生成性评估模型提供的评估可能受到固有文本质量之外的因素的影响，这突出了未来制定更可靠的评估协议的必要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09766v3" target="_blank">2311.09766v3</a>
                              </td>
                              <td>LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores</td>
                              <td>Yiqi Liu</td>
                              <td>2023-11-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09766v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09766v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13148v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Defending Jailbreak Prompts via In-Context Adversarial Game</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13148v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13148v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13148v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13148v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）展示了跨各种应用程序的卓越功能。然而，对他们的安全问题，特别是易受越狱袭击的问题仍然存在。从深度学习和LLM代理学习过程中的对抗性训练中汲取灵感，我们引入了上下文对抗游戏（ICAG），用于在不需要微调的情况下防御越狱。ICAG利用代理学习进行对抗性游戏，旨在动态扩展知识以防御越狱。与依赖于静态数据集的传统方法不同，ICAG采用迭代过程来增强防御和攻击代理。这个不断改进的过程加强了对新生成的越狱提示的防御。我们的实证研究证实了ICAG的有效性，在各种攻击场景中，由ICAG保护的LLM表现出显著降低的越狱成功率。此外，ICAG表现出显著的可转移性，表明其作为一种多功能防御机制的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13148v1" target="_blank">2402.13148v1</a>
                              </td>
                              <td>Defending Jailbreak Prompts via In-Context Adversarial Game</td>
                              <td>Yujun Zhou</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13148v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13148v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13146v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13146v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13146v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13146v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the Object Language Video Transformer (OLViT) - a novel model for video dialog operating over a multi-modal attention-based dialog state tracker. Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal reasoning, and accurate object tracking across multiple dialog turns. OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns. In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous multi-modal dialog state representations of the most relevant objects and rounds. As a result, they can be seamlessly integrated into Large Language Models (LLMs) and offer high flexibility in dealing with different datasets and tasks. Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13146v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了对象语言视频转换器（OLViT）——一种在基于多模式注意力的对话状态跟踪器上操作的视频对话的新模型。现有的视频对话模型很难解决需要在视频中进行空间和时间定位、长期时间推理以及在多个对话回合中准确跟踪对象的问题。OLViT通过基于对象状态跟踪器（OST）和语言状态跟踪器（LST）的输出来维护全局对话状态来解决这些挑战：当OST关注视频中最重要的对象时，LST跟踪对先前对话回合的最重要的语言共同引用。与之前的工作形成鲜明对比的是，我们的方法本质上是通用的，因此能够学习最相关对象和轮次的连续多模式对话状态表示。因此，它们可以无缝集成到大型语言模型（LLM）中，并在处理不同的数据集和任务时提供很高的灵活性。对具有挑战性的DVD（响应分类）和SIMMC 2.1（响应生成）数据集的评估表明，OLViT在这两个数据集上都实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13146v1" target="_blank">2402.13146v1</a>
                              </td>
                              <td>OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog</td>
                              <td>Adnen Abdessaied</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13146v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13146v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10669v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Humans or LLMs as the Judge? A Study on Judgement Biases</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10669v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10669v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10669v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Adopting human and large language models (LLM) as judges (\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework for investigating 5 types of biases for LLM and human judges. We curate a dataset with 142 samples referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of developing robust evaluation systems.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10669v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>采用人类和大型语言模型（LLM）作为评判（\textit｛a.k.a｝人类和LLM作为评判）来评估现有LLM的性能最近受到了关注。尽管如此，这种方法同时引入了来自人类和LLM法官的潜在偏见，质疑评估结果的可靠性。在本文中，我们提出了一个新的框架来调查LLM和人类法官的5种类型的偏见。我们参考修订后的Bloom分类法，整理了一个包含142个样本的数据集，并进行了数千次人类和LLM评估。结果表明，人类和LLM法官在不同程度上容易受到干扰，即使是最前沿的法官也有相当大的偏见。我们进一步利用他们的弱点，对LLM法官进行攻击。我们希望我们的工作能够向社区通报人类和LLM作为一名法官对扰动的脆弱性，以及开发强大评估系统的紧迫性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10669v2" target="_blank">2402.10669v2</a>
                              </td>
                              <td>Humans or LLMs as the Judge? A Study on Judgement Biases</td>
                              <td>Guiming Hardy Chen</td>
                              <td>2024-02-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10669v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10669v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03190v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unified Hallucination Detection for Multimodal Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03190v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03190v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03190v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03190v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在多模式任务方面取得了重大进展，但多模式大型语言模型（MLLMs）仍受到幻觉这一关键问题的困扰。因此，在MLLMs中可靠地检测这种幻觉已成为模型评估和实际应用部署保障的一个重要方面。先前在这一领域的研究受到了对单一任务的狭隘关注、所涉及的幻觉类别范围不足以及缺乏详细粒度的限制。为了应对这些挑战，我们的工作拓展了幻觉检测的研究视野。我们提出了一种新的元评估基准，MHaluBench，它经过精心制作，有助于评估幻觉检测方法的进展。此外，我们还推出了一种新的统一多模式幻觉检测框架UNIHD，该框架利用一套辅助工具来有力地验证幻觉的发生。我们通过细致的评估和全面的分析，展示了UNIHD的有效性。我们还提供了关于解决各类幻觉的特定工具应用的战略见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03190v3" target="_blank">2402.03190v3</a>
                              </td>
                              <td>Unified Hallucination Detection for Multimodal Large Language Models</td>
                              <td>Xiang Chen</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03190v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03190v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/openkg-org/easydetect" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13125v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13125v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13125v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13125v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce $\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around $45$ questions. We also conduct more analysis to show the robustness and reliability of TreeEval. Our code can be accessed via the provided https://github.com/Ashura5/TreeEval.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13125v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，已经建立了许多新的基准，通过计算整体得分或使用另一个LLM作为评判来评估大型语言模型（LLM）的性能。然而，由于基准的开放访问和不灵活的评估过程，这些方法存在数据泄露的问题。为了解决这个问题，我们引入了$\textbf｛TreeEval｝$，这是一种LLM的无基准评估方法，它使高性能LLM能够主持不可复制的评估会话，并从根本上避免了数据泄露。此外，该LLM作为审查员，以树规划策略在一个主题下提出一系列问题，考虑当前评估状态来决定下一个问题生成，并确保评估过程的完整性和效率。我们评估了不同参数大小的6$模型，包括7$B、13$B和33$B，并最终仅使用约45$的问题就实现了与AlpacaEval2.0的最高相关系数。我们还进行了更多的分析，以显示TreeEval的稳健性和可靠性。我们的代码可以通过提供的https://github.com/Ashura5/TreeEval.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13125v1" target="_blank">2402.13125v1</a>
                              </td>
                              <td>TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning</td>
                              <td>Xiang Li</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13125v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13125v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ashura5/treeeval" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_09868v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">INTERVENOR: Prompting the Coding Ability of Large Language Models with the Interactive Chain of Repair</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_09868v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_09868v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_09868v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces INTERVENOR (INTERactiVE chaiN Of Repair), a system designed to emulate the interactive code repair processes observed in humans, encompassing both code diagnosis and code repair. INTERVENOR prompts Large Language Models (LLMs) to play distinct roles during the code repair process, functioning as both a Code Learner and a Code Teacher. Specifically, the Code Learner is tasked with adhering to instructions to generate or repair code, while the Code Teacher is responsible for crafting a Chain-of-Repair (CoR) to serve as guidance for the Code Learner. During generating the CoR, the Code Learner needs to check the generated codes from Code Learner and reassess how to address code bugs based on error feedback received from compilers. Experimental results demonstrate that INTERVENOR surpasses baseline models, exhibiting improvements of approximately 18% and 4.3% over GPT-3.5 in code generation and code translation tasks, respectively. Our further analyses show that CoR is effective to illuminate the reasons behind bugs and outline solution plans in natural language. With the feedback of code compilers, INTERVENOR can accurately identify syntax errors and assertion errors and provide precise instructions to repair codes. All data and codes are available at https://github.com/NEUIR/INTERVENOR</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_09868v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了INTERVENOR（INTERactiVE chaiN Of Repair），这是一个旨在模拟在人类中观察到的交互式代码修复过程的系统，包括代码诊断和代码修复。INTERVENOR提示大型语言模型（LLM）在代码修复过程中扮演不同的角色，既是代码学习者又是代码教师。具体而言，代码学习者的任务是遵守指令生成或修复代码，而代码教师则负责制定修复链（CoR），作为代码学习者指南。在生成CoR期间，代码学习者需要检查代码学习者生成的代码，并根据从编译器收到的错误反馈重新评估如何解决代码错误。实验结果表明，INTERVENOR超过了基线模型，在代码生成和代码翻译任务中分别比GPT-3.5提高了约18%和4.3%。我们的进一步分析表明，CoR可以有效地阐明错误背后的原因，并用自然语言概述解决方案计划。在代码编译器的反馈下，INTERVENOR可以准确识别语法错误和断言错误，并提供精确的代码修复指令。所有数据和代码可在https://github.com/NEUIR/INTERVENOR</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.09868v4" target="_blank">2311.09868v4</a>
                              </td>
                              <td>INTERVENOR: Prompting the Coding Ability of Large Language Models with the Interactive Chain of Repair</td>
                              <td>Hanbin Wang</td>
                              <td>2023-11-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_09868v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.09868v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/neuir/intervenor" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13116v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Knowledge Distillation of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13116v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13116v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13116v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance. By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in knowledge distillation and proposing future research directions. By bridging the gap between proprietary and open-source LLMs, this survey underscores the potential for more accessible, efficient, and sustainable AI solutions, fostering a more inclusive and equitable landscape in AI advancements. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13116v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项调查深入探讨了大型语言模型（LLM）领域内的知识提取（KD）技术，突出了KD在将复杂功能从GPT-4等专有巨头转移到LLaMA和Mistral等可访问的开源模型方面的关键作用。在不断发展的人工智能环境中，这项工作阐明了专有LLM和开源LLM之间的关键差异，展示了KD如何成为向后者灌输前者先进功能和细微理解的重要渠道。我们的调查围绕三个基本支柱精心构建：算法、技能和垂直化——对KD机制、特定认知能力的增强及其在不同领域的实际意义进行了全面的研究。至关重要的是，该调查揭示了数据增强（DA）和KD之间复杂的相互作用，说明了DA如何在KD框架内成为一种强大的范式，以提高LLM的性能。通过利用DA生成丰富的上下文、特定技能的培训数据，KD超越了传统的界限，使开源模型能够接近其专有模型的上下文熟练度、道德一致性和深度语义洞察力。这项工作旨在为研究人员和从业者提供一个有见地的指南，详细概述当前知识提炼的方法，并提出未来的研究方向。通过弥合专有LLM和开源LLM之间的差距，这项调查强调了更容易获得、高效和可持续的人工智能解决方案的潜力，促进了人工智能进步的包容性和公平性。相关的Github存储库位于https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13116v1" target="_blank">2402.13116v1</a>
                              </td>
                              <td>A Survey on Knowledge Distillation of Large Language Models</td>
                              <td>Xiaohan Xu</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13116v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13116v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tebmer/awesome-knowledge-distillation-of-llms" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13109v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13109v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13109v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13109v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our evaluation of 28 selected LLMs reveals a noticeable performance gap, with the best model scoring only 52.9%, highlighting the limitations of LLMs in less familiar language and task contexts. This work aims to uncover the current limitations of LLMs in handling Chinese tasks, pushing towards the development of more culturally informed and linguistically diverse models with the released data and benchmark (https://yizhilll.github.io/CIF-Bench/).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13109v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的进步增强了通过指令跟随在各种看不见的自然语言处理（NLP）任务中进行泛化的能力。然而，在中文等资源匮乏的语言中，它们的有效性往往会降低，数据泄露带来的偏见评估加剧了这种情况，使人们怀疑它们在新的语言领域的真实可推广性。作为回应，我们介绍了汉语教学跟踪基准（CIF-Bench），旨在评估LLM对汉语的零样本可推广性。CIF Bench由150个任务和15000个输入输出对组成，由母语人士开发，用于测试20个类别的复杂推理和中国文化的细微差别。为了减轻评估偏差，我们只公开发布了一半的数据集，其余的数据集保密，并引入了多样化的指令来最大限度地减少得分差异，共有45000个数据实例。我们对28个选定的LLM的评估显示出明显的性能差距，最佳模型的得分仅为52.9%，这突出了LLM在不太熟悉的语言和任务环境中的局限性。这项工作旨在揭示LLM在处理中文任务方面的当前局限性，利用已发布的数据和基准，推动开发更具文化信息和语言多样性的模型(https://yizhilll.github.io/CIF-Bench/).</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13109v1" target="_blank">2402.13109v1</a>
                              </td>
                              <td>CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models</td>
                              <td>Yizhi LI</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13109v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13109v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13098v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ELAD: Explanation-Guided Large Language Models Active Distillation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13098v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13098v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13098v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the student model's reasoning. Our experiments across various reasoning datasets demonstrate that our framework significantly enhances the efficiency of LLM knowledge distillation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13098v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的部署和应用受到其内存效率低、计算需求和API推理成本高的阻碍。传统的蒸馏方法将LLM的能力转移到较小的模型中，往往无法确定知识是否已经充分转移，这可能导致高成本或蒸馏不完全。在本文中，我们提出了一种解释引导的LLMs主动蒸馏（ELAD）框架，该框架采用主动学习策略来优化注释成本和模型性能之间的平衡。为了提高有效的样本选择，我们引入了一种解释引导的样本选择方法，该方法通过利用解释步骤中的不确定性来识别挑战其推理的样本。此外，我们提出了一种定制的LLM注释解释修订技术，其中教师模型检测并纠正学生模型推理中的缺陷。我们在各种推理数据集上的实验表明，我们的框架显著提高了LLM知识提取的效率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13098v1" target="_blank">2402.13098v1</a>
                              </td>
                              <td>ELAD: Explanation-Guided Large Language Models Active Distillation</td>
                              <td>Yifei Zhang</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13098v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13098v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13093v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Event-level Knowledge Editing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13093v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13093v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13093v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies. We systematically evaluate the performance of various knowledge editing methods and LLMs on this benchmark. We find that ELKEN poses significant challenges to existing knowledge editing approaches. Our codes and dataset are publicly released to facilitate further research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13093v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>知识编辑旨在更新大型语言模型的知识，防止它们过时。现有工作在事实知识三元组的水平上编辑LLM。然而，现实世界中的自然知识更新来自于新事件的发生，而不是事实三元组的直接变化。在本文中，我们提出了一种新的任务设置：事件级知识编辑，它将新的事件直接编辑为LLM，并在（1）效率方面优于传统的三元组级编辑。单个事件编辑会导致多个包含的知识三元组的更新。（2） 完整性。除了更新事实知识外，事件级编辑还需要考虑事件的影响，并更新LLM对未来趋势的知识。我们构建了一个高质量的事件级编辑基准ELKEN，包括1515个事件编辑、6449个关于事实知识的问题和10150个关于未来趋势的问题。我们在此基准上系统地评估了各种知识编辑方法和LLM的性能。我们发现ELKEN对现有的知识编辑方法提出了重大挑战。我们的代码和数据集是公开发布的，以便于进一步研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13093v1" target="_blank">2402.13093v1</a>
                              </td>
                              <td>Event-level Knowledge Editing</td>
                              <td>Hao Peng</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13093v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13093v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/thu-keg/event-level-knowledge-editing" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13088v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Slot-VLM: SlowFast Slots for Video-Language Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13088v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13088v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13088v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure. The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information. Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13088v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在大型语言模型（LLM）的进步的推动下，视频语言模型（VLM）正在为视频理解开辟新的领域。一个关键的挑战是开发一种有效的方法，将视频内容封装到一组具有代表性的令牌中，以与LLM保持一致。在这项工作中，我们介绍了Slot VLM，这是一种新的框架，旨在根据对象和事件的视觉表示生成语义分解的视频令牌，以促进LLM推理。特别地，我们设计了一个SlowFast Slots模块，即SF Slots，它自适应地将来自CLIP视觉编码器的密集视频令牌聚合到一组具有代表性的时隙。为了同时考虑空间对象细节和变化的时间动力学，SF Slots采用双分支结构。慢速槽分支专注于从高空间分辨率但低（慢）帧采样率的特征中提取以对象为中心的槽，强调详细的对象信息。相反，Fast Slots分支被设计为从高时间采样率但低空间分辨率的特征中学习以事件为中心的时隙。这些互补的插槽组合在一起形成视觉上下文，作为LLM的输入，实现高效的问答。我们的实验结果证明了我们的插槽VLM的有效性，它在视频问答方面实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13088v1" target="_blank">2402.13088v1</a>
                              </td>
                              <td>Slot-VLM: SlowFast Slots for Video-Language Modeling</td>
                              <td>Jiaqi Xu</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13088v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13088v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_06692v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_06692v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_06692v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_06692v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on pre-defined task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose GeM-CoT, a Generalizable CoT prompting mechanism in Mixed-task scenarios where the type of input questions is unknown. GeM-CoT first categorizes the question type and subsequently samples or constructs demonstrations from the corresponding data pool in an automatic pattern. With this technical design, GeM-CoT simultaneously enjoys superior generalization capabilities and remarkable performances on 10 public reasoning tasks and 23 BBH tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_06692v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）通过利用思维链（CoT）提示揭示了非凡的推理能力，它生成中间推理链作为推导答案的基本原理。然而，当前的CoT方法要么简单地使用一般提示，如“让我们一步一步地思考”，要么严重依赖预定义的特定任务演示来获得更好的性能，从而在性能和泛化之间产生不可避免的差距。为了弥补这一差距，我们提出了GeM-CoT，这是一种在输入问题类型未知的混合任务场景中的可推广CoT提示机制。GeM-CoT首先对问题类型进行分类，然后以自动模式从相应的数据池中采样或构建演示。通过这种技术设计，GeM-CoT同时在10个公共推理任务和23个BBH任务上具有卓越的泛化能力和显著的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.06692v3" target="_blank">2310.06692v3</a>
                              </td>
                              <td>Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models</td>
                              <td>Anni Zou</td>
                              <td>2023-10-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_06692v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.06692v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/anni-zou/meta-cot" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07859v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lissard: Long and Simple Sequential Reasoning Datasets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07859v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07859v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07859v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at https://github.com/unicamp-dl/Lissard</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07859v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言模型现在能够解决需要处理由数十万个标记组成的长序列的任务。然而，他们经常在需要重复使用简单规则的任务上失败，即使是在比训练中看到的序列短得多的序列上。例如，最先进的LLM可以在两个最多有20个项目的列表中找到常见项目，但当列表有80个项目时失败。在本文中，我们介绍了Lissard，这是一个由七个任务组成的基准，其目标是评估模型处理和生成宽范围序列长度的能力，需要重复的过程执行。我们对开源（Mistral-7B和Mixtral-8x7B）和专有模型（GPT-3.5和GPT-4）的评估显示，随着序列复杂性的增加，所有模型的性能都在持续下降。数据集和代码可在https://github.com/unicamp-dl/Lissard</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07859v2" target="_blank">2402.07859v2</a>
                              </td>
                              <td>Lissard: Long and Simple Sequential Reasoning Datasets</td>
                              <td>Mirelle Bueno</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07859v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07859v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_07251v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">In-Contextual Gender Bias Suppression for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_07251v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_07251v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_07251v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of LLMs, which are computationally costly. Moreover, one might not even have access to the model parameters for performing debiasing such as in the case of closed LLMs such as GPT-4. To address this challenge, we propose bias suppression that prevents biased generations of LLMs by simply providing textual preambles constructed from manually designed templates and real-world statistics, without accessing to model parameters. We show that, using CrowsPairs dataset, our textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2. Moreover, we find that gender-neutral descriptions of gender-biased objects can also suppress their gender biases. Moreover, we show that bias suppression has acceptable adverse effect on downstream task performance with HellaSwag and COPA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_07251v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型在广泛的NLP任务中表现出色，但据报道，它们编码了令人担忧的性别偏见水平。先前的工作已经提出了去偏方法，这些方法需要人工标记的例子、数据扩充和LLM的微调，这在计算上是昂贵的。此外，甚至可能无法访问用于执行去偏的模型参数，例如在诸如GPT-4的闭合LLM的情况下。为了应对这一挑战，我们提出了偏差抑制，通过简单地提供由手动设计的模板和真实世界的统计数据构建的文本前导码，而不访问模型参数，来防止LLM的偏差生成。我们表明，使用CrowsPairs数据集，我们覆盖反事实陈述的文本序言可以抑制英语LLM（如LLaMA2）中的性别偏见。此外，我们发现对性别偏见对象的中性描述也可以抑制其性别偏见。此外，我们还表明，使用HellaSwag和COPA，偏差抑制对下游任务性能有可接受的不利影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.07251v2" target="_blank">2309.07251v2</a>
                              </td>
                              <td>In-Contextual Gender Bias Suppression for Large Language Models</td>
                              <td>Daisuke Oba</td>
                              <td>2023-09-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_07251v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.07251v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10949v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The Unreasonable Effectiveness of Eccentric Automatic Prompts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10949v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10949v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10949v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus computation time, of experimenting with hand-tuning prompts for large black-box models, we then compared the performance of the best "positive thinking" prompt against the output of systematic prompt optimization. We show that employing an automated prompt optimizer emerges as the most effective method for enhancing performance, even when working with smaller open-source models. Additionally, our findings reveal that the highest-scoring, automatically-optimized prompt exhibits a degree of peculiarity far beyond expectations.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10949v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）已经显示出非凡的解决问题和基本数学能力。然而，它们的功效在很大程度上取决于提示的配方。本研究试图量化将“积极思维”纳入提示的系统信息的影响，并将其与系统提示优化进行比较。我们在GSM8K数据集上评估了60种系统消息片段组合的性能，这些组合在有思想链提示和没有思想链提示的情况下进行了测试，涉及三个参数从70亿到700亿不等的模型。我们的研究结果表明，结果并不能在模型中普遍推广。在大多数情况下，“积极思考”的加入会促使受到积极影响的模型性能。然而，值得注意的是，Llama2-70B在不使用思想链时表现出异常，因为发现最优系统消息根本没有。考虑到对大型黑匣子模型进行手动调整提示实验的组合复杂性和计算时间，我们将最佳“积极思考”提示的性能与系统提示优化的输出进行了比较。我们表明，即使在使用较小的开源模型时，使用自动提示优化器也是提高性能的最有效方法。此外，我们的研究结果表明，得分最高的自动优化提示表现出远超预期的特殊程度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10949v2" target="_blank">2402.10949v2</a>
                              </td>
                              <td>The Unreasonable Effectiveness of Eccentric Automatic Prompts</td>
                              <td>Rick Battle</td>
                              <td>2024-02-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10949v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10949v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13064v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13064v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13064v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13064v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13064v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了广义指令调优（GLAN），这是一种用于大型语言模型（LLM）指令调优的通用且可扩展的方法。与之前依靠种子示例或现有数据集构建教学调整数据的工作不同，GLAN专门利用预先策划的人类知识和能力分类法作为输入，并生成所有学科的大规模合成教学数据。具体而言，受人类教育系统中系统结构的启发，我们在LLM的推动下，通过半自动地将人类知识和能力分解为各个领域、子领域，最终分解为不同的学科，来构建分类法。随后，我们为每个学科生成一个全面的科目列表，并继续设计针对每个科目的教学大纲，再次使用LLM。通过在教学大纲的每一节课上详细介绍细粒度的关键概念，我们能够生成涵盖人类知识和技能的各种指令。在大型语言模型（如Mistral）上进行的大量实验表明，GLAN在从数学推理、编码、学术考试、逻辑推理到一般教学遵循的多个维度上都表现出色，而无需使用这些任务的特定任务训练数据。此外，GLAN允许轻松定制，只需将一个新节点合并到我们的分类法中，就可以添加新的字段或技能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13064v1" target="_blank">2402.13064v1</a>
                              </td>
                              <td>Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models</td>
                              <td>Haoran Li</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13064v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13064v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_12030v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_12030v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_12030v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_12030v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families. In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread use of distillation techniques.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_12030v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于成本、延迟限制和硬件可访问性等限制，在大多数工业用例中部署数十亿个参数的大型语言模型（LLM）可能不切实际。知识蒸馏（KD）通过将知识从资源密集型的大型模型压缩到较小的模型来提供解决方案。存在各种策略，其中一些依赖于教师模型生成的文本，并选择性地利用他的逻辑来增强学习。然而，这些基于logits的方法通常要求教师和学生模型共享相同的标记器，这限制了它们在不同LLM家族中的适用性。在本文中，我们引入了基于最优运输的通用物流蒸馏（ULD）损失，以解决这一限制。我们的实验结果证明了ULD损失在实现具有不同架构和标记器的模型之间的蒸馏方面的有效性，为蒸馏技术的更广泛使用铺平了道路。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.12030v2" target="_blank">2402.12030v2</a>
                              </td>
                              <td>Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs</td>
                              <td>Nicolas Boizard</td>
                              <td>2024-02-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_12030v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.12030v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13055v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Identifying Semantic Induction Heads to Understand In-Context Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13055v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13055v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13055v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our understanding of the intricate operations of attention heads in transformers, and further provides new insights into the in-context learning of LLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13055v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管大型语言模型（LLM）表现出了显著的性能，但其推理逻辑缺乏透明度，这引发了人们对其可信度的担忧。为了更好地了解LLM，我们对注意力头的操作进行了详细的分析，旨在更好地理解LLM的上下文学习。具体来说，我们研究了注意力头是否编码了自然语言中存在的表征之间的两种类型的关系：从句子中解析的句法依赖关系和知识图中的关系。我们发现，某些注意力头部表现出一种模式，当关注头部标记时，它们会回忆起尾部标记，并增加这些尾部标记的输出logits。更重要的是，这种语义归纳头的形成与语言模型的上下文学习能力的出现密切相关。语义注意头的研究促进了我们对注意头在转换器中复杂操作的理解，并进一步为LLM的上下文学习提供了新的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13055v1" target="_blank">2402.13055v1</a>
                              </td>
                              <td>Identifying Semantic Induction Heads to Understand In-Context Learning</td>
                              <td>Jie Ren</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13055v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13055v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13048v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Stable Knowledge Editing in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13048v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13048v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13048v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Efficient knowledge editing of large language models is crucial for replacing obsolete information or incorporating specialized knowledge on a large scale. However, previous methods implicitly assume that knowledge is localized and isolated within the model, an assumption that oversimplifies the interconnected nature of model knowledge. The premise of localization results in an incomplete knowledge editing, whereas an isolated assumption may impair both other knowledge and general abilities. It introduces instability to the performance of the knowledge editing method. To transcend these assumptions, we introduce StableKE, a method adopts a novel perspective based on knowledge augmentation rather than knowledge localization. To overcome the expense of human labeling, StableKE integrates two automated knowledge augmentation strategies: Semantic Paraphrase Enhancement strategy, which diversifies knowledge descriptions to facilitate the teaching of new information to the model, and Contextual Description Enrichment strategy, expanding the surrounding knowledge to prevent the forgetting of related information. StableKE surpasses other knowledge editing methods, demonstrating stability both edited knowledge and multi-hop knowledge, while also preserving unrelated knowledge and general abilities. Moreover, StableKE can edit knowledge on ChatGPT.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13048v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型的有效知识编辑对于大规模替换过时信息或整合专业知识至关重要。然而，以前的方法隐含地假设知识在模型中是本地化和孤立的，这一假设过于简化了模型知识的相互关联性。本地化的前提导致知识编辑不完整，而孤立的假设可能会损害其他知识和一般能力。它给知识编辑方法的性能带来了不稳定性。为了超越这些假设，我们引入了StableKE，这是一种基于知识扩充而非知识本地化的新颖方法。为了克服人为标注的成本，StableKE集成了两种自动知识增强策略：语义短语增强策略，它使知识描述多样化，以便于向模型教授新信息；上下文描述增强策略，扩展周围的知识，以防止相关信息的遗忘。StableKE超越了其他知识编辑方法，展示了编辑知识和多跳知识的稳定性，同时也保留了无关知识和一般能力。此外，StableKE可以在ChatGPT上编辑知识。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13048v1" target="_blank">2402.13048v1</a>
                              </td>
                              <td>Stable Knowledge Editing in Large Language Models</td>
                              <td>Zihao Wei</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13048v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13048v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06770v4_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06770v4_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06770v4_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06770v4_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) offer significant promise as a knowledge source for task learning. Prompt engineering has been shown to be effective for eliciting knowledge from an LLM, but alone it is insufficient for acquiring relevant, situationally grounded knowledge for an embodied agent learning novel tasks. We describe a cognitive-agent approach, STARS, that extends and complements prompt engineering, mitigating its limitations and thus enabling an agent to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences. The STARS approach is to increase the response space of LLMs and deploy general strategies, embedded within the autonomous agent, to evaluate, repair, and select among candidate responses produced by the LLM. We describe the approach and experiments that show how an agent, by retrieving and evaluating a breadth of responses from the LLM, can achieve 77-94% task completion in one-shot learning without user oversight. The approach achieves 100% task completion when human oversight (such as an indication of preference) is provided. Further, the type of oversight largely shifts from explicit, natural language instruction to simple confirmation/discomfirmation of high-quality responses that have been vetted by the agent before presentation to a user.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06770v4_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）作为任务学习的知识来源具有重要的前景。即时工程已被证明对从LLM中获取知识是有效的，但仅凭它不足以为具体代理学习新任务获取相关的、基于情境的知识。我们描述了一种认知代理方法STARS，它扩展和补充了即时工程，减轻了其局限性，从而使代理能够获得与其母语能力、实施方式、环境和用户偏好相匹配的新任务知识。STARS方法是增加LLM的响应空间，并部署嵌入自主代理中的通用策略，以评估、修复LLM产生的候选响应并从中进行选择。我们描述了一种方法和实验，这些方法和实验表明，通过检索和评估LLM的广泛响应，代理可以在没有用户监督的情况下，在一次学习中完成77-94%的任务。当提供人工监督（例如偏好指示）时，该方法实现了100%的任务完成。此外，监督类型在很大程度上从明确的自然语言指令转变为对高质量响应的简单确认/不确认，这些响应在呈现给用户之前已经由代理进行了审查。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06770v4" target="_blank">2306.06770v4</a>
                              </td>
                              <td>Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis</td>
                              <td>James R. Kirk</td>
                              <td>2023-06-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06770v4_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06770v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13043v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13043v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13043v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13043v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We validate our retrieval approach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The experimental results show a significant improvement over relevant baselines in real few-shot DST settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13043v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>使用大型语言模型（LLM）的少镜头对话状态跟踪（DST）依赖于一个有效的会话检索器来找到类似的上下文示例，以进行即时学习。以前的工作使用原始对话上下文作为搜索键和查询，检索器使用带注释的对话进行微调，以实现卓越的性能。然而，这种方法不太适合扩展到新的域或新的注释语言，因为在这些域或语言中无法微调数据。为了解决这个问题，我们处理了基于会话文本摘要的会话检索任务。采用基于LLM的会话汇总器进行查询和密钥生成，实现了有效的最大内部产品搜索。为了避免基于LLM的会话摘要带来的额外推理成本，我们进一步提取了一种轻量级的会话编码器，该编码器在不解码测试会话摘要的情况下生成查询嵌入。我们使用GPT-Neo-2.7B和LLaMA-7B/30B在MultiWOZ数据集上验证了我们的检索方法。实验结果表明，在实际的少炮DST设置中，与相关基线相比有了显著的改进。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13043v1" target="_blank">2402.13043v1</a>
                              </td>
                              <td>Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries</td>
                              <td>Seanie Lee</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13043v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13043v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08189v3_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PRewrite: Prompt Rewriting with Reinforcement Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08189v3_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08189v3_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08189v3_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion that can be time consuming, ineffective, and sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these problems, we investigate automated prompt engineering in this paper. Specifically, we propose PRewrite, an automated method to rewrite an under-optimized prompt to a more effective prompt. We instantiate the prompt rewriter using a LLM. The rewriter LLM is trained using reinforcement learning to optimize the performance on a given downstream task. We conduct experiments on diverse benchmark datasets, which demonstrates the effectiveness of PRewrite.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08189v3_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>快速工程对于基于LLM的应用程序的开发至关重要。然而，它通常是以“试错”的方式手动完成的，这可能会耗时、无效和次优。即使对于看似效果良好的提示，也总是有一个挥之不去的问题：通过进一步的修改，提示能变得更好吗？为了解决这些问题，我们在本文中研究了自动提示工程。具体来说，我们提出了PRewrite，这是一种将优化不足的提示重写为更有效提示的自动化方法。我们使用LLM实例化提示重写器。使用强化学习来训练重写器LLM，以优化给定下游任务的性能。我们在不同的基准数据集上进行了实验，这证明了PRewrite的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08189v3" target="_blank">2401.08189v3</a>
                              </td>
                              <td>PRewrite: Prompt Rewriting with Reinforcement Learning</td>
                              <td>Weize Kong</td>
                              <td>2024-01-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08189v3_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08189v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13036v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SiLLM: Large Language Models for Simultaneous Machine Translation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13036v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13036v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13036v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Simultaneous Machine Translation (SiMT) generates translations while reading the source sentence, necessitating a policy to determine the optimal timing for reading and generating words. Despite the remarkable performance achieved by Large Language Models (LLM) across various NLP tasks, existing SiMT methods predominantly focus on conventional transformers, employing a single model to concurrently determine the policy and generate the translations. However, given the complexity of SiMT, it is challenging to effectively address both tasks with a single model. Therefore, there is a need to decouple the SiMT task into policy-decision and translation sub-tasks. We propose SiLLM, which delegates the two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The policy-decision agent is managed by a conventional SiMT model, responsible for determining the translation policy. The translation agent, leveraging the capabilities of LLM, generates translation using the partial source sentence. The two agents collaborate to accomplish SiMT. To facilitate the application of token-level policies determined by conventional SiMT models to LLM, we propose a word-level policy adapted for LLM. Experiments on two datasets demonstrate that, with a small amount of data for fine-tuning LLM, SiLLM attains state-of-the-art performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13036v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器翻译（SiMT）在阅读源句子的同时生成翻译，这就需要制定一项政策来确定阅读和生成单词的最佳时机。尽管大型语言模型（LLM）在各种NLP任务中取得了显著的性能，但现有的SiMT方法主要集中在传统的转换器上，使用单个模型来同时确定策略和生成翻译。然而，考虑到SiMT的复杂性，用单个模型有效地处理这两个任务是具有挑战性的。因此，需要将SiMT任务解耦为策略决策和翻译子任务。我们提出了SiLLM，它将两个子任务委托给单独的代理，从而将LLM合并到SiMT中。策略决策代理由传统的SiMT模型管理，负责确定翻译策略。翻译代理利用LLM的功能，使用部分源语句生成翻译。两个代理协作完成SiMT。为了便于将传统SiMT模型确定的令牌级策略应用于LLM，我们提出了一种适用于LLM的单词级策略。在两个数据集上的实验表明，通过少量的数据微调LLM，SiLLM达到了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13036v1" target="_blank">2402.13036v1</a>
                              </td>
                              <td>SiLLM: Large Language Models for Simultaneous Machine Translation</td>
                              <td>Shoutao Guo</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13036v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13036v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ictnlp/sillm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13035v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13035v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13035v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13035v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need for external feedback or ground truth labels to ascertain the endpoint of correction. We compare the performance of models fine-tuned with the ``Step CoT Check'' prompt against those refined using other promps within the context of checking-correction data. The ``Step CoT Check'' outperforms the other two check formats in model with lager parameters, providing more precise feedback thus achieving a higher rate of correctness. For reproducibility, all the datasets and codes are provided in \url{https://github.com/bammt/Learn-to-check}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13035v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）在推理能力方面取得了重大进展，正在努力通过自我纠正来完善其推理。然而，最近的研究表明，如果没有外部准确的知识，自我纠正可能会受到限制，甚至适得其反，这引发了人们对自我纠正的局限性和有效性的质疑。在本文中，我们旨在通过精心设计训练数据来增强LLM的自检能力，从而提高自校正的准确性。我们对数学推理中的错误类型进行了详细分析，并开发了一个定制的提示，称为“步骤CoT检查”。然后，我们为训练模型构建了一个校验校正数据集。在整合原始CoT数据和检查校正数据进行训练后，我们观察到模型可以提高其自检能力，从而增强其自校正能力，并消除了对外部反馈或基本事实标签的需求，以确定校正的终点。我们将使用“Step-CoT Check”提示微调的模型的性能与在检查校正数据的背景下使用其他提示优化的模型进行比较。在具有较大参数的模型中，“步骤CoT检查”优于其他两种检查格式，提供了更精确的反馈，从而实现了更高的正确率。为了再现性，所有数据集和代码都在\url中提供{https://github.com/bammt/Learn-to-check}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13035v1" target="_blank">2402.13035v1</a>
                              </td>
                              <td>Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models</td>
                              <td>Che Zhang</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13035v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13035v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/bammt/learn-to-check" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_05926v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Convergence of Zeroth-Order Federated Tuning for Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_05926v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_05926v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_05926v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on clients with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we term as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as FedAvg but also significantly reduces GPU memory usage during training to levels comparable to those during inference. Moreover, the proposed personalized FL strategy that is built upon the theoretical insights to customize the client-wise learning rate can effectively accelerate loss reduction. We hope our work can help to bridge theoretical and practical aspects of federated fine-tuning for LLMs, thereby stimulating further advancements and research in this area.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_05926v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>联合学习（FL）和大型语言模型（LLM）的融合正在开创一个保护隐私的自然语言处理的新时代。然而，微调LLM的密集内存需求带来了重大挑战，尤其是在计算资源有限的客户端上部署时。为了避免这种情况，我们探索了在联邦设置中集成高效内存的零阶优化的新方法，我们称之为FedMeZO。我们的研究首次在LLM的背景下检验了FedMeZO的理论基础，解决了大参数空间对优化行为的影响、收敛特性的建立以及收敛关键参数的识别等关键问题，为个性化联邦策略提供信息。我们广泛的经验证据支持了这一理论，表明FedMeZO不仅比传统的一阶方法（如FedAvg）收敛得更快，而且还将训练期间的GPU内存使用量显著降低到与推理期间相当的水平。此外，所提出的个性化FL策略建立在定制客户学习率的理论见解的基础上，可以有效地加速减少损失。我们希望我们的工作能够帮助弥合LLM联邦微调的理论和实践方面，从而促进该领域的进一步进步和研究。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.05926v2" target="_blank">2402.05926v2</a>
                              </td>
                              <td>On the Convergence of Zeroth-Order Federated Tuning for Large Language Models</td>
                              <td>Zhenqing Ling</td>
                              <td>2024-02-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_05926v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.05926v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13016v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Understanding the effects of language-specific class imbalance in multilingual fine-tuning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13016v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13016v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13016v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13016v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究了现实生活中多语言分类数据集中经常存在的一种不平衡的影响：不同语言之间标签的不均匀分布。我们展示的证据表明，在具有这种不平衡的数据集上微调基于转换器的大型语言模型（LLM）会导致更差的性能、潜在空间中更明显的语言分离以及无信息特征的提升。我们通过分别计算每种语言的类权重来修改传统的类权重方法，以解决不平衡问题，并表明这有助于减轻这些不利影响。这些结果使人们意识到在多语言微调中特定语言类别失衡的负面影响，以及模型学习依赖语言分离来执行任务的方式。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13016v1" target="_blank">2402.13016v1</a>
                              </td>
                              <td>Understanding the effects of language-specific class imbalance in multilingual fine-tuning</td>
                              <td>Vincent Jung</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13016v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13016v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/idiap/class-imbalance-multilingual-ft" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13013v1_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Code Needs Comments: Enhancing Code LLMs with Comment Augmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13013v1_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13013v1_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13013v1_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The programming skill is one crucial ability for Large Language Models (LLMs), necessitating a deep understanding of programming languages (PLs) and their correlation with natural languages (NLs). We examine the impact of pre-training data on code-focused LLMs' performance by assessing the comment density as a measure of PL-NL alignment. Given the scarcity of code-comment aligned data in pre-training corpora, we introduce a novel data augmentation method that generates comments for existing code, coupled with a data filtering strategy that filters out code data poorly correlated with natural language. We conducted experiments on three code-focused LLMs and observed consistent improvements in performance on two widely-used programming skill benchmarks. Notably, the model trained on the augmented data outperformed both the model used for generating comments and the model further trained on the data without augmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13013v1_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>编程技能是大型语言模型（LLM）的一项关键能力，需要深入了解编程语言（PL）及其与自然语言（NL）的关系。我们通过评估作为PL-NL对齐度量的注释密度，来检验预训练数据对以代码为中心的LLM性能的影响。鉴于预训练语料库中代码注释对齐数据的稀缺性，我们引入了一种新的数据扩充方法，该方法为现有代码生成注释，并结合数据过滤策略，过滤出与自然语言相关性较差的代码数据。我们在三个以代码为中心的LLM上进行了实验，并在两个广泛使用的编程技能基准上观察到性能的一致改进。值得注意的是，在增强数据上训练的模型优于用于生成评论的模型和在没有增强的情况下在数据上进一步训练的模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13013v1" target="_blank">2402.13013v1</a>
                              </td>
                              <td>Code Needs Comments: Enhancing Code LLMs with Comment Augmentation</td>
                              <td>Demin Song</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13013v1_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13013v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08976v2_2">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dynamic Retrieval-Augmented Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08976v2_2_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08976v2_2_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08976v2_2_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current state-of-the-art large language models are effective in generating high-quality text and encapsulating a broad spectrum of world knowledge. These models, however, often hallucinate and lack locally relevant factual data. Retrieval-augmented approaches were introduced to overcome these problems and provide more accurate responses. Typically, the retrieved information is simply appended to the main request, restricting the context window size of the model. We propose a novel approach for the Dynamic Retrieval-Augmented Generation (DRAG), based on the entity-augmented generation, which injects compressed embeddings of the retrieved entities into the generative model. The proposed pipeline was developed for code-generation tasks, yet can be transferred to some domains of natural language processing. To train the model, we collect and publish a new project-level code generation dataset. We use it for the evaluation along with publicly available datasets. Our approach achieves several targets: (1) lifting the length limitations of the context window, saving on the prompt size; (2) allowing huge expansion of the number of retrieval entities available for the context; (3) alleviating the problem of misspelling or failing to find relevant entity names. This allows the model to beat all baselines (except GPT-3.5) with a strong margin.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08976v2_2_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当前最先进的大型语言模型在生成高质量文本和封装广泛的世界知识方面是有效的。然而，这些模型往往产生幻觉，缺乏与当地相关的事实数据。为了克服这些问题并提供更准确的响应，引入了检索增强方法。通常，检索到的信息只是附加到主请求中，从而限制模型的上下文窗口大小。我们提出了一种基于实体增强生成的动态检索增强生成（DRAG）新方法，该方法将检索到的实体的压缩嵌入注入生成模型中。所提出的管道是为代码生成任务开发的，但可以转移到自然语言处理的一些领域。为了训练模型，我们收集并发布一个新的项目级代码生成数据集。我们将其与公开可用的数据集一起用于评估。我们的方法实现了几个目标：（1）取消了上下文窗口的长度限制，节省了提示大小；（2） 允许上下文可用的检索实体的数量的巨大扩展；（3） 从而减轻拼写错误或找不到相关实体名称的问题。这使得该模型能够以强大的优势击败所有基线（GPT-3.5除外）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>48</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08976v2" target="_blank">2312.08976v2</a>
                              </td>
                              <td>Dynamic Retrieval-Augmented Generation</td>
                              <td>Anton Shapkin</td>
                              <td>2023-12-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08976v2_2"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08976v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="SAM"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2304_02688v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Going Further: Flatness at the Rescue of Early Stopping for Adversarial Example Transferability</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_02688v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_02688v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_02688v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Transferability is the property of adversarial examples to be misclassified by other models than the surrogate model for which they were crafted. Previous research has shown that early stopping the training of the surrogate model substantially increases transferability. A common hypothesis to explain this is that deep neural networks (DNNs) first learn robust features, which are more generic, thus a better surrogate. Then, at later epochs, DNNs learn non-robust features, which are more brittle, hence worst surrogate. First, we tend to refute this hypothesis, using transferability as a proxy for representation similarity. We then establish links between transferability and the exploration of the loss landscape in parameter space, focusing on sharpness, which is affected by early stopping. This leads us to evaluate surrogate models trained with seven minimizers that minimize both loss value and loss sharpness. Among them, SAM consistently outperforms early stopping by up to 28.8 percentage points. We discover that the strong SAM regularization from large flat neighborhoods tightly links to transferability. Finally, the best sharpness-aware minimizers prove competitive with other training methods and complement existing transferability techniques.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_02688v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>可转移性是对抗性示例的特性，会被其他模型错误分类，而不是为其制作的代理模型。先前的研究表明，尽早停止代理模型的训练可以显著提高可转移性。解释这一点的一个常见假设是，深度神经网络（DNN）首先学习鲁棒特征，这些特征更通用，因此是更好的替代品。然后，在以后的时代，DNN学习非鲁棒特征，这些特征更脆弱，因此是最糟糕的替代。首先，我们倾向于反驳这一假设，使用可转移性作为表示相似性的代理。然后，我们在可转移性和参数空间中损失景观的探索之间建立联系，重点关注受早期停止影响的清晰度。这使我们能够评估使用七个最小化器训练的代理模型，这些最小化器使损失值和损失清晰度最小化。其中，SAM持续跑赢提前停车高达28.8个百分点。我们发现，来自大平坦邻域的强SAM正则化与可转移性密切相关。最后，最好的敏锐度感知最小化器与其他训练方法相比具有竞争力，并补充了现有的可转移性技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.02688v2" target="_blank">2304.02688v2</a>
                              </td>
                              <td>Going Further: Flatness at the Rescue of Early Stopping for Adversarial Example Transferability</td>
                              <td>Martin Gubri</td>
                              <td>2023-04-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_02688v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.02688v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/Framartin/rfn-flatness-transferability" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/framartin/rfn-flatness-transferability" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_17085v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_17085v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_17085v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_17085v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Single object tracking aims to locate one specific target in video sequences, given its initial state. Classical trackers rely solely on visual cues, restricting their ability to handle challenges such as appearance variations, ambiguity, and distractions. Hence, Vision-Language (VL) tracking has emerged as a promising approach, incorporating language descriptions to directly provide high-level semantics and enhance tracking performance. However, current VL trackers have not fully exploited the power of VL learning, as they suffer from limitations such as heavily relying on off-the-shelf backbones for feature extraction, ineffective VL fusion designs, and the absence of VL-related loss functions. Consequently, we present a novel tracker that progressively explores target-centric semantics for VL tracking. Specifically, we propose the first Synchronous Learning Backbone (SLB) for VL tracking, which consists of two novel modules: the Target Enhance Module (TEM) and the Semantic Aware Module (SAM). These modules enable the tracker to perceive target-related semantics and comprehend the context of both visual and textual modalities at the same pace, facilitating VL feature extraction and fusion at different semantic levels. Moreover, we devise the dense matching loss to further strengthen multi-modal representation learning. Extensive experiments on VL tracking datasets demonstrate the superiority and effectiveness of our methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_17085v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>单目标跟踪的目标是在给定初始状态的情况下定位视频序列中的一个特定目标。传统的追踪器只依赖视觉提示，限制了它们处理诸如外观变化、模糊和分心等挑战的能力。因此，视觉语言（VL）跟踪已经成为一种很有前途的方法，它结合了语言描述来直接提供高级语义并提高跟踪性能。然而，当前的VL跟踪器还没有充分利用VL学习的能力，因为它们受到限制，例如严重依赖现成的主干进行特征提取、无效的VL融合设计以及缺乏与VL相关的损失函数。因此，我们提出了一种新的跟踪器，它逐步探索VL跟踪的以目标为中心的语义。具体来说，我们提出了第一个用于VL跟踪的同步学习主干（SLB），它由两个新模块组成：目标增强模块（TEM）和语义感知模块（SAM）。这些模块使跟踪器能够感知与目标相关的语义，并以相同的速度理解视觉和文本模态的上下文，从而促进不同语义级别的VL特征提取和融合。此外，我们设计了密集匹配损失来进一步加强多模态表示学习。在VL跟踪数据集上进行的大量实验证明了我们的方法的优越性和有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.17085v2" target="_blank">2311.17085v2</a>
                              </td>
                              <td>Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking</td>
                              <td>Jiawei Ge</td>
                              <td>2023-11-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_17085v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.17085v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11996v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ISCUTE: Instance Segmentation of Cables Using Text Embedding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11996v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11996v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11996v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11996v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在机器人和自动化领域，当涉及到感知可变形线性对象（如电线、电缆和软管）时，传统的对象识别和实例分割方法面临着巨大的挑战。这一挑战主要源于缺乏独特的属性，如形状、颜色和纹理，这需要量身定制的解决方案来实现精确的识别。在这项工作中，我们提出了一种基于基础模型的DLO实例分割技术，该技术具有文本提示性和用户友好性。具体而言，我们的方法将CLIPSeg模型的文本条件语义分割能力与Segment Anything模型（SAM）的零样本泛化能力相结合。我们表明，我们的方法在DLO实例分割上超过了SOTA的性能，实现了$91.21\%$的mIoU。我们还介绍了一个丰富多样的NetBackup DLO特定数据集，用于实例分割。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11996v1" target="_blank">2402.11996v1</a>
                              </td>
                              <td>ISCUTE: Instance Segmentation of Cables Using Text Embedding</td>
                              <td>Shir Kozlovsky</td>
                              <td>2024-02-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11996v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11996v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_12968v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PassViz: A Visualisation System for Analysing Leaked Passwords</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_12968v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_12968v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_12968v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Passwords remain the most widely used form of user authentication, despite advancements in other methods. However, their limitations, such as susceptibility to attacks, especially weak passwords defined by human users, are well-documented. The existence of weak human-defined passwords has led to repeated password leaks from websites, many of which are of large scale. While such password leaks are unfortunate security incidents, they provide security researchers and practitioners with good opportunities to learn valuable insights from such leaked passwords, in order to identify ways to improve password policies and other security controls on passwords. Researchers have proposed different data visualisation techniques to help analyse leaked passwords. However, many approaches rely solely on frequency analysis, with limited exploration of distance-based graphs. This paper reports PassViz, a novel method that combines the edit distance with the t-SNE (t-distributed stochastic neighbour embedding) dimensionality reduction algorithm for visualising and analysing leaked passwords in a 2-D space. We implemented PassViz as an easy-to-use command-line tool for visualising large-scale password databases, and also as a graphical user interface (GUI) to support interactive visual analytics of small password databases. Using the "000webhost" leaked database as an example, we show how PassViz can be used to visually analyse different aspects of leaked passwords and to facilitate the discovery of previously unknown password patterns. Overall, our approach empowers researchers and practitioners to gain valuable insights and improve password security through effective data visualisation and analysis.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_12968v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管其他方法有所进步，但密码仍然是最广泛使用的用户身份验证形式。然而，它们的局限性，如易受攻击，尤其是人类用户定义的弱密码，已经有了充分的证明。弱的人工定义密码的存在导致了网站多次出现密码泄露，其中许多都是大规模的。虽然此类密码泄露是不幸的安全事件，但它们为安全研究人员和从业人员提供了很好的机会，让他们从此类泄露的密码中学习有价值的见解，以确定改进密码策略和其他密码安全控制的方法。研究人员提出了不同的数据可视化技术来帮助分析泄露的密码。然而，许多方法仅依赖于频率分析，对基于距离的图的探索有限。本文报道了PassViz，这是一种将编辑距离与t-SNE（t-分布式随机邻居嵌入）降维算法相结合的新方法，用于在二维空间中可视化和分析泄露的密码。我们实现了PassViz，它是一个易于使用的命令行工具，用于可视化大型密码数据库，也是一个图形用户界面（GUI），用于支持小型密码数据库的交互式可视化分析。以“000webhost”泄漏数据库为例，我们展示了如何使用PassViz直观地分析泄漏密码的不同方面，并促进发现以前未知的密码模式。总的来说，我们的方法使研究人员和从业者能够通过有效的数据可视化和分析获得有价值的见解，并提高密码安全性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.12968v3" target="_blank">2309.12968v3</a>
                              </td>
                              <td>PassViz: A Visualisation System for Analysing Leaked Passwords</td>
                              <td>Sam Parker</td>
                              <td>2023-09-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_12968v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.12968v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11413v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Multispectral Automated Transfer Technique (MATT) for machine-driven image labeling utilizing the Segment Anything Model (SAM)</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11413v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11413v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11413v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model (SAM) is drastically accelerating the speed and accuracy of automatically segmenting and labeling large Red-Green-Blue (RGB) imagery datasets. However, SAM is unable to segment and label images outside of the visible light spectrum, for example, for multispectral or hyperspectral imagery. Therefore, this paper outlines a method we call the Multispectral Automated Transfer Technique (MATT). By transposing SAM segmentation masks from RGB images we can automatically segment and label multispectral imagery with high precision and efficiency. For example, the results demonstrate that segmenting and labeling a 2,400-image dataset utilizing MATT achieves a time reduction of 87.8% in developing a trained model, reducing roughly 20 hours of manual labeling, to only 2.4 hours. This efficiency gain is associated with only a 6.7% decrease in overall mean average precision (mAP) when training multispectral models via MATT, compared to a manually labeled dataset. We consider this an acceptable level of precision loss when considering the time saved during training, especially for rapidly prototyping experimental modeling methods. This research greatly contributes to the study of multispectral object detection by providing a novel and open-source method to rapidly segment, label, and train multispectral object detection models with minimal human interaction. Future research needs to focus on applying these methods to (i) space-based multispectral, and (ii) drone-based hyperspectral imagery.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11413v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Segment Anything Model（SAM）大大加快了自动分割和标记大型红-绿-蓝（RGB）图像数据集的速度和准确性。然而，SAM无法对可见光谱之外的图像进行分割和标记，例如，对于多光谱或高光谱图像。因此，本文概述了一种我们称之为多光谱自动传输技术（MATT）的方法。通过从RGB图像中转换SAM分割掩模，我们可以高精度和高效地自动分割和标记多光谱图像。例如，结果表明，利用MATT对2400个图像数据集进行分割和标记，在开发经过训练的模型时可减少87.8%的时间，将大约20小时的手动标记减少到仅2.4小时。与手动标记的数据集相比，当通过MATT训练多光谱模型时，这种效率增益仅与总体平均精度（mAP）下降6.7%有关。当考虑到训练期间节省的时间时，我们认为这是一个可接受的精度损失水平，特别是对于快速原型实验建模方法。这项研究提供了一种新的开源方法，可以在最少的人类互动的情况下快速分割、标记和训练多光谱物体检测模型，大大有助于多光谱物体探测的研究。未来的研究需要侧重于将这些方法应用于（i）天基多光谱和（ii）无人机高光谱图像。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11413v1" target="_blank">2402.11413v1</a>
                              </td>
                              <td>A Multispectral Automated Transfer Technique (MATT) for machine-driven image labeling utilizing the Segment Anything Model (SAM)</td>
                              <td>James E. Gallagher</td>
                              <td>2024-02-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11413v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11413v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10494v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mechanised uniform interpolation for modal logics K, GL and iSL</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10494v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10494v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10494v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The uniform interpolation property in a given logic can be understood as the definability of propositional quantifiers. We mechanise the computation of these quantifiers and prove correctness in the Coq proof assistant for three modal logics, namely: (1) the modal logic K, for which a pen-and-paper proof exists; (2) G\"odel-L\"ob logic GL, for which our formalisation clarifies an important point in an existing, but incomplete, sequent-style proof; and (3) intuitionistic strong L\"ob logic iSL, for which this is the first proof-theoretic construction of uniform interpolants. Our work also yields verified programs that allow one to compute the propositional quantifiers on any formula in this logic.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10494v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>给定逻辑中的一致插值性质可以理解为命题量词的可定义性。我们机械化了这些量词的计算，并在Coq证明辅助中证明了三个模态逻辑的正确性，即：（1）模态逻辑K，它存在纸笔证明；（2） G\“odel-L\”ob逻辑GL，我们的形式化澄清了现有但不完整的序列式证明中的一个重要点；和（3）直觉强L\“ob逻辑iSL，这是一致插入项的第一个证明论构造。我们的工作还产生了允许计算该逻辑中任何公式上的命题量词的验证程序。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10494v1" target="_blank">2402.10494v1</a>
                              </td>
                              <td>Mechanised uniform interpolation for modal logics K, GL and iSL</td>
                              <td>Hugo Férée</td>
                              <td>2024-02-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10494v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10494v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10435v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10435v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10435v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10435v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Person re-identification (re-ID) continues to pose a significant challenge, particularly in scenarios involving occlusions. Prior approaches aimed at tackling occlusions have predominantly focused on aligning physical body features through the utilization of external semantic cues. However, these methods tend to be intricate and susceptible to noise. To address the aforementioned challenges, we present an innovative end-to-end solution known as the Dynamic Patch-aware Enrichment Transformer (DPEFormer). This model effectively distinguishes human body information from occlusions automatically and dynamically, eliminating the need for external detectors or precise image alignment. Specifically, we introduce a dynamic patch token selection module (DPSM). DPSM utilizes a label-guided proxy token as an intermediary to identify informative occlusion-free tokens. These tokens are then selected for deriving subsequent local part features. To facilitate the seamless integration of global classification features with the finely detailed local features selected by DPSM, we introduce a novel feature blending module (FBM). FBM enhances feature representation through the complementary nature of information and the exploitation of part diversity. Furthermore, to ensure that DPSM and the entire DPEFormer can effectively learn with only identity labels, we also propose a Realistic Occlusion Augmentation (ROA) strategy. This strategy leverages the recent advances in the Segment Anything Model (SAM). As a result, it generates occlusion images that closely resemble real-world occlusions, greatly enhancing the subsequent contrastive learning process. Experiments on occluded and holistic re-ID benchmarks signify a substantial advancement of DPEFormer over existing state-of-the-art approaches. The code will be made publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10435v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人的重新识别（re-ID）仍然是一个重大挑战，特别是在涉及闭塞的情况下。先前旨在解决遮挡的方法主要集中在通过利用外部语义线索来对齐身体特征。然而，这些方法往往是复杂的并且容易受到噪声的影响。为了应对上述挑战，我们提出了一种创新的端到端解决方案，称为动态补丁感知富集转换器（DPEFormer）。该模型自动动态地有效区分人体信息和遮挡，消除了对外部检测器或精确图像对齐的需求。具体来说，我们介绍了一个动态补丁令牌选择模块（DPSM）。DPSM利用标签引导的代理令牌作为中介来识别信息性的无遮挡令牌。然后选择这些标记以获得后续的局部零件特征。为了促进全局分类特征与DPSM选择的精细局部特征的无缝集成，我们引入了一种新的特征混合模块（FBM）。FBM通过信息的互补性和零件多样性的利用来增强特征表示。此外，为了确保DPSM和整个DPEFormer能够仅使用身份标签进行有效学习，我们还提出了一种真实遮挡增强（ROA）策略。该战略利用了分段任何模型（SAM）的最新进展。因此，它生成的遮挡图像与真实世界的遮挡非常相似，大大增强了随后的对比学习过程。在闭塞和整体re-ID基准上的实验表明，DPEFormer比现有的最先进的方法有了实质性的进步。该代码将公开发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10435v1" target="_blank">2402.10435v1</a>
                              </td>
                              <td>Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification</td>
                              <td>Xin Zhang</td>
                              <td>2024-02-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10435v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10435v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10280v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10280v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10280v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10280v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose a novel energy-aware federated learning (FL)-based system, namely SusFL, for sustainable smart farming to address the challenge of inconsistent health monitoring due to fluctuating energy levels of solar sensors. This system equips animals, such as cattle, with solar sensors with computational capabilities, including Raspberry Pis, to train a local deep-learning model on health data. These sensors periodically update Long Range (LoRa) gateways, forming a wireless sensor network (WSN) to detect diseases like mastitis. Our proposed SusFL system incorporates mechanism design, a game theory concept, for intelligent client selection to optimize monitoring quality while minimizing energy use. This strategy ensures the system's sustainability and resilience against adversarial attacks, including data poisoning and privacy threats, that could disrupt FL operations. Through extensive comparative analysis using real-time datasets, we demonstrate that our FL-based monitoring system significantly outperforms existing methods in prediction accuracy, operational efficiency, system reliability (i.e., mean time between failures or MTBF), and social welfare maximization by the mechanism designer. Our findings validate the superiority of our system for effective and sustainable animal health monitoring in smart farms. The experimental results show that SusFL significantly improves system performance, including a $10\%$ reduction in energy consumption, a $15\%$ increase in social welfare, and a $34\%$ rise in Mean Time Between Failures (MTBF), alongside a marginal increase in the global model's prediction accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10280v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的基于能量感知的联合学习（FL）系统，即SusFL，用于可持续智能农业，以应对由于太阳能传感器的能量水平波动而导致的健康监测不一致的挑战。该系统为牛等动物配备了具有计算能力的太阳能传感器，包括树莓派，以训练当地的健康数据深度学习模型。这些传感器定期更新远程（LoRa）网关，形成无线传感器网络（WSN）来检测乳腺炎等疾病。我们提出的SusFL系统结合了机制设计和博弈论概念，用于智能客户选择，以优化监控质量，同时最大限度地减少能源使用。这一策略确保了系统的可持续性和抵御对抗性攻击的能力，包括可能干扰FL运营的数据中毒和隐私威胁。通过使用实时数据集进行广泛的比较分析，我们证明了我们基于FL的监测系统在预测准确性、运行效率、系统可靠性（即平均无故障时间或MTBF）和机制设计者的社会福利最大化方面显著优于现有方法。我们的研究结果验证了我们的系统在智能农场中进行有效和可持续的动物健康监测的优越性。实验结果表明，SusFL显著提高了系统性能，包括能耗减少了10%，社会福利增加了15%，平均无故障时间（MTBF）增加了34%，同时全球模型的预测精度略有提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10280v1" target="_blank">2402.10280v1</a>
                              </td>
                              <td>SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms</td>
                              <td>Dian Chen</td>
                              <td>2024-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10280v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10280v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10260v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A StrongREJECT for Empty Jailbreaks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10260v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10260v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10260v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rise of large language models (LLMs) has drawn attention to the existence of "jailbreaks" that allow the models to be used maliciously. However, there is no standard benchmark for measuring the severity of a jailbreak, leaving authors of jailbreak papers to create their own. We show that these benchmarks often include vague or unanswerable questions and use grading criteria that are biased towards overestimating the misuse potential of low-quality model responses. Some jailbreak techniques make the problem worse by decreasing the quality of model responses even on benign questions: we show that several jailbreaking techniques substantially reduce the zero-shot performance of GPT-4 on MMLU. Jailbreaks can also make it harder to elicit harmful responses from an "uncensored" open-source model. We present a new benchmark, StrongREJECT, which better discriminates between effective and ineffective jailbreaks by using a higher-quality question set and a more accurate response grading algorithm. We show that our new grading scheme better accords with human judgment of response quality and overall jailbreak effectiveness, especially on the sort of low-quality responses that contribute the most to over-estimation of jailbreak performance on existing benchmarks. We release our code and data at https://github.com/alexandrasouly/strongreject.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10260v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的兴起引起了人们对允许恶意使用这些模型的“越狱”的关注。然而，没有衡量越狱严重程度的标准基准，让越狱论文的作者自己创作。我们表明，这些基准通常包括模糊或无法回答的问题，并使用的评分标准偏向于高估低质量模型响应的滥用潜力。一些越狱技术甚至在良性问题上也会降低模型响应的质量，从而使问题变得更糟：我们发现，几种越狱技术大大降低了GPT-4在MMLU上的零样本性能。越狱也会使“未经审查”的开源模型更难引发有害反应。我们提出了一个新的基准StrongREJECT，它通过使用更高质量的问题集和更准确的回答评分算法，更好地区分有效和无效的越狱。我们表明，我们的新评分方案更好地符合人类对反应质量和整体越狱有效性的判断，尤其是在低质量反应方面，这些反应在现有基准上对越狱表现的高估贡献最大。我们在发布代码和数据https://github.com/alexandrasouly/strongreject.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10260v1" target="_blank">2402.10260v1</a>
                              </td>
                              <td>A StrongREJECT for Empty Jailbreaks</td>
                              <td>Alexandra Souly</td>
                              <td>2024-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10260v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10260v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alexandrasouly/strongreject" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_09883v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lester: rotoscope animation through video object segmentation and tracking</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_09883v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_09883v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_09883v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer from temporal consistency problems and do not cope well with pixelated and schematic outputs. The method is also much most practical than techniques based on 3D human pose estimation, which require custom handcrafted 3D models and are very limited with respect to the type of scenes they can process.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_09883v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了一种从视频中自动合成复古风格2D动画的新方法Lester。该方法主要作为一个对象分割和跟踪问题来解决这一挑战。视频帧使用分段任意模型（SAM）进行处理，并使用DeAOT（一种用于半监督视频对象分割的分层传播方法）通过后续帧跟踪得到的掩码。使用Douglas Peucker算法简化了掩模轮廓的几何结构。最后，可以选择添加面部特征、像素化和基本的阴影效果。结果表明，该方法具有良好的时间一致性，能够正确处理不同姿态和外观、动态镜头、局部镜头和不同背景的视频。与基于扩散模型的视频到视频翻译管道相比，所提出的方法提供了一种更简单、更具确定性的方法，后者存在时间一致性问题，不能很好地处理像素化和示意图输出。该方法也比基于3D人体姿态估计的技术实用得多，后者需要定制手工制作的3D模型，并且在可以处理的场景类型方面非常有限。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.09883v1" target="_blank">2402.09883v1</a>
                              </td>
                              <td>Lester: rotoscope animation through video object segmentation and tracking</td>
                              <td>Ruben Tous</td>
                              <td>2024-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_09883v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.09883v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/rtous/lester" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01188v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Segment Any Change</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01188v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01188v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01188v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem. In this paper, we propose the segment any change models (AnyChange), a new type of change detection model that supports zero-shot prediction and generalization on unseen change types and data distributions. AnyChange is built on the segment anything model (SAM) via our training-free adaptation method, bitemporal latent matching. By revealing and exploiting intra-image and inter-image semantic similarities in SAM's latent space, bitemporal latent matching endows SAM with zero-shot change detection capabilities in a training-free way. We also propose a point query mechanism to enable AnyChange's zero-shot object-centric change detection capability. We perform extensive experiments to confirm the effectiveness of AnyChange for zero-shot change detection. AnyChange sets a new record on the SECOND benchmark for unsupervised change detection, exceeding the previous SOTA by up to 4.4% F$_1$ score, and achieving comparable accuracy with negligible manual annotations (1 pixel per image) for supervised change detection.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01188v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉基础模型在零样本图像分类和分割方面取得了显著的成果，但零样本变化检测仍然是一个悬而未决的问题。在本文中，我们提出了分段任意变化模型（AnyChange），这是一种新型的变化检测模型，支持对看不见的变化类型和数据分布的零样本预测和泛化。AnyChange是通过我们的无训练自适应方法，双时态潜在匹配，建立在分段任意模型（SAM）上的。通过揭示和利用SAM潜在空间中的图像内和图像间语义相似性，双时态潜在匹配以无训练的方式赋予SAM零样本变化检测能力。我们还提出了一种点查询机制，以实现AnyChange的零样本对象中心变化检测功能。我们进行了大量实验，以确认AnyChange对零样本变化检测的有效性。AnyChange在第二个无监督变化检测基准上创下了新纪录，超过了之前的SOTA高达4.4%F$_1$的分数，并在监督变化检测中实现了可忽略的手动注释（每张图像1个像素）的可比精度。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01188v2" target="_blank">2402.01188v2</a>
                              </td>
                              <td>Segment Any Change</td>
                              <td>Zhuo Zheng</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01188v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01188v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_05902v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_05902v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_05902v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_05902v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input prompts, training capabilities, and efficient model design. However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images. Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures. In this project, we developed ClickSAM, which fine-tunes the Segment Anything Model using click prompts for ultrasound images. ClickSAM has two stages of training: the first stage is trained on single-click prompts centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click prompts. By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated. Positive clicks are generated using the true positive and false negative segments, and negative clicks are generated using the false positive segments. The Centroidal Voronoi Tessellation algorithm is then employed to collect positive and negative click prompts in each segment that are used to enhance the model performance during the second stage of training. With click-train methods, ClickSAM exhibits superior performance compared to other existing models for ultrasound image segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_05902v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最新发布的Segment Anything Model（SAM）是图像处理中使用的一种流行工具，因为它具有卓越的分割精度、多种输入提示、训练能力和高效的模型设计。然而，它目前的模型是在一个不同的数据集上训练的，而不是针对医学图像，特别是超声图像。超声图像往往有很多噪声，使得很难分割出重要的结构。在这个项目中，我们开发了ClickSAM，它使用超声波图像的点击提示来微调Segment Anything模型。ClickSAM有两个阶段的训练：第一阶段是在以地面实况轮廓为中心的单击提示上进行训练，第二阶段侧重于通过额外的正面和负面单击提示来提高模型性能。通过将第一阶段预测与地面实况掩码进行比较，计算出真阳性、假阳性和假阴性片段。正点击使用真正和假负片段生成，负点击使用假正片段生成。然后，使用质心Voronoi细分算法来收集每个片段中的正点击提示和负点击提示，这些提示用于在第二阶段的训练中增强模型性能。与其他现有的超声图像分割模型相比，使用点击训练方法，ClickSAM表现出优越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.05902v3" target="_blank">2402.05902v3</a>
                              </td>
                              <td>ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation</td>
                              <td>Aimee Guo</td>
                              <td>2024-02-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_05902v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.05902v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_09370v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Pseudorandom Error-Correcting Codes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_09370v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_09370v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_09370v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.   We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationally indistinguishable from text output by the original model. This is the first undetectable watermarking scheme that can tolerate a constant rate of errors.   Our second application is to steganography, where a secret message is hidden in innocent-looking content. We present a constant-rate stateless steganography scheme with robustness to a constant rate of substitutions. Ours is the first stateless steganography scheme with provable steganographic security and any robustness to errors.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_09370v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们构造伪随机纠错码（或简单的伪随机码），它是纠错码，具有任何多项式数量的码字对任何计算上有界的对手都是伪随机的性质。在解码密钥的帮助下，对损坏的码字的有效解码是可能的。我们构建了对替换和删除错误具有鲁棒性的伪随机码，其中伪随机性基于标准密码假设。具体地说，伪随机性是基于LPN的$2^{O（\sqrt｛n｝）}$硬度，或者LPN的多项式硬度和低密度下的种植XOR问题。作为伪随机码的主要应用，我们为语言模型的输出提出了一种不可检测的水印方案，该方案对裁剪和恒定的随机替换和删除率具有鲁棒性。水印是不可检测的，因为任何数量的带水印文本样本在计算上都与原始模型输出的文本不可区分。这是第一个可以容忍恒定错误率的不可检测的水印方案。我们的第二个应用程序是隐写术，在隐写术中，一条秘密消息隐藏在看起来无辜的内容中。我们提出了一种对恒定替换率具有鲁棒性的恒定速率无状态隐写方案。我们的方案是第一个具有可证明的隐写安全性和对错误的鲁棒性的无状态隐写方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.09370v1" target="_blank">2402.09370v1</a>
                              </td>
                              <td>Pseudorandom Error-Correcting Codes</td>
                              <td>Miranda Christ</td>
                              <td>2024-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_09370v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.09370v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_15562v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning from SAM: Harnessing a Foundation Model for Sim2Real Adaptation by Regularization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_15562v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_15562v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_15562v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Domain adaptation is especially important for robotics applications, where target domain training data is usually scarce and annotations are costly to obtain. We present a method for self-supervised domain adaptation for the scenario where annotated source domain data (e.g. from synthetic generation) is available, but the target domain data is completely unannotated. Our method targets the semantic segmentation task and leverages a segmentation foundation model (Segment Anything Model) to obtain segment information on unannotated data. We take inspiration from recent advances in unsupervised local feature learning and propose an invariance-variance loss over the detected segments for regularizing feature representations in the target domain. Crucially, this loss structure and network architecture can handle overlapping segments and oversegmentation as produced by Segment Anything. We demonstrate the advantage of our method on the challenging YCB-Video and HomebrewedDB datasets and show that it outperforms prior work and, on YCB-Video, even a network trained with real annotations. Additionally, we provide insight through model ablations and show applicability to a custom robotic application.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_15562v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>领域自适应对于机器人应用尤其重要，因为在机器人应用中，目标领域训练数据通常很少，并且获取注释的成本很高。我们提出了一种自监督域自适应的方法，用于注释源域数据（例如来自合成生成）可用，但目标域数据完全未注释的场景。我们的方法以语义分割任务为目标，并利用分割基础模型（Segment Anything model）来获得未标记数据上的分割信息。我们从无监督局部特征学习的最新进展中获得灵感，并提出了一种在检测片段上的不变性方差损失，用于正则化目标域中的特征表示。至关重要的是，这种损失结构和网络架构可以处理Segment Anything产生的重叠段和过度段。我们在具有挑战性的YCB视频和HomebrewedDB数据集上展示了我们的方法的优势，并表明它优于先前的工作，在YCB视频上，甚至优于用真实注释训练的网络。此外，我们通过模型消融提供了见解，并展示了对定制机器人应用的适用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.15562v2" target="_blank">2309.15562v2</a>
                              </td>
                              <td>Learning from SAM: Harnessing a Foundation Model for Sim2Real Adaptation by Regularization</td>
                              <td>Mayara E. Bonani</td>
                              <td>2023-09-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_15562v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.15562v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08788v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Syllable based DNN-HMM Cantonese Speech to Text System</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08788v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08788v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08788v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper reports our work on building up a Cantonese Speech-to-Text (STT) system with a syllable based acoustic model. This is a part of an effort in building a STT system to aid dyslexic students who have cognitive deficiency in writing skills but have no problem expressing their ideas through speech. For Cantonese speech recognition, the basic unit of acoustic models can either be the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC) syllables where finals are further split into nucleus and coda to reflect the intra-syllable variations in Cantonese. By using the Kaldi toolkit, our system is trained using the stochastic gradient descent optimization model with the aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model (DNN-HMM) with and without I-vector based speaker adaptive training technique. The input features of the same Gaussian Mixture Model with speaker adaptive training (GMM-SAT) to DNN are used in all cases. Experiments show that the ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the best performance with the word error rate (WER) of 9.66% and the real time factor (RTF) of 1.38812.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08788v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文报道了我们使用基于音节的声学模型构建广东话语音到文本（STT）系统的工作。这是建立STT系统的努力的一部分，该系统旨在帮助那些在写作技能方面存在认知缺陷但通过言语表达想法没有问题的阅读障碍学生。对于广东话语音识别，声学模型的基本单元可以是传统的声母-韵母（IF）音节，也可以是起始核-尾音（ONC）音节，其中韵母进一步分为核和尾音，以反映广东话的音节内变化。通过使用Kaldi工具包，我们的系统使用随机梯度下降优化模型，并借助于混合深度神经网络和隐马尔可夫模型（DNN-HMM）的GPU，在有和没有基于I向量的说话人自适应训练技术的情况下进行训练。在所有情况下都使用具有说话人自适应训练的相同高斯混合模型（GMM-SAT）对DNN的输入特征。实验表明，基于I矢量的DNN-HMM基于ONC的音节声学建模取得了最好的性能，错误率（WER）为9.66%，实时因子（RTF）为1.38812。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08788v1" target="_blank">2402.08788v1</a>
                              </td>
                              <td>Syllable based DNN-HMM Cantonese Speech to Text System</td>
                              <td>Timothy Wong</td>
                              <td>2024-02-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08788v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08788v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01274v3_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01274v3_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01274v3_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01274v3_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data. Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning. While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics. This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification. Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks. Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01274v3_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，自监督学习因其从未标记数据中学习鲁棒特征表示的能力而脱颖而出。通过自我监督预训练的网络可以作为下游任务的有效特征提取器，包括少镜头学习。虽然在图像中对无监督的少镜头学习方法的评估是公认的，但在声学中却明显缺乏。这项研究通过评估大规模自监督模型在少镜头音频分类中的性能来解决这一差距。此外，我们还探讨了模型的少镜头学习能力与其他下游任务基准之间的关系。我们的研究结果揭示了一些少镜头问题（如SpeechCommandsv2）的最先进性能，以及基于语音的少镜头问题与各种下游音频任务之间的强相关性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01274v3" target="_blank">2402.01274v3</a>
                              </td>
                              <td>On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification</td>
                              <td>Calum Heggan</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01274v3_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01274v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08671v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Are Semi-Dense Detector-Free Methods Good at Matching Local Features?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08671v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08671v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08671v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image matching methods. While SDF methods are trained to establish correspondences between two images, their performances are almost exclusively evaluated using relative pose estimation metrics. Thus, the link between their ability to establish correspondences and the quality of the resulting estimated pose has thus far received little attention. This paper is a first attempt to study this link. We start with proposing a novel structured attention-based image matching architecture (SAM). It allows us to show a counter-intuitive result on two datasets (MegaDepth and HPatches): on the one hand SAM either outperforms or is on par with SDF methods in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly better than SAM in terms of matching accuracy. We then propose to limit the computation of the matching accuracy to textured regions, and show that in this case SAM often surpasses SDF methods. Our findings highlight a strong correlation between the ability to establish accurate correspondences in textured regions and the accuracy of the resulting estimated pose/homography. Our code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08671v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>半密集无检测器方法（SDF），如LoFTR，是目前最流行的图像匹配方法之一。虽然SDF方法被训练来建立两个图像之间的对应关系，但它们的性能几乎完全使用相对姿态估计度量来评估。因此，到目前为止，它们建立对应关系的能力与所得到的估计姿态的质量之间的联系很少受到关注。本文是对这一环节进行研究的首次尝试。我们首先提出了一种新颖的基于结构注意力的图像匹配架构（SAM）。它允许我们在两个数据集（MegaDepth和HPatches）上显示反直觉的结果：一方面，SAM在姿态/单应性估计指标方面优于或等于SDF方法，但另一方面，SDF方法在匹配精度方面明显优于SAM。然后，我们建议将匹配精度的计算限制在纹理区域，并表明在这种情况下，SAM通常超过SDF方法。我们的发现强调了在纹理区域中建立精确对应的能力与由此估计的姿态/单应性的准确性之间的强相关性。我们的代码将可用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08671v1" target="_blank">2402.08671v1</a>
                              </td>
                              <td>Are Semi-Dense Detector-Free Methods Good at Matching Local Features?</td>
                              <td>Matthieu Vilain</td>
                              <td>2024-02-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08671v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08671v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01845v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Zero-Shot Refinement of Buildings' Segmentation Models using SAM</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01845v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01845v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01845v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different prompting strategies, including integrating a pre-trained CNN as a prompt generator. This novel approach augments SAM with recognition abilities, a first of its kind. We evaluated our method on three remote sensing datasets, including the WHU Buildings dataset, the Massachusetts Buildings dataset, and the AICrowd Mapping Challenge. For out-of-distribution performance on the WHU dataset, we achieve a 5.47\% increase in IoU and a 4.81\% improvement in F1-score. For in-distribution performance on the WHU dataset, we observe a 2.72\% and 1.58\% increase in True-Positive-IoU and True-Positive-F1 score, respectively. Our code is publicly available at this Repo (https://github.com/geoaigroup/GEOAI-ECRS2023), hoping to inspire further exploration of foundation models for domain-specific tasks within the remote sensing community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01845v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型在各种任务中表现出色，但通常在通用基准上进行评估。将这些模型适用于遥感图像等特定领域仍然是一个未充分探索的领域。在遥感领域，精确的建筑实例分割对于城市规划等应用至关重要。虽然卷积神经网络（CNNs）表现良好，但其泛化能力可能受到限制。为此，我们提出了一种新的方法来调整基础模型，以解决现有模型的泛化滞后问题。在几个模型中，我们的重点是分段任意模型（SAM），这是一个强大的基础模型，以其在类无关图像分割能力方面的卓越能力而闻名。我们首先确定SAM的局限性，揭示其应用于遥感图像时的次优性能。此外，SAM不提供识别能力，因此无法对本地化对象进行分类和标记。为了解决这些局限性，我们引入了不同的提示策略，包括集成预先训练的CNN作为提示生成器。这种新颖的方法增强了SAM的识别能力，这是同类方法中的第一个。我们在三个遥感数据集上评估了我们的方法，包括WHU建筑数据集、马萨诸塞州建筑数据集和AICrowd地图挑战。对于WHU数据集上的分布外性能，我们实现了IoU增加5.47%和F1得分提高4.81%。对于WHU数据集上的分布内性能，我们观察到True Positive IoU和True Positive-F1得分分别增加了2.72%和1.58%。我们的代码在此回购中公开(https://github.com/geoaigroup/GEOAI-ECRS2023)，希望能启发遥感界进一步探索特定领域任务的基础模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01845v2" target="_blank">2310.01845v2</a>
                              </td>
                              <td>Zero-Shot Refinement of Buildings' Segmentation Models using SAM</td>
                              <td>Ali Mayladan</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01845v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01845v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/geoaigroup/geoai-ecrs2023" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07098v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Pallet Detection Using Synthetic Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07098v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07098v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07098v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The use of synthetic data in machine learning saves a significant amount of time when implementing an effective object detector. However, there is limited research in this domain. This study aims to improve upon previously applied implementations in the task of instance segmentation of pallets in a warehouse environment. This study proposes using synthetically generated domain-randomised data as well as data generated through Unity to achieve this. This study achieved performance improvements on the stacked and racked pallet categories by 69% and 50% mAP50, respectively when being evaluated on real data. Additionally, it was found that there was a considerable impact on the performance of a model when it was evaluated against images in a darker environment, dropping as low as 3% mAP50 when being evaluated on images with an 80% brightness reduction. This study also created a two-stage detector that used YOLOv8 and SAM, but this proved to have unstable performance. The use of domain-randomised data proved to have negligible performance improvements when compared to the Unity-generated data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07098v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在实现有效的对象检测器时，在机器学习中使用合成数据可以节省大量时间。然而，这一领域的研究有限。本研究旨在改进先前在仓库环境中托盘实例分割任务中应用的实现。这项研究建议使用综合生成的领域随机数据以及通过Unity生成的数据来实现这一点。当对真实数据进行评估时，这项研究在堆叠托盘和支架托盘类别上分别实现了69%和50%的性能改进。此外，研究发现，当在较暗的环境中对图像进行评估时，对模型的性能有相当大的影响，当在亮度降低80%的图像上进行评估时降至3%mAP50。这项研究还创建了一个使用YOLOv8和SAM的两阶段检测器，但这被证明具有不稳定的性能。与Unity生成的数据相比，使用领域随机数据的性能改进微不足道。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07098v1" target="_blank">2402.07098v1</a>
                              </td>
                              <td>Improving Pallet Detection Using Synthetic Data</td>
                              <td>Henry Gann</td>
                              <td>2024-02-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07098v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07098v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07059v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm Surveillance</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07059v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07059v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07059v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this study, we propose an automated framework for camel farm monitoring, introducing two key contributions: the Unified Auto-Annotation framework and the Fine-Tune Distillation framework. The Unified Auto-Annotation approach combines two models, GroundingDINO (GD), and Segment-Anything-Model (SAM), to automatically annotate raw datasets extracted from surveillance videos. Building upon this foundation, the Fine-Tune Distillation framework conducts fine-tuning of student models using the auto-annotated dataset. This process involves transferring knowledge from a large teacher model to a student model, resembling a variant of Knowledge Distillation. The Fine-Tune Distillation framework aims to be adaptable to specific use cases, enabling the transfer of knowledge from the large models to the small models, making it suitable for domain-specific applications. By leveraging our raw dataset collected from Al-Marmoom Camel Farm in Dubai, UAE, and a pre-trained teacher model, GroundingDINO, the Fine-Tune Distillation framework produces a lightweight deployable model, YOLOv8. This framework demonstrates high performance and computational efficiency, facilitating efficient real-time object detection. Our code is available at \href{https://github.com/Razaimam45/Fine-Tune-Distillation}{https://github.com/Razaimam45/Fine-Tune-Distillation}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07059v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项研究中，我们提出了一个骆驼场监控的自动化框架，介绍了两个关键贡献：统一自动注释框架和微调蒸馏框架。统一自动注释方法结合了两个模型，GroundingDINO（GD）和Segment Anything Model（SAM），以自动注释从监控视频中提取的原始数据集。在此基础上，微调蒸馏框架使用自动注释数据集对学生模型进行微调。这一过程涉及到将知识从大型教师模型转移到学生模型，类似于知识蒸馏的变体。微调蒸馏框架旨在适应特定的用例，使知识能够从大模型转移到小模型，从而适用于特定领域的应用。通过利用我们从阿联酋迪拜的Al Marmoom骆驼农场收集的原始数据集和经过预训练的教师模型GroundingDINO，微调蒸馏框架产生了一个轻量级的可部署模型YOLOv8。该框架展示了高性能和计算效率，促进了高效的实时对象检测。我们的代码位于\ href{https://github.com/Razaimam45/Fine-Tune-Distillation}{https://github.com/Razaimam45/Fine-Tune-Distillation}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07059v1" target="_blank">2402.07059v1</a>
                              </td>
                              <td>Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm Surveillance</td>
                              <td>Raza Imam</td>
                              <td>2024-02-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07059v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07059v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/razaimam45/fine-tune-distillation" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17083v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Online Robot Navigation and Manipulation with Distilled Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17083v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17083v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17083v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Autonomous robot navigation within the dynamic unknown environment is of crucial significance for mobile robotic applications including robot navigation in last-mile delivery and robot-enabled automated supplies in industrial and hospital delivery applications. Current solutions still suffer from limitations, such as the robot cannot recognize unknown objects in real time and cannot navigate freely in a dynamic, narrow, and complex environment. We propose a complete software framework for autonomous robot perception and navigation within very dense obstacles and dense human crowds. First, we propose a framework that accurately detects and segments open-world object categories in a zero-shot manner, which overcomes the over-segmentation limitation of the current SAM model. Second, we proposed the distillation strategy to distill the knowledge to segment the free space of the walkway for robot navigation without the label. In the meantime, we design the trimming strategy that works collaboratively with distillation to enable lightweight inference to deploy the neural network on edge devices such as NVIDIA-TX2 or Xavier NX during autonomous navigation. Integrated into the robot navigation system, extensive experiments demonstrate that our proposed framework has achieved superior performance in terms of both accuracy and efficiency in robot scene perception and autonomous robot navigation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17083v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>动态未知环境中的自主机器人导航对移动机器人应用具有至关重要的意义，包括最后一英里配送中的机器人导航以及工业和医院配送应用中的机器人自动化供应。目前的解决方案仍然存在局限性，例如机器人无法实时识别未知物体，也无法在动态、狭窄和复杂的环境中自由导航。我们提出了一个完整的软件框架，用于在非常密集的障碍物和密集的人群中进行自主机器人感知和导航。首先，我们提出了一个框架，以零样本的方式准确地检测和分割开放世界对象类别，这克服了当前SAM模型的过度分割限制。其次，我们提出了提取知识的提取策略，以在没有标签的情况下分割机器人导航通道的自由空间。与此同时，我们设计了与蒸馏协同工作的微调策略，以实现轻量级推理，从而在自主导航期间将神经网络部署在NVIDIA-TX2或Xavier NX等边缘设备上。集成到机器人导航系统中，大量实验表明，我们提出的框架在机器人场景感知和自主机器人导航方面的准确性和效率都取得了优异的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17083v2" target="_blank">2401.17083v2</a>
                              </td>
                              <td>Online Robot Navigation and Manipulation with Distilled Vision-Language Models</td>
                              <td>Kangcheng Liu</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17083v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17083v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_06497v1_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Iris-SAM: Iris Segmentation Using a Foundational Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_06497v1_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_06497v1_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_06497v1_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Iris segmentation is a critical component of an iris biometric system and it involves extracting the annular iris region from an ocular image. In this work, we develop a pixel-level iris segmentation model from a foundational model, viz., Segment Anything Model (SAM), that has been successfully used for segmenting arbitrary objects. The primary contribution of this work lies in the integration of different loss functions during the fine-tuning of SAM on ocular images. In particular, the importance of Focal Loss is borne out in the fine-tuning process since it strategically addresses the class imbalance problem (i.e., iris versus non-iris pixels). Experiments on ND-IRIS-0405, CASIA-Iris-Interval-v3, and IIT-Delhi-Iris datasets convey the efficacy of the trained model for the task of iris segmentation. For instance, on the ND-IRIS-0405 dataset, an average segmentation accuracy of 99.58% was achieved, compared to the best baseline performance of 89.75%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_06497v1_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>虹膜分割是虹膜生物识别系统的关键组成部分，它涉及从眼部图像中提取环形虹膜区域。在这项工作中，我们从一个基本模型，即Segment Anything model（SAM），开发了一个像素级虹膜分割模型，该模型已成功用于分割任意对象。这项工作的主要贡献在于在对眼部图像进行SAM微调期间整合不同的损失函数。特别是，焦丢失的重要性在微调过程中得到了证实，因为它从战略上解决了类不平衡问题（即虹膜与非虹膜像素）。在ND-IRIS-0405、CASIA-IRIS-Interval-v3和IIT Delhi IRIS数据集上的实验表明了训练模型对虹膜分割任务的有效性。例如，在ND-IRIS-0405数据集上，平均分割准确率达到99.58%，而最佳基线性能为89.75%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.06497v1" target="_blank">2402.06497v1</a>
                              </td>
                              <td>Iris-SAM: Iris Segmentation Using a Foundational Model</td>
                              <td>Parisa Farmanifard</td>
                              <td>2024-02-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_06497v1_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.06497v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_16704v2_3">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Lookbehind-SAM: k steps back, 1 step forward</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_16704v2_3_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_16704v2_3_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_16704v2_3_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sharpness-aware minimization (SAM) methods have gained increasing popularity by formulating the problem of minimizing both loss value and loss sharpness as a minimax objective. In this work, we increase the efficiency of the maximization and minimization parts of SAM's objective to achieve a better loss-sharpness trade-off. By taking inspiration from the Lookahead optimizer, which uses multiple descent steps ahead, we propose Lookbehind, which performs multiple ascent steps behind to enhance the maximization step of SAM and find a worst-case perturbation with higher loss. Then, to mitigate the variance in the descent step arising from the gathered gradients across the multiple ascent steps, we employ linear interpolation to refine the minimization step. Lookbehind leads to a myriad of benefits across a variety of tasks. Particularly, we show increased generalization performance, greater robustness against noisy weights, as well as improved learning and less catastrophic forgetting in lifelong learning settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_16704v2_3_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>清晰度感知最小化（SAM）方法通过将最小化损失值和损失清晰度的问题公式化为最小-最大目标而越来越受欢迎。在这项工作中，我们提高了SAM目标的最大化和最小化部分的效率，以实现更好的损失-锐度权衡。通过从使用多个下降步骤的Lookahead优化器中获得灵感，我们提出了Lookbacking，其在后面执行多个上升步骤以增强SAM的最大化步骤并找到具有更高损失的最坏情况扰动。然后，为了减轻由多个上升步骤中收集的梯度引起的下降步骤中的方差，我们使用线性插值来细化最小化步骤。Lookbacking可以在各种任务中带来无数好处。特别是，我们展示了在终身学习环境中提高的泛化性能、对噪声权重的更强鲁棒性，以及改进的学习和更少的灾难性遗忘。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.16704v2" target="_blank">2307.16704v2</a>
                              </td>
                              <td>Lookbehind-SAM: k steps back, 1 step forward</td>
                              <td>Gonçalo Mordido</td>
                              <td>2023-07-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_16704v2_3"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.16704v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="CLIP"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_13254v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13254v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13254v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13254v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13254v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了CounterCurate，这是一个全面提高对比和生成多模式模型的视觉-语言组合推理能力的框架。特别是，我们发现了两个未被充分探索的关键问题：忽视基于物理的推理（计数和位置理解），以及使用高效的文本和图像生成模型进行语义反事实微调的潜力。我们的工作开创了一种解决这些差距的方法。我们首先关注CLIP和LLaVA等多模式模型在基于物理的组合推理中的近乎偶然的性能。然后，我们使用基础图像生成模型GLIGEN应用简单的数据扩充来生成微调数据，从而显著提高性能：在我们新策划的Flickr30k Positions基准上，CLIP和LLaVA分别提高了33%和37%。此外，我们还利用了高性能文本生成和图像生成模型的功能，特别是GPT-4V和DALLE-3，以策划具有挑战性的语义反事实，从而进一步增强在SugarCrepe等基准上的合成推理能力，其中CounterCurate的性能优于GPT-4V。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13254v1" target="_blank">2402.13254v1</a>
                              </td>
                              <td>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</td>
                              <td>Jianrui Zhang</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13254v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13254v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13250v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Video ReCap: Recursive Captioning of Hour-Long Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13250v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13250v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13250v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g., objects, scenes, atomic actions). However, most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities. We propose Video ReCap, a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels. The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently. We utilize a curriculum learning training scheme to learn the hierarchical structure of videos, starting from clip-level captions describing atomic actions, then focusing on segment-level descriptions, and concluding with generating summaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by augmenting Ego4D with 8,267 manually collected long-range video summaries. Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks, such as VideoQA on EgoSchema. Data, code, and models are available at: https://sites.google.com/view/vidrecap</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13250v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数视频字幕模型被设计为处理几秒钟的短视频片段，并输出描述低级视觉概念（例如，对象、场景、原子动作）的文本。然而，大多数真实世界的视频持续数分钟或数小时，并且具有跨越不同时间粒度的复杂层次结构。我们提出了Video ReCap，这是一种递归视频字幕模型，可以处理长度显著不同（从1秒到2小时）的视频输入，并在多个层次级别输出视频字幕。递归视频语言架构利用了不同视频层次结构之间的协同作用，可以有效地处理长达一小时的视频。我们利用课程学习培训方案来学习视频的层次结构，从描述原子动作的剪辑级字幕开始，然后关注片段级描述，最后生成一小时长的视频摘要。此外，我们通过用8267个手动收集的远程视频摘要扩充Ego4D，引入了Ego4D HCap数据集。我们的递归模型可以灵活地生成不同层次的字幕，同时也适用于其他复杂的视频理解任务，如EgoSchema上的VideoQA。数据、代码和模型可在以下位置获取：https://sites.google.com/view/vidrecap</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13250v1" target="_blank">2402.13250v1</a>
                              </td>
                              <td>Video ReCap: Recursive Captioning of Hour-Long Videos</td>
                              <td>Md Mohaiminul Islam</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13250v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13250v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13217v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VideoPrism: A Foundational Visual Encoder for Video Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13217v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13217v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13217v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13217v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍VideoPrism，这是一种通用视频编码器，可通过单个冻结模型处理各种视频理解任务。我们在异构语料库上预训练VideoPrism，该语料库包含36M个高质量视频字幕对和582M个具有噪声并行文本的视频片段（例如ASR转录本）。预训练方法通过语义视频嵌入的全局局部提取和令牌混洗方案改进了掩蔽自动编码，使VideoPrism能够主要关注视频模态，同时利用与视频相关的宝贵文本。我们在四大类视频理解任务上广泛测试了VideoPrism，从网络视频问答到科学简历，在33个视频理解基准中的30个上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13217v1" target="_blank">2402.13217v1</a>
                              </td>
                              <td>VideoPrism: A Foundational Visual Encoder for Video Understanding</td>
                              <td>Long Zhao</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13217v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13217v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_13088v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Slot-VLM: SlowFast Slots for Video-Language Modeling</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13088v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13088v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13088v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure. The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information. Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13088v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在大型语言模型（LLM）的进步的推动下，视频语言模型（VLM）正在为视频理解开辟新的领域。一个关键的挑战是开发一种有效的方法，将视频内容封装到一组具有代表性的令牌中，以与LLM保持一致。在这项工作中，我们介绍了Slot VLM，这是一种新的框架，旨在根据对象和事件的视觉表示生成语义分解的视频令牌，以促进LLM推理。特别地，我们设计了一个SlowFast Slots模块，即SF Slots，它自适应地将来自CLIP视觉编码器的密集视频令牌聚合到一组具有代表性的时隙。为了同时考虑空间对象细节和变化的时间动力学，SF Slots采用双分支结构。慢速槽分支专注于从高空间分辨率但低（慢）帧采样率的特征中提取以对象为中心的槽，强调详细的对象信息。相反，Fast Slots分支被设计为从高时间采样率但低空间分辨率的特征中学习以事件为中心的时隙。这些互补的插槽组合在一起形成视觉上下文，作为LLM的输入，实现高效的问答。我们的实验结果证明了我们的插槽VLM的有效性，它在视频问答方面实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13088v1" target="_blank">2402.13088v1</a>
                              </td>
                              <td>Slot-VLM: SlowFast Slots for Video-Language Modeling</td>
                              <td>Jiaqi Xu</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13088v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13088v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_08128v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Clockwork Diffusion: Efficient Generation With Model-Step Distillation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_08128v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_08128v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_08128v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This work aims to improve the efficiency of text-to-image diffusion models. While diffusion models use computationally expensive UNet-based denoising operations in every generation step, we identify that not all operations are equally relevant for the final output quality. In particular, we observe that UNet layers operating on high-res feature maps are relatively sensitive to small perturbations. In contrast, low-res feature maps influence the semantic layout of the final image and can often be perturbed with no noticeable change in the output. Based on this observation, we propose Clockwork Diffusion, a method that periodically reuses computation from preceding denoising steps to approximate low-res feature maps at one or more subsequent steps. For multiple baselines, and for both text-to-image generation and image editing, we demonstrate that Clockwork leads to comparable or improved perceptual scores with drastically reduced computational complexity. As an example, for Stable Diffusion v1.5 with 8 DPM++ steps we save 32% of FLOPs with negligible FID and CLIP change.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_08128v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项工作旨在提高文本到图像扩散模型的效率。虽然扩散模型在每个生成步骤中都使用计算成本高昂的基于UNet的去噪操作，但我们发现，并非所有操作都与最终输出质量同等相关。特别是，我们观察到，在高分辨率特征图上操作的UNet层对小扰动相对敏感。相反，低分辨率特征图会影响最终图像的语义布局，并且通常会受到干扰，而输出没有明显变化。基于这一观察结果，我们提出了Clockwork Diffusion，这是一种周期性地重复使用先前去噪步骤的计算，以在一个或多个后续步骤中近似低分辨率特征图的方法。对于多个基线，以及文本到图像的生成和图像编辑，我们证明了Clockwork可以显著降低计算复杂度，从而获得可比或改进的感知分数。例如，对于带有8个DPM++步骤的Stable Diffusion v1.5，我们节省了32%的FLOP，而FID和CLIP的变化可以忽略不计。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.08128v2" target="_blank">2312.08128v2</a>
                              </td>
                              <td>Clockwork Diffusion: Efficient Generation With Model-Step Distillation</td>
                              <td>Amirhossein Habibian</td>
                              <td>2023-12-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_08128v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.08128v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/qualcomm-ai-research/clockwork-diffusion" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_17261v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_17261v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_17261v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_17261v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstructing 3D objects from a single image guided by pretrained diffusion models has demonstrated promising outcomes. However, due to utilizing the case-agnostic rigid strategy, their generalization ability to arbitrary cases and the 3D consistency of reconstruction are still poor. In this work, we propose Consistent123, a case-aware two-stage method for highly consistent 3D asset reconstruction from one image with both 2D and 3D diffusion priors. In the first stage, Consistent123 utilizes only 3D structural priors for sufficient geometry exploitation, with a CLIP-based case-aware adaptive detection mechanism embedded within this process. In the second stage, 2D texture priors are introduced and progressively take on a dominant guiding role, delicately sculpting the details of the 3D model. Consistent123 aligns more closely with the evolving trends in guidance requirements, adaptively providing adequate 3D geometric initialization and suitable 2D texture refinement for different objects. Consistent123 can obtain highly 3D-consistent reconstruction and exhibits strong generalization ability across various objects. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art image-to-3D methods. See https://Consistent123.github.io for a more comprehensive exploration of our generated 3D assets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_17261v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在预训练的扩散模型的引导下，从单个图像重建3D对象已经证明了有希望的结果。然而，由于使用了不区分情况的刚性策略，它们对任意情况的泛化能力和重建的三维一致性仍然较差。在这项工作中，我们提出了Consistent123，这是一种案例感知的两阶段方法，用于从一张具有2D和3D扩散先验的图像中进行高度一致的3D资产重建。在第一阶段，Consistent123仅利用3D结构先验进行充分的几何利用，并在此过程中嵌入了基于CLIP的案例感知自适应检测机制。在第二阶段，引入2D纹理先验，并逐渐发挥主导作用，精细地雕刻3D模型的细节。Consistent123更紧密地与制导要求的发展趋势保持一致，为不同的物体自适应地提供适当的3D几何初始化和适当的2D纹理细化。Consistent123可以获得高度3D一致性的重建，并在各种对象上表现出强大的泛化能力。定性和定量实验表明，我们的方法显著优于最先进的图像到三维方法。看见https://Consistent123.github.io以便对我们生成的3D资产进行更全面的探索。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.17261v2" target="_blank">2309.17261v2</a>
                              </td>
                              <td>Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors</td>
                              <td>Yukang Lin</td>
                              <td>2023-09-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_17261v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.17261v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_12927v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_12927v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_12927v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_12927v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_12927v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>生成对抗性网络（GANs）的最新进展和扩散模型的出现大大简化了高度逼真和可广泛访问的合成内容的生产。因此，迫切需要有效的通用检测机制来减轻deepfakes带来的潜在风险。在本文中，我们探讨了预训练的视觉语言模型（VLM）与最新的通用深度伪造检测自适应方法相结合的有效性。根据之前在该领域的研究，我们仅使用单个数据集（ProGAN）来调整CLIP用于深度伪造检测。然而，与之前的研究不同，以前的研究只依赖CLIP的视觉部分而忽略其文本成分，我们的分析表明，保留文本部分至关重要。因此，我们使用的简单而轻量级的基于提示调整的自适应策略比以前的SOTA方法高出5.01%mAP和6.61%的准确率，同时使用了不到三分之一的训练数据（与720k相比，200k张图像）。为了评估我们提出的模型在现实世界中的适用性，我们对各种场景进行了全面评估。这涉及对来自21个不同数据集的图像进行严格测试，包括基于GAN、基于扩散和商业工具生成的图像。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.12927v1" target="_blank">2402.12927v1</a>
                              </td>
                              <td>CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection</td>
                              <td>Sohail Ahmed Khan</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_12927v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.12927v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_12765v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GOOD: Towards Domain Generalized Orientated Object Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_12765v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_12765v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_12765v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Oriented object detection has been rapidly developed in the past few years, but most of these methods assume the training and testing images are under the same statistical distribution, which is far from reality. In this paper, we propose the task of domain generalized oriented object detection, which intends to explore the generalization of oriented object detectors on arbitrary unseen target domains. Learning domain generalized oriented object detectors is particularly challenging, as the cross-domain style variation not only negatively impacts the content representation, but also leads to unreliable orientation predictions. To address these challenges, we propose a generalized oriented object detector (GOOD). After style hallucination by the emerging contrastive language-image pre-training (CLIP), it consists of two key components, namely, rotation-aware content consistency learning (RAC) and style consistency learning (SEC). The proposed RAC allows the oriented object detector to learn stable orientation representation from style-diversified samples. The proposed SEC further stabilizes the generalization ability of content representation from different image styles. Extensive experiments on multiple cross-domain settings show the state-of-the-art performance of GOOD. Source code will be publicly available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_12765v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>定向目标检测在过去几年中得到了快速发展，但大多数方法都假设训练和测试图像处于相同的统计分布下，这与现实相去甚远。在本文中，我们提出了域广义定向目标检测的任务，旨在探索定向目标检测器在任意不可见目标域上的泛化。学习域广义定向对象检测器尤其具有挑战性，因为跨域风格的变化不仅会对内容表示产生负面影响，还会导致不可靠的定向预测。为了解决这些挑战，我们提出了一种广义定向对象检测器（GOOD）。经过新兴的对比语言图像预训练（CLIP）的风格幻觉，它由两个关键组成部分组成，即旋转感知内容一致性学习（RAC）和风格一致性学习。所提出的RAC允许定向对象检测器从风格多样化的样本中学习稳定的定向表示。所提出的SEC进一步稳定了来自不同图像风格的内容表示的泛化能力。在多个跨域设置上进行的大量实验显示了GOOD最先进的性能。源代码将公开。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.12765v1" target="_blank">2402.12765v1</a>
                              </td>
                              <td>GOOD: Towards Domain Generalized Orientated Object Detection</td>
                              <td>Qi Bi</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_12765v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.12765v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08949v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08949v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08949v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08949v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge modalities, EasyGen leverages BiDiffuser, a bidirectional conditional diffusion model, to foster more efficient modality interactions. EasyGen achieves text generation by training a projection layer linking BiDiffuser and an LLM, and facilities image generation by training an adapter to align the LLM's text space with the BiDiffuser's image space. Comprehensive quantitative and qualitative experiments show that EasyGen excels in data-efficient training, high-quality image generation, and extendibility, effectively addressing the challenges in multimodal generation. The source code is available at https://github.com/zxy556677/EasyGen.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08949v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了EasyGen，这是一种高效的模型，旨在通过利用扩散模型和大型语言模型（LLM）的功能来增强多模式理解和生成。与主要依赖CLIP或ImageBind等编码器并需要大量训练数据来桥接模态的现有多模态模型不同，EasyGen利用双向条件扩散模型BiDiffuser来促进更高效的模态交互。EasyGen通过训练连接BiDiffuser和LLM的投影层来实现文本生成，并通过训练适配器将LLM的文本空间与BiDiffuser的图像空间对齐来促进图像生成。综合的定量和定性实验表明，EasyGen在数据高效训练、高质量图像生成和可扩展性方面表现出色，有效应对了多模式生成的挑战。源代码位于https://github.com/zxy556677/EasyGen.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08949v2" target="_blank">2310.08949v2</a>
                              </td>
                              <td>EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs</td>
                              <td>Xiangyu Zhao</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08949v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08949v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zxy556677/easygen" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_12613v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analysis of Using Sigmoid Loss for Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_12613v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_12613v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_12613v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive learning has emerged as a prominent branch of self-supervised learning for several years. Especially, CLIP, which applies contrastive learning to large sets of captioned images, has garnered significant attention. Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid loss instead of the standard InfoNCE loss. SigLIP achieves the performance comparable to CLIP in a more efficient manner by eliminating the need for a global view. However, theoretical understanding of using the sigmoid loss in contrastive learning is underexplored. In this paper, we provide a theoretical analysis of using the sigmoid loss in contrastive learning, in the perspective of the geometric structure of learned embeddings. First, we propose the double-Constant Embedding Model (CCEM), a framework for parameterizing various well-known embedding structures by a single variable. Interestingly, the proposed CCEM is proven to contain the optimal embedding with respect to the sigmoid loss. Second, we mathematically analyze the optimal embedding minimizing the sigmoid loss for contrastive learning. The optimal embedding ranges from simplex equiangular-tight-frame to antipodal structure, depending on the temperature parameter used in the sigmoid loss. Third, our experimental results on synthetic datasets coincide with the theoretical results on the optimal embedding structures.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_12613v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>多年来，对比学习已成为自我监督学习的一个突出分支。尤其是将对比学习应用于大型字幕图像集的CLIP，引起了人们的极大关注。最近，已经提出了CLIP的一种变体SigLIP，它使用S形损失而不是标准的InfoNCE损失。SigLIP通过消除对全局视图的需要，以更有效的方式实现了与CLIP相当的性能。然而，在对比学习中使用S形损失的理论理解还没有得到充分的探索。在本文中，我们从学习嵌入的几何结构的角度，对在对比学习中使用S形损失进行了理论分析。首先，我们提出了双常数嵌入模型（CCEM），这是一个通过单个变量参数化各种已知嵌入结构的框架。有趣的是，所提出的CCEM被证明包含关于S形损失的最优嵌入。其次，我们从数学上分析了对比学习中最小化S形损失的最优嵌入。最佳嵌入范围从单纯形等角紧框架到对足结构，这取决于S形损失中使用的温度参数。第三，我们在合成数据集上的实验结果与最优嵌入结构的理论结果一致。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.12613v1" target="_blank">2402.12613v1</a>
                              </td>
                              <td>Analysis of Using Sigmoid Loss for Contrastive Learning</td>
                              <td>Chungpa Lee</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_12613v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.12613v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_11106v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The importance of feature preprocessing for differentially private linear optimization</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_11106v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_11106v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_11106v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Training machine learning models with differential privacy (DP) has received increasing interest in recent years. One of the most popular algorithms for training differentially private models is differentially private stochastic gradient descent (DPSGD) and its variants, where at each step gradients are clipped and combined with some noise. Given the increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints? Towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization. In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs an optimality gap proportional to the maximum Euclidean norm of features over all samples. We then propose an algorithm called DPSGD-F, which combines DPSGD with feature preprocessing and prove that for classification tasks, it incurs an optimality gap proportional to the diameter of the features $\max_{x, x' \in D} \|x - x'\|_2$. We finally demonstrate the practicality of our algorithm on image classification benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_11106v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，训练具有差分隐私（DP）的机器学习模型越来越受到人们的关注。用于训练差分私有模型的最流行的算法之一是差分私有随机梯度下降（DPSGD）及其变体，其中在每个步骤处，梯度被剪裁并与一些噪声相结合。考虑到DPSGD的使用量不断增加，我们提出了一个问题：在隐私约束下，单独的DPSGD是否足以为每个数据集找到一个好的最小化器？为了回答这个问题，我们表明，即使对于线性分类的简单情况，与非私有优化不同，（私有）特征预处理对于差异私有优化也是至关重要的。详细地说，我们首先从理论上证明，存在一个例子，在没有特征预处理的情况下，DPSGD在所有样本上产生与特征的最大欧几里得范数成比例的最优性间隙。然后，我们提出了一种称为DPSGD-F的算法，该算法将DPSGD与特征预处理相结合，并证明了对于分类任务，它产生了与特征$\max_{x，x'\in D}-x-x'\|_2$的直径成比例的最优性间隙。最后，我们在图像分类基准上展示了算法的实用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.11106v2" target="_blank">2307.11106v2</a>
                              </td>
                              <td>The importance of feature preprocessing for differentially private linear optimization</td>
                              <td>Ziteng Sun</td>
                              <td>2023-07-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_11106v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.11106v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_12336v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_12336v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_12336v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_12336v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the VLM is required. The code and robust models are available at https://github.com/chs20/RobustVLM</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_12336v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>OpenFlamingo、LLaVA和GPT-4等多模式基础模型越来越多地用于各种现实世界的任务。先前的工作表明，这些模型非常容易受到视觉模态上的对抗性攻击。这些攻击可能被用来传播虚假信息或诈骗用户，从而构成重大风险，这使得大型多模态基础模型的稳健性成为一个紧迫的问题。CLIP模型或其变体之一在许多视觉语言模型（VLM）中用作冻结视觉编码器，例如LLaVA和OpenFlamingo。我们提出了一种无监督对抗性微调方案，以获得鲁棒的CLIP视觉编码器，该编码器在依赖于CLIP的所有视觉下游任务（VLM，零样本分类）上产生鲁棒性。特别是，我们表明，一旦用我们的稳健模型替换了原始的CLIP模型，恶意第三方提供被操纵的图像就不再可能对VLM的用户进行隐形攻击。无需对VLM进行再培训或微调。代码和稳健模型可在https://github.com/chs20/RobustVLM</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.12336v1" target="_blank">2402.12336v1</a>
                              </td>
                              <td>Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models</td>
                              <td>Christian Schlarmann</td>
                              <td>2024-02-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_12336v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.12336v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/chs20/robustvlm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_12065v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_12065v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_12065v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_12065v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Proximal Policy Optimization algorithm employing a clipped surrogate objective (PPO-Clip) is a prominent exemplar of the policy optimization methods. However, despite its remarkable empirical success, PPO-Clip lacks theoretical substantiation to date. In this paper, we contribute to the field by establishing the first global convergence results of a PPO-Clip variant in both tabular and neural function approximation settings. Our findings highlight the $O(1/\sqrt{T})$ min-iterate convergence rate specifically in the context of neural function approximation. We tackle the inherent challenges in analyzing PPO-Clip through three central concepts: (i) We introduce a generalized version of the PPO-Clip objective, illuminated by its connection with the hinge loss. (ii) Employing entropic mirror descent, we establish asymptotic convergence for tabular PPO-Clip with direct policy parameterization. (iii) Inspired by the tabular analysis, we streamline convergence analysis by introducing a two-step policy improvement approach. This decouples policy search from complex neural policy parameterization using a regression-based update scheme. Furthermore, we gain deeper insights into the efficacy of PPO-Clip by interpreting these generalized objectives. Our theoretical findings also mark the first characterization of the influence of the clipping mechanism on PPO-Clip convergence. Importantly, the clipping range affects only the pre-constant of the convergence rate.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_12065v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>采用截断代理目标的近端策略优化算法（PPO-Clip）是策略优化方法的一个突出例子。然而，尽管PPO Clip在经验上取得了显著的成功，但迄今为止缺乏理论证据。在本文中，我们通过在表格和神经函数近似设置中建立PPO-Clip变量的第一个全局收敛结果，为该领域做出了贡献。我们的发现强调了$O（1/\sqrt｛T｝）$min迭代收敛速度，特别是在神经函数近似的情况下。我们通过三个中心概念来解决分析PPO Clip的固有挑战：（i）我们介绍了PPO Clips目标的广义版本，通过其与铰链损耗的联系来说明。（ii）利用熵镜下降，我们建立了具有直接策略参数化的表格式PPO Clip的渐近收敛性。（iii）受表格分析的启发，我们通过引入两步政策改进方法来简化收敛性分析。这使用基于回归的更新方案将策略搜索与复杂的神经策略参数化解耦。此外，通过解释这些广义目标，我们对PPO Clip的疗效有了更深入的了解。我们的理论发现也标志着首次表征了剪切机制对PPO-Clip收敛的影响。重要的是，削波范围仅影响收敛速率的预常数。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.12065v2" target="_blank">2312.12065v2</a>
                              </td>
                              <td>PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping</td>
                              <td>Nai-Chieh Huang</td>
                              <td>2023-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_12065v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.12065v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11973v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Bayesian Active Learning for Censored Regression</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11973v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11973v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11973v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Bayesian active learning is based on information theoretical approaches that focus on maximising the information that new observations provide to the model parameters. This is commonly done by maximising the Bayesian Active Learning by Disagreement (BALD) acquisitions function. However, we highlight that it is challenging to estimate BALD when the new data points are subject to censorship, where only clipped values of the targets are observed. To address this, we derive the entropy and the mutual information for censored distributions and derive the BALD objective for active learning in censored regression ($\mathcal{C}$-BALD). We propose a novel modelling approach to estimate the $\mathcal{C}$-BALD objective and use it for active learning in the censored setting. Across a wide range of datasets and models, we demonstrate that $\mathcal{C}$-BALD outperforms other Bayesian active learning methods in censored regression.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11973v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>贝叶斯主动学习基于信息理论方法，该方法侧重于最大化新观测提供给模型参数的信息。这通常是通过最大化贝叶斯差异主动学习（BALD）获取函数来实现的。然而，我们强调，当新的数据点受到审查时，估计BALD是具有挑战性的，因为只观察到目标的截断值。为了解决这一问题，我们推导了删失分布的熵和互信息，并推导了在删失回归中主动学习的BALD目标（$\mathcal{C}$-BALD）。我们提出了一种新的建模方法来估计$\mathcal{C}$-BALD目标，并将其用于审查环境中的主动学习。在广泛的数据集和模型中，我们证明$\mathcal{C}$-BALD在截尾回归中优于其他贝叶斯主动学习方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11973v1" target="_blank">2402.11973v1</a>
                              </td>
                              <td>Bayesian Active Learning for Censored Regression</td>
                              <td>Frederik Boe Hüttel</td>
                              <td>2024-02-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11973v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11973v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_15072v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_15072v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_15072v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_15072v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_15072v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大语言模型（LLM）和多模式技术的不断成熟，通用多模式大语言模型的发展激增，在解释自然图像方面提供了重要的应用。然而，病理学领域在很大程度上仍未开发，特别是在收集高质量数据和设计全面的模型框架方面。为了弥补病理学MLLMs的差距，我们推出了PathAsst，这是一款多模式生成基础人工智能助手，旨在彻底改变病理学中的诊断和预测分析。PathAsst的开发涉及三个关键步骤：数据采集、CLIP模型自适应和PathAsst多模式生成能力的训练。首先，我们从权威来源收集了超过207K个高质量的病理学图像文本对。利用ChatGPT的先进功能，我们生成了超过180K的指令如下示例。此外，我们根据专门为调用我们准备的八个病理学特定子模型而定制的数据，设计了额外的说明，使PathAsst能够与这些模型有效协作，增强其诊断能力。其次，通过利用收集的数据，我们构建了PathCLIP，一个病理学专用的CLIP，以增强PathAsst解释病理学图像的能力。最后，我们将PathCLIP与Vicuna-13b集成，并利用病理学特定的指令调整数据来增强PathAsst的多模式生成能力，并增强其与子模型的协同作用。PathAsst的实验结果显示了利用人工智能驱动的生成基础模型改进病理诊断和治疗过程的潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.15072v2" target="_blank">2305.15072v2</a>
                              </td>
                              <td>PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology</td>
                              <td>Yuxuan Sun</td>
                              <td>2023-05-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_15072v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.15072v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/superjamessyx/generative-foundation-ai-assistant-for-pathology" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11816v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11816v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11816v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11816v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-learned features. Extensive experiments conducted on various publicly available benchmarks validate the effectiveness of our proposed framework. In addition, we demonstrate that the proposed MCL can be adapted to a variety of popular contrastive learning backbones and boost their performance by learning features that could not be gained from standard contrastive learning procedures.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11816v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督对比学习已成为从未标记数据中获得高质量表示的一种强大方法。然而，最近在标准对比学习（$e.g.$，SimCLR，CLIP）中发现了特征抑制：在单个端到端训练阶段，对比模型只捕获对比视图中共享信息的一部分，而忽略其他潜在有用信息。在特征抑制的情况下，对比模型往往无法学习到能够用于各种下游任务的足够表示。为了缓解特征抑制问题，确保对比模型学习全面表征，我们开发了一种新的多阶段对比学习（MCL）框架。与通常导致特征抑制的标准对比学习不同，MCL在保持良好学习特征的同时，逐步学习前一阶段未经探索的新特征。在各种公开的基准上进行的大量实验验证了我们提出的框架的有效性。此外，我们还证明了所提出的MCL可以适应各种流行的对比学习主干，并通过学习标准对比学习程序无法获得的特征来提高其性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11816v1" target="_blank">2402.11816v1</a>
                              </td>
                              <td>Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before</td>
                              <td>Jihai Zhang</td>
                              <td>2024-02-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11816v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11816v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07494v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TALL: Thumbnail Layout for Deepfake Video Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07494v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07494v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07494v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The growing threats of deepfakes to society and cybersecurity have raised enormous public concerns, and increasing efforts have been devoted to this critical topic of deepfake video detection. Existing video methods achieve good performance but are computationally intensive. This paper introduces a simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies. Specifically, consecutive frames are masked in a fixed position in each frame to improve generalization, then resized to sub-images and rearranged into a pre-defined layout as the thumbnail. TALL is model-agnostic and extremely simple by only modifying a few lines of code. Inspired by the success of vision transformers, we incorporate TALL into Swin Transformer, forming an efficient and effective method TALL-Swin. Extensive experiments on intra-dataset and cross-dataset validate the validity and superiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79$\%$ AUC on the challenging cross-dataset task, FaceForensics++ $\to$ Celeb-DF. The code is available at https://github.com/rainy-xu/TALL4Deepfake.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07494v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>deepfakes对社会和网络安全的威胁越来越大，这引起了公众的极大关注，人们越来越多地致力于deepfake视频检测这一关键话题。现有的视频方法实现了良好的性能，但是计算密集。本文介绍了一种简单而有效的策略——缩略图布局（TALL），它将视频剪辑转换为预定义的布局，以实现空间和时间相关性的保留。具体地说，连续的帧在每个帧中的固定位置被屏蔽以提高泛化能力，然后调整大小为子图像，并重新排列为预定义的布局作为缩略图。TALL与模型无关，并且只需修改几行代码就非常简单。受视觉转换器成功的启发，我们将TALL融入Swin Transformer，形成了一种高效、有效的TALL Swin方法。在数据集内和跨数据集上进行的大量实验验证了TALL和SOTA TALL Swin的有效性和优越性。TALL Swin在具有挑战性的跨数据集任务FaceForensics++$\to$Celeb DF上实现了90.79$\%%$AUC。代码位于https://github.com/rainy-xu/TALL4Deepfake.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07494v3" target="_blank">2307.07494v3</a>
                              </td>
                              <td>TALL: Thumbnail Layout for Deepfake Video Detection</td>
                              <td>Yuting Xu</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07494v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07494v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/rainy-xu/tall4deepfake" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_18961v6_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_18961v6_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_18961v6_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_18961v6_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_18961v6_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>零样本异常检测（ZSAD）需要使用辅助数据训练的检测模型来检测异常，而无需在目标数据集中使用任何训练样本。当由于各种问题（如数据隐私）而无法访问训练数据时，这是一项至关重要的任务，但这是一个挑战，因为模型需要推广到不同领域的异常，其中前景对象、异常区域和背景特征（如不同产品/器官上的缺陷/肿瘤）的外观可能会有很大差异。最近，大型预先训练的视觉语言模型（VLM），如CLIP，在包括异常检测在内的各种视觉任务中表现出强大的零样本识别能力。然而，它们的ZSAD性能较弱，因为VLM更侧重于对前景对象的类语义建模，而不是对图像中的异常/正常性建模。在本文中，我们介绍了一种新的方法，即AnomalyCLIP，以使CLIP适应不同领域的精确ZSAD。AnomalyCLIP的关键见解是学习对象不可知的文本提示，无论图像的前景对象如何，都可以捕捉图像中的一般正常和异常。这使我们的模型能够专注于异常图像区域，而不是对象语义，从而实现对不同类型对象的广义正态和异常识别。在17个真实世界异常检测数据集上进行的大规模实验表明，AnomalyCLIP在来自各种缺陷检测和医学成像领域的具有高度不同类别语义的数据集中实现了检测和分割异常的优异零样本性能。代码将在提供https://github.com/zqhang/AnomalyCLIP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.18961v6" target="_blank">2310.18961v6</a>
                              </td>
                              <td>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection</td>
                              <td>Qihang Zhou</td>
                              <td>2023-10-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_18961v6_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.18961v6" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zqhang/anomalyclip" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11235v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11235v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11235v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11235v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to zero-shot transferability in graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models. Especially, ZeroG, as a zero-shot method, can even achieve results comparable to those of semi-supervised learning on Pubmed.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11235v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着大型语言模型等基础模型的发展，零样本迁移学习变得越来越重要。GPT-4等NLP模型的生成能力和CLIP等CV模型的基于检索的方法突出了这一点，这两种方法都有效地弥合了可见数据和不可见数据之间的差距。在图形学习领域，新图形的不断出现和人类标记的挑战也扩大了零样本迁移学习的必要性，推动了对可以在不同图形数据之间进行归纳而不需要特定于数据集和特定于标签的微调的方法的探索。在这项研究中，我们通过引入ZeroG，将这种范式扩展到图中的零样本可转移性，ZeroG是一种专门用于实现跨数据集泛化的新框架。针对特征错位、标签空间不匹配和负迁移等固有挑战，我们利用语言模型对节点属性和类语义进行编码，确保数据集的特征维度一致。我们还提出了一个基于提示的子图采样模块，该模块分别使用提示节点和邻域聚合来丰富提取的子图的语义信息和结构信息。我们进一步采用了轻量级微调策略，降低了过度拟合的风险，并保持了语言模型的零样本学习效率。结果强调了我们的模型在实现显著的跨数据集零样本可转移性方面的有效性，为图形基础模型的开发开辟了途径。特别是，ZeroG作为一种零样本方法，甚至可以在Pubmed上获得与半监督学习相当的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11235v1" target="_blank">2402.11235v1</a>
                              </td>
                              <td>ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs</td>
                              <td>Yuhan Li</td>
                              <td>2024-02-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11235v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11235v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11159v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11159v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11159v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11159v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper delves into the critical challenge of understanding the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the main subject discussed in the news text. To serve the challenge, we introduce \textsc{NewsTT}, a manually annotated dataset of news thumbnail image and text pairs. We found that pretrained vision and language models, such as CLIP and BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, a pretrained model could not have the ability to match its visual and textual appearances. To fill the gap, we propose CFT-CLIP, a counterfactual text-guided contrastive language-image pretraining framework. We hypothesize that learning to contrast news text with its counterfactual, of which named entities are replaced, can enhance the cross-modal matching ability in the target task. Evaluation experiments using NewsTT show that CFT-CLIP outperforms the pretrained models, such as CLIP and BLIP-2. Our code and data will be made accessible to the public after the paper is accepted.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11159v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文深入探讨了理解新闻缩略图的代表性这一关键挑战，当文章在社交媒体上传播时，缩略图通常是读者的第一视觉参与。我们关注的是新闻图像是否代表了新闻文本中讨论的主要主题。为了应对这一挑战，我们引入了\textsc｛NewsTT｝，这是一个由新闻缩略图和文本对组成的手动注释数据集。我们发现，预训练的视觉和语言模型，如CLIP和BLIP-2，很难完成这项任务。由于新闻主题经常涉及命名实体或专有名词，因此预先训练的模型无法匹配其视觉和文本外观。为了填补这一空白，我们提出了一个反事实文本引导的对比语言图像预训练框架CFT-CLIP。我们假设，学习将新闻文本与其反事实文本进行对比，其中命名实体被替换，可以增强目标任务中的跨模态匹配能力。使用NewsTT的评估实验表明，CFT-CLIP优于预训练的模型，如CLIP和BLIP-2。论文被接受后，我们的代码和数据将向公众开放。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11159v1" target="_blank">2402.11159v1</a>
                              </td>
                              <td>Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining</td>
                              <td>Yejun Yoon</td>
                              <td>2024-02-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11159v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11159v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_00626v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_00626v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_00626v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_00626v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Typographic Attacks, which involve pasting misleading text onto an image, were noted to harm the performance of Vision-Language Models like CLIP. However, the susceptibility of recent Large Vision-Language Models to these attacks remains understudied. Furthermore, prior work's Typographic attacks against CLIP randomly sample a misleading class from a predefined set of categories. However, this simple strategy misses more effective attacks that exploit LVLM(s) stronger language skills. To address these issues, we first introduce a benchmark for testing Typographic attacks against LVLM(s). Moreover, we introduce two novel and more effective \textit{Self-Generated} attacks which prompt the LVLM to generate an attack against itself: 1) Class Based Attack where the LVLM (e.g. LLaVA) is asked which deceiving class is most similar to the target class and 2) Descriptive Attacks where a more advanced LVLM (e.g. GPT4-V) is asked to recommend a Typographic attack that includes both a deceiving class and description. Using our benchmark, we uncover that Self-Generated attacks pose a significant threat, reducing LVLM(s) classification performance by up to 33\%. We also uncover that attacks generated by one model (e.g. GPT-4V or LLaVA) are effective against the model itself and other models like InstructBLIP and MiniGPT4. Code: \url{https://github.com/mqraitem/Self-Gen-Typo-Attack}</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_00626v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>打字攻击涉及将误导性文本粘贴到图像上，会损害CLIP等视觉语言模型的性能。然而，最近的大型视觉语言模型对这些攻击的易感性仍然研究不足。此外，先前的工作对CLIP的排版攻击从预定义的一组类别中随机抽取了一个误导性的类。然而，这种简单的策略错过了利用LVLM更强的语言技能的更有效的攻击。为了解决这些问题，我们首先引入了一个测试针对LVLM的打字攻击的基准。此外，我们介绍了两种新颖且更有效的\textit｛Self-Generated｝攻击，它们促使LVLM生成针对自身的攻击：1）基于类的攻击，其中LVLM（例如LLaVA）被问到哪一个欺骗类与目标类最相似；2）描述性攻击，其中更高级的LVLM（如GPT4-V）被要求推荐一种包括欺骗性类别和描述的打字攻击。使用我们的基准，我们发现自生成攻击构成了重大威胁，使LVLM的分类性能降低了33%。我们还发现，由一个模型（例如GPT-4V或LLaVA）生成的攻击对模型本身和其他模型（如InstructionBLIP和MiniGPT4）有效。代码：\url{https://github.com/mqraitem/Self-Gen-Typo-Attack}</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.00626v2" target="_blank">2402.00626v2</a>
                              </td>
                              <td>Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks</td>
                              <td>Maan Qraitem</td>
                              <td>2024-02-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_00626v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.00626v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mqraitem/self-gen-typo-attack" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10631v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10631v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10631v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10631v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit configurations on general language understanding and complex reasoning benchmarks. Notably, BitDistiller is shown to be more cost-effective, demanding fewer data and training resources. The code is available at https://github.com/DD-DuDa/BitDistiller.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10631v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大型语言模型（LLM）的升级在自然语言处理方面取得了令人印象深刻的进展，但也带来了重大的部署挑战。权重量化已成为一种广泛采用的解决方案，以减少内存和计算需求。本文介绍了BitDistiller，这是一个将量化感知训练（QAT）与知识蒸馏（KD）相结合的框架，以提高LLM在超低精度（低于4位）下的性能。具体而言，BitDistiller首先结合了一种量身定制的不对称量化和剪裁技术，以最大限度地保持量化权重的保真度，然后提出了一种新的置信度感知Kullback-Leibler发散（CAKLD）目标，该目标以自蒸馏的方式使用，以实现更快的收敛和卓越的模型性能。经验评估表明，BitDistiller在通用语言理解和复杂推理基准方面，在3位和2位配置方面都显著优于现有方法。值得注意的是，BitDistiller被证明更具成本效益，需要更少的数据和培训资源。代码位于https://github.com/DD-DuDa/BitDistiller.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10631v1" target="_blank">2402.10631v1</a>
                              </td>
                              <td>BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation</td>
                              <td>Dayou Du</td>
                              <td>2024-02-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10631v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10631v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/dd-duda/bitdistiller" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_15103v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Contrastive Learning Is Spectral Clustering On Similarity Graph</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_15103v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_15103v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_15103v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the Kernel-InfoNCE loss, incorporating mixtures of kernel functions that outperform the standard Gaussian kernel on several vision datasets. The code is available at https://github.com/yifanzhang-pro/Kernel-InfoNCE.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_15103v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比学习是一种强大的自我监督学习方法，但我们对它是如何工作的以及为什么工作的理论理解有限。在本文中，我们证明了标准InfoNCE损失的对比学习等价于相似图上的谱聚类。使用这种等价性作为构建块，我们将分析扩展到CLIP模型，并严格描述相似的多模态对象是如何嵌入在一起的。受我们理论见解的启发，我们引入了Kernel InfoNCE损失，结合了在几个视觉数据集上优于标准高斯核的核函数的混合。代码位于https://github.com/yifanzhang-pro/Kernel-InfoNCE.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.15103v3" target="_blank">2303.15103v3</a>
                              </td>
                              <td>Contrastive Learning Is Spectral Clustering On Similarity Graph</td>
                              <td>Yifan Zhang</td>
                              <td>2023-03-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_15103v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.15103v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/yifanzhang-pro/Kernel-InfoNCE" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yifanzhang-pro/kernel-infonce" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10376v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10376v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10376v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10376v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representations, maintaining equivalent downstream performance while significantly improving their interpretability. We also demonstrate several use cases of SpLiCE representations including detecting spurious correlations, model editing, and quantifying semantic shifts in datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10376v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CLIP嵌入在广泛的计算机视觉任务中表现出了非凡的性能。然而，这些高维、密集的矢量表示不容易解释，限制了它们在需要透明度的下游应用中的有用性。在这项工作中，我们从经验上证明了CLIP的潜在空间是高度结构化的，因此CLIP表示可以分解为其潜在的语义成分。我们利用这一理解提出了一种新的方法，稀疏线性概念嵌入（SpLiCE），用于将CLIP表示转换为人类可解释概念的稀疏线性组合。与之前的工作不同，SpLiCE不需要概念标签，可以事后应用。通过对多个真实世界数据集的广泛实验，我们验证了SpLiCE输出的表示可以解释甚至取代传统的密集CLIP表示，在显著提高其可解释性的同时保持了等效的下游性能。我们还演示了SpLiCE表示的几个用例，包括检测虚假相关性、模型编辑和量化数据集中的语义变化。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10376v1" target="_blank">2402.10376v1</a>
                              </td>
                              <td>Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)</td>
                              <td>Usha Bhalla</td>
                              <td>2024-02-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10376v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10376v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ai4life-group/splice" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10099v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Any-Shift Prompting for Generalization over Distributions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10099v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10099v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10099v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image-language models with prompt learning have shown remarkable advances in numerous downstream vision tasks. Nevertheless, conventional prompt learning methods overfit their training distribution and lose the generalization ability on test distributions. To improve generalization across various distribution shifts, we propose any-shift prompting: a general probabilistic inference framework that considers the relationship between training and test distributions during prompt learning. We explicitly connect training and test distributions in the latent space by constructing training and test prompts in a hierarchical architecture. Within this framework, the test prompt exploits the distribution relationships to guide the generalization of the CLIP image-language model from training to any test distribution. To effectively encode the distribution information and their relationships, we further introduce a transformer inference network with a pseudo-shift training mechanism. The network generates the tailored test prompt with both training and test information in a feedforward pass, avoiding extra training costs at test time. Extensive experiments on twenty-three datasets demonstrate the effectiveness of any-shift prompting on the generalization over various distribution shifts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10099v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有即时学习的图像语言模型在许多下游视觉任务中显示出显著的进步。然而，传统的即时学习方法过于拟合其训练分布，并失去了对测试分布的泛化能力。为了提高各种分布变化的泛化能力，我们提出了任何变化提示：一个通用的概率推理框架，它考虑了即时学习过程中训练和测试分布之间的关系。我们通过在层次结构中构建训练和测试提示，明确地将潜在空间中的训练和测试分布联系起来。在这个框架内，测试提示利用分布关系来指导CLIP图像语言模型从训练到任何测试分布的泛化。为了有效地对分布信息及其关系进行编码，我们进一步引入了一种具有伪移位训练机制的变换器推理网络。该网络在前馈过程中使用训练和测试信息生成定制的测试提示，避免了测试时的额外训练成本。在23个数据集上进行的大量实验表明，在各种分布偏移上，任何偏移提示对泛化的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10099v1" target="_blank">2402.10099v1</a>
                              </td>
                              <td>Any-Shift Prompting for Generalization over Distributions</td>
                              <td>Zehao Xiao</td>
                              <td>2024-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10099v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10099v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_09816v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_09816v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_09816v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_09816v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Deep Learning (DL) is undergoing a paradigm shift with the emergence of foundation models, aptly named by their crucial, yet incomplete nature. In this work, we focus on Contrastive Language-Image Pre-training (CLIP), an open-vocabulary foundation model, which achieves high accuracy across many image classification tasks and is often competitive with a fully supervised baseline without being explicitly trained. Nevertheless, there are still domains where zero-shot CLIP performance is far from optimal, such as Remote Sensing (RS) and medical imagery. These domains do not only exhibit fundamentally different distributions compared to natural images, but also commonly rely on complementary modalities, beyond RGB, to derive meaningful insights. To this end, we propose a methodology for the purpose of aligning distinct RS imagery modalities with the visual and textual modalities of CLIP. Our two-stage procedure, comprises of robust fine-tuning CLIP in order to deal with the distribution shift, accompanied by the cross-modal alignment of a RS modality encoder, in an effort to extend the zero-shot capabilities of CLIP. We ultimately demonstrate our method on the tasks of RS imagery classification and cross-modal retrieval. We empirically show that both robust fine-tuning and cross-modal alignment translate to significant performance gains, across several RS benchmark datasets. Notably, these enhancements are achieved without the reliance on textual descriptions, without introducing any task-specific parameters, without training from scratch and without catastrophic forgetting.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_09816v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着基础模型的出现，深度学习（DL）正在经历一场范式转变，这些模型以其关键但不完整的性质恰当地命名。在这项工作中，我们专注于对比语言图像预训练（CLIP），这是一种开放的词汇基础模型，它在许多图像分类任务中实现了高精度，并且在没有明确训练的情况下，通常与完全监督的基线相竞争。尽管如此，仍有一些领域的零样本CLIP性能远未达到最佳，如遥感（RS）和医学图像。与自然图像相比，这些领域不仅表现出根本不同的分布，而且通常依赖RGB之外的互补模式来获得有意义的见解。为此，我们提出了一种方法，旨在将不同的RS图像模式与CLIP的视觉和文本模式相一致。我们的两阶段程序包括对CLIP进行稳健的微调，以应对分布偏移，同时对RS模态编码器进行跨模态对准，从而扩展CLIP的零样本功能。我们最终在遥感图像分类和跨模态检索任务中演示了我们的方法。我们的经验表明，在几个RS基准数据集中，稳健的微调和跨模态对准都转化为显著的性能增益。值得注意的是，这些增强是在不依赖文本描述、不引入任何特定于任务的参数、不从头开始训练和不灾难性遗忘的情况下实现的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.09816v1" target="_blank">2402.09816v1</a>
                              </td>
                              <td>Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment</td>
                              <td>Angelos Zavras</td>
                              <td>2024-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_09816v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.09816v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_12716v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The CLIP Model is Secretly an Image-to-Prompt Converter</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_12716v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_12716v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_12716v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The Stable Diffusion model is a prominent text-to-image generation model that relies on a text prompt as its input, which is encoded using the Contrastive Language-Image Pre-Training (CLIP). However, text prompts have limitations when it comes to incorporating implicit information from reference images. Existing methods have attempted to address this limitation by employing expensive training procedures involving millions of training samples for image-to-image generation. In contrast, this paper demonstrates that the CLIP model, as utilized in Stable Diffusion, inherently possesses the ability to instantaneously convert images into text prompts. Such an image-to-prompt conversion can be achieved by utilizing a linear projection matrix that is calculated in a closed form. Moreover, the paper showcases that this capability can be further enhanced by either utilizing a small amount of similar-domain training data (approximately 100 images) or incorporating several online training steps (around 30 iterations) on the reference images. By leveraging these approaches, the proposed method offers a simple and flexible solution to bridge the gap between images and text prompts. This methodology can be applied to various tasks such as image variation and image editing, facilitating more effective and seamless interaction between images and textual prompts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_12716v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>稳定扩散模型是一种突出的文本到图像生成模型，它依赖于文本提示作为其输入，该文本提示使用对比语言图像预训练（CLIP）进行编码。然而，当涉及到结合来自参考图像的隐含信息时，文本提示具有局限性。现有方法试图通过采用涉及数百万训练样本的昂贵训练过程来解决这一限制，以用于图像到图像的生成。相反，本文证明了在稳定扩散中使用的CLIP模型固有地具有将图像即时转换为文本提示的能力。这种图像到提示的转换可以通过使用以闭合形式计算的线性投影矩阵来实现。此外，该论文展示了通过使用少量相似的域训练数据（约100幅图像）或在参考图像上结合几个在线训练步骤（约30次迭代），可以进一步增强这种能力。通过利用这些方法，所提出的方法提供了一个简单而灵活的解决方案来弥合图像和文本提示之间的差距。这种方法可以应用于各种任务，如图像变化和图像编辑，促进图像和文本提示之间更有效和无缝的交互。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.12716v2" target="_blank">2305.12716v2</a>
                              </td>
                              <td>The CLIP Model is Secretly an Image-to-Prompt Converter</td>
                              <td>Yuxuan Ding</td>
                              <td>2023-05-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_12716v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.12716v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18237v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18237v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18237v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18237v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, "How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x, 4x and 15x reduction in pretraining compute cost when compared to task-agnostic VFM distillation, ImageNet pretraining and DINO pretraining, respectively, while outperforming them. We also show that the dataset used for transferring knowledge has a significant effect on the final target task performance, and introduce a retrieval-augmented knowledge transfer strategy that uses web-scale image retrieval to curate effective transfer sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18237v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在海量数据集上预训练的视觉基础模型（VFM）在各种下游任务上表现出令人印象深刻的性能，尤其是在标记的目标数据有限的情况下。然而，由于它们的高推理计算成本，这些模型不能用于许多真实世界的应用。受此启发，我们提出了以下重要问题，“我们如何利用大型VFM的知识，用有限的标记训练数据为新的目标任务训练小型任务专用模型？”，并提出了一种简单的面向任务的知识转移方法，作为该问题的高效解决方案。我们在五个目标任务上的实验结果表明，所提出的方法分别比任务无关的VFM提取、网络级CLIP预训练、监督ImageNet预训练和自监督DINO预训练高达11.6%、22.1%、13.7%和29.8%。此外，与任务无关的VFM提取、ImageNet预训练和DINO预训练相比，所提出的方法还证明了预训练计算成本分别降低了9倍、4倍和15倍，同时优于它们。我们还表明，用于传递知识的数据集对最终目标任务的性能有显著影响，并引入了一种检索增强知识传递策略，该策略使用网络规模的图像检索来策划有效的传递集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18237v2" target="_blank">2311.18237v2</a>
                              </td>
                              <td>Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models</td>
                              <td>Raviteja Vemulapalli</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18237v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18237v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_09613v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_09613v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_09613v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_09613v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundations models are presented as generalists that often perform well over a myriad of tasks. Fine-tuning these models, even on limited data, provides an additional boost in task-specific performance but often at the cost of their wider generalization, an effect termed catastrophic forgetting. In this paper, we analyze the relation between task difficulty in the CLIP model and the performance of several simple parameter-efficient fine-tuning methods through the lens of domain generalization and catastrophic forgetting. We provide evidence that the silhouette score of the zero-shot image and text embeddings is a better measure of task difficulty than the average cosine similarity of correct image/label embeddings, and discuss observable relationships between task difficulty, fine-tuning method, domain generalization, and catastrophic forgetting. Additionally, the averaged results across tasks and performance measures demonstrate that a simplified method that trains only a subset of attention weights, which we call A-CLIP, yields a balance between domain generalization and catastrophic forgetting.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_09613v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础模型被描述为多面手，通常在无数任务中表现良好。即使在有限的数据上对这些模型进行微调，也能进一步提高特定任务的性能，但往往是以其更广泛的泛化为代价的，这种效应被称为灾难性遗忘。在本文中，我们从领域泛化和灾难性遗忘的角度分析了CLIP模型中的任务难度与几种简单参数有效微调方法的性能之间的关系。我们提供的证据表明，零样本图像和文本嵌入的轮廓分数比正确图像/标签嵌入的平均余弦相似性更好地衡量任务难度，并讨论了任务难度、微调方法、领域泛化和灾难性遗忘之间的可观察关系。此外，任务和性能指标的平均结果表明，只训练注意力权重子集的简化方法（我们称之为a-CLIP）在领域泛化和灾难性遗忘之间取得了平衡。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.09613v1" target="_blank">2402.09613v1</a>
                              </td>
                              <td>Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP</td>
                              <td>Laura Niss</td>
                              <td>2024-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_09613v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.09613v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_11481v3_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_11481v3_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_11481v3_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_11481v3_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Referring image segmentation (RIS) is a fundamental vision-language task that intends to segment a desired object from an image based on a given natural language expression. Due to the essentially distinct data properties between image and text, most of existing methods either introduce complex designs towards fine-grained vision-language alignment or lack required dense alignment, resulting in scalability issues or mis-segmentation problems such as over- or under-segmentation. To achieve effective and efficient fine-grained feature alignment in the RIS task, we explore the potential of masked multimodal modeling coupled with self-distillation and propose a novel cross-modality masked self-distillation framework named CM-MaskSD, in which our method inherits the transferred knowledge of image-text semantic alignment from CLIP model to realize fine-grained patch-word feature alignment for better segmentation accuracy. Moreover, our CM-MaskSD framework can considerably boost model performance in a nearly parameter-free manner, since it shares weights between the main segmentation branch and the introduced masked self-distillation branches, and solely introduces negligible parameters for coordinating the multimodal features. Comprehensive experiments on three benchmark datasets (i.e. RefCOCO, RefCOCO+, G-Ref) for the RIS task convincingly demonstrate the superiority of our proposed framework over previous state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_11481v3_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>参考图像分割（RIS）是一项基本的视觉语言任务，旨在根据给定的自然语言表达从图像中分割出所需的对象。由于图像和文本之间本质上不同的数据属性，大多数现有方法要么引入复杂的细粒度视觉语言对齐设计，要么缺乏所需的密集对齐，从而导致可扩展性问题或误分割问题，如过度分割或欠分割。为了在RIS任务中实现有效和高效的细粒度特征对齐，我们探索了掩蔽多模态建模与自蒸馏相结合的潜力，并提出了一种新的跨模态掩蔽自蒸馏框架CM-MaskSD，其中我们的方法继承了CLIP模型中传递的图像-文本语义对齐知识，实现了细粒度的补丁词特征对齐，从而提高了分割精度。此外，我们的CM-MaskSD框架可以以几乎无参数的方式显著提高模型性能，因为它在主分割分支和引入的掩蔽自蒸馏分支之间共享权重，并且仅引入可忽略的参数来协调多模式特征。在RIS任务的三个基准数据集（即RefCOCO、RefCOCO+、G-Ref）上进行的综合实验令人信服地证明了我们提出的框架优于以前最先进的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.11481v3" target="_blank">2305.11481v3</a>
                              </td>
                              <td>CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation</td>
                              <td>Wenxuan Wang</td>
                              <td>2023-05-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_11481v3_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.11481v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08994v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08994v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08994v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08994v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method consists of a Transformer-based feature extractor to effectively model global neural representations. It also incorporates learnable subject-specific tokens that facilitates the aggregation of multi-subject data without a linear increase of parameters. Additionally, we employ representational similarity analysis (RSA) to guide token representation learning based on the topological relationship of visual stimuli in the representation space of CLIP, enabling full characterization of the relationship between neural responses of different subjects under different stimuli. Finally, token representations are used for multi-subject semantic decoding. Our proposed method outperforms single-subject decoding methods and achieves state-of-the-art performance among the existing multi-subject methods on two fMRI datasets. Visualization results provide insights into the effectiveness of our proposed method. Code is available at https://github.com/CLIP-MUSED/CLIP-MUSED.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08994v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于个体差异，解码视觉神经信息的研究在将单主题解码模型推广到多主题方面面临挑战。此外，来自单个受试者的数据的有限可用性对模型性能产生了制约性影响。尽管现有的多受试者解码方法已经取得了重大进展，但它们仍然存在一些局限性，包括难以提取全局神经反应特征、模型参数随受试者数量的线性缩放，以及对不同受试者对各种刺激的神经反应之间关系的描述不足。为了克服这些限制，我们提出了一种CLIP引导的多对象视觉神经信息语义解码（CLIP-MUSED）方法。我们的方法包括一个基于Transformer的特征提取器，以有效地对全局神经表示进行建模。它还结合了可学习的主题特定令牌，有助于在不线性增加参数的情况下聚合多主题数据。此外，我们使用表征相似性分析（RSA）来指导基于CLIP表示空间中视觉刺激的拓扑关系的表征学习，从而能够全面表征不同受试者在不同刺激下的神经反应之间的关系。最后，令牌表示用于多主题语义解码。我们提出的方法优于单主题解码方法，并在两个fMRI数据集上实现了现有多主题方法中最先进的性能。可视化结果为我们提出的方法的有效性提供了见解。代码位于https://github.com/CLIP-MUSED/CLIP-MUSED.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08994v1" target="_blank">2402.08994v1</a>
                              </td>
                              <td>CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding</td>
                              <td>Qiongyi Zhou</td>
                              <td>2024-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08994v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08994v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/clip-mused/clip-mused" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08960v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08960v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08960v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08960v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contemporary cutting-edge open-vocabulary segmentation approaches commonly rely on image-mask-text triplets, yet this restricted annotation is labour-intensive and encounters scalability hurdles in complex real-world scenarios. Although some methods are proposed to reduce the annotation cost with only text supervision, the incompleteness of supervision severely limits the versatility and performance. In this paper, we liberate the strict correspondence between masks and texts by using independent image-mask and image-text pairs, which can be easily collected respectively. With this unpaired mask-text supervision, we propose a new weakly-supervised open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident pairs of mask predictions and entities in text descriptions. Using the independent image-mask and image-text pairs, we predict a set of binary masks and associate them with entities by resorting to the CLIP embedding space. However, the inherent noise in the correspondence between masks and entities poses a significant challenge when obtaining reliable pairs. In light of this, we advocate using the large vision-language model (LVLM) to refine text descriptions and devise a multi-scale ensemble to stablise the matching between masks and entities. Compared to text-only weakly-supervised methods, our Uni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K datasets, and even surpasses fully-supervised methods on the challenging PASCAL Context-459 dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08960v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>当代尖端的开放式词汇分割方法通常依赖于图像掩码文本三元组，然而这种受限的注释是劳动密集型的，并且在复杂的现实世界场景中遇到了可扩展性障碍。尽管有人提出了一些仅通过文本监督来降低注释成本的方法，但监督的不完全性严重限制了其通用性和性能。在本文中，我们通过使用独立的图像掩码和图像文本对来解放掩码和文本之间的严格对应关系，这些图像掩码和文本对可以很容易地分别收集。利用这种不成对的掩码文本监督，我们提出了一种新的弱监督开放词汇分割框架（Uni-OVSeg），该框架利用文本描述中的掩码预测和实体的置信对。使用独立的图像掩码和图像文本对，我们预测了一组二进制掩码，并通过CLIP嵌入空间将它们与实体相关联。然而，在获得可靠的对时，掩模和实体之间的对应关系中的固有噪声带来了重大挑战。有鉴于此，我们主张使用大视觉语言模型（LVLM）来细化文本描述，并设计多尺度集成来建立遮罩和实体之间的匹配。与纯文本弱监督方法相比，我们的Uni-OVSeg在ADE20K数据集上实现了15.5%mIoU的显著改进，甚至在具有挑战性的PASCAL Context-459数据集上超过了完全监督方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08960v1" target="_blank">2402.08960v1</a>
                              </td>
                              <td>Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision</td>
                              <td>Zhaoqing Wang</td>
                              <td>2024-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08960v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08960v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/derrickwang005/uni-ovseg.pytorch" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08875v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08875v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08875v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08875v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The increasing variety and quantity of tagged multimedia content on platforms such as TikTok provides an opportunity to advance computer vision modeling. We have curated a distinctive dataset of 283,582 unique video clips categorized under 386 hashtags relating to modern human actions. We release this dataset as a valuable resource for building domain-specific foundation models for human movement modeling tasks such as action recognition. To validate this dataset, which we name TikTokActions, we perform two sets of experiments. First, we pretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone on TikTokActions subset, and then fine-tune and evaluate on popular datasets such as UCF101 and the HMDB51. We find that the performance of the model pre-trained using our Tik-Tok dataset is comparable to models trained on larger action recognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, our investigation into the relationship between pre-training dataset size and fine-tuning performance reveals that beyond a certain threshold, the incremental benefit of larger training sets diminishes. This work introduces a useful TikTok video dataset that is available for public use and provides insights into the marginal benefit of increasing pre-training dataset sizes for video-based foundation models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08875v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>TikTok等平台上标记的多媒体内容的种类和数量不断增加，为推进计算机视觉建模提供了机会。我们策划了一个独特的数据集，包含283582个独特的视频片段，分类在386个与现代人类行为有关的标签下。我们发布该数据集作为一个宝贵的资源，用于为动作识别等人类运动建模任务构建特定领域的基础模型。为了验证这个数据集，我们将其命名为TikTokActions，我们进行了两组实验。首先，我们在TikTokActions子集上使用ViT基础骨干对最先进的VideoMAEv2进行预训练，然后在UCF101和HMDB51等流行数据集上进行微调和评估。我们发现，使用我们的Tik Tok数据集预训练的模型的性能与在更大的动作识别数据集上训练的模型相当（在UCF101上为95.3%，在HMDB51上为53.24%）。此外，我们对预训练数据集大小和微调性能之间关系的研究表明，超过一定阈值，较大训练集的增量效益就会减弱。这项工作介绍了一个有用的TikTok视频数据集，可供公众使用，并深入了解了增加基于视频的基础模型的预训练数据集大小的边际效益。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08875v1" target="_blank">2402.08875v1</a>
                              </td>
                              <td>TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition</td>
                              <td>Yang Qian</td>
                              <td>2024-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08875v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08875v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08532v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Captions Are Worth a Thousand Words: Enhancing Product Retrieval with Pretrained Image-to-Text Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08532v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08532v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08532v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper explores the usage of multimodal image-to-text models to enhance text-based item retrieval. We propose utilizing pre-trained image captioning and tagging models, such as instructBLIP and CLIP, to generate text-based product descriptions which are combined with existing text descriptions. Our work is particularly impactful for smaller eCommerce businesses who are unable to maintain the high-quality text descriptions necessary to effectively perform item retrieval for search and recommendation use cases. We evaluate the searchability of ground-truth text, image-generated text, and combinations of both texts on several subsets of Amazon's publicly available ESCI dataset. The results demonstrate the dual capability of our proposed models to enhance the retrieval of existing text and generate highly-searchable standalone descriptions.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08532v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文探讨了使用多模式图像到文本模型来增强基于文本的项目检索。我们建议利用预先训练的图像字幕和标记模型，如instructionBLIP和CLIP，生成基于文本的产品描述，这些描述与现有的文本描述相结合。我们的工作对那些无法维护有效执行搜索和推荐用例的项目检索所需的高质量文本描述的小型电子商务企业尤其有影响。我们在亚马逊公开的ESCI数据集的几个子集上评估了基本事实文本、图像生成文本以及这两种文本的组合的可搜索性。结果表明，我们提出的模型具有增强现有文本检索和生成高度可搜索的独立描述的双重能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08532v1" target="_blank">2402.08532v1</a>
                              </td>
                              <td>Captions Are Worth a Thousand Words: Enhancing Product Retrieval with Pretrained Image-to-Text Models</td>
                              <td>Jason Tang</td>
                              <td>2024-02-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08532v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08532v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_06198v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D Pretraining from Real-World Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_06198v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_06198v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_06198v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>3D Shape represented as point cloud has achieve advancements in multimodal pre-training to align image and language descriptions, which is curial to object identification, classification, and retrieval. However, the discrete representations of point cloud lost the object's surface shape information and creates a gap between rendering results and 2D correspondences. To address this problem, we propose GS-CLIP for the first attempt to introduce 3DGS (3D Gaussian Splatting) into multimodal pre-training to enhance 3D representation. GS-CLIP leverages a pre-trained vision-language model for a learned common visual and textual space on massive real world image-text pairs and then learns a 3D Encoder for aligning 3DGS optimized per object. Additionally, a novel Gaussian-Aware Fusion is proposed to extract and fuse global explicit feature. As a general framework for language-image-3D pre-training, GS-CLIP is agnostic to 3D backbone networks. Experiments on challenging shows that GS-CLIP significantly improves the state-of-the-art, outperforming the previously best results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_06198v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>以点云表示的3D形状在多模式预训练中取得了进步，以对齐图像和语言描述，这对对象识别、分类和检索非常重要。然而，点云的离散表示会丢失对象的曲面形状信息，并在渲染结果和2D对应关系之间产生间隙。为了解决这个问题，我们首次尝试将3DGS（3D Gaussian Splatting）引入多模式预训练以增强3D表示，并提出了GS-CLIP。GS-CLIP利用预先训练的视觉语言模型，在大规模的真实世界图像-文本对上学习公共视觉和文本空间，然后学习3D编码器，用于对齐每个对象优化的3DGS。此外，提出了一种新的高斯感知融合方法来提取和融合全局显式特征。GS-CLIP作为语言图像3D预训练的通用框架，对3D骨干网络是不可知的。具有挑战性的实验表明，GS-CLIP显著改进了最先进的技术，优于之前的最佳结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.06198v2" target="_blank">2402.06198v2</a>
                              </td>
                              <td>GS-CLIP: Gaussian Splatting for Contrastive Language-Image-3D Pretraining from Real-World Data</td>
                              <td>Haoyuan Li</td>
                              <td>2024-02-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_06198v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.06198v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_09478v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Data Reconstruction Attacks and Defenses: A Systematic Evaluation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_09478v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_09478v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_09478v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Reconstruction attacks and defenses are essential in understanding the data leakage problem in machine learning. However, prior work has centered around empirical observations of gradient inversion attacks, lacks theoretical groundings, and was unable to disentangle the usefulness of defending methods versus the computational limitation of attacking methods. In this work, we propose a strong reconstruction attack in the setting of federated learning. The attack reconstructs intermediate features and nicely integrates with and outperforms most of the previous methods. On this stronger attack, we thoroughly investigate both theoretically and empirically the effect of the most common defense methods. Our findings suggest that among various defense mechanisms, such as gradient clipping, dropout, additive noise, local aggregation, etc., gradient pruning emerges as the most effective strategy to defend against state-of-the-art attacks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_09478v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>重构攻击和防御对于理解机器学习中的数据泄露问题至关重要。然而，先前的工作集中在梯度反演攻击的经验观察上，缺乏理论基础，并且无法区分防御方法的有用性与攻击方法的计算限制。在这项工作中，我们提出了一种在联合学习环境中的强重构攻击。该攻击重建了中间功能，并与以前的大多数方法很好地集成并优于它们。对于这种更强的攻击，我们从理论和经验上深入研究了最常见的防御方法的效果。我们的研究结果表明，在各种防御机制中，如梯度修剪、丢弃、加性噪声、局部聚集等，梯度修剪是抵御最先进攻击的最有效策略。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.09478v1" target="_blank">2402.09478v1</a>
                              </td>
                              <td>Data Reconstruction Attacks and Defenses: A Systematic Evaluation</td>
                              <td>Sheng Liu</td>
                              <td>2024-02-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_09478v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.09478v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07410v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07410v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07410v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07410v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related features is less-explored. Driven by the above, this work comprehensively investigates the safety objectives of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study 83 CLIP models and 127 ImageNet classifiers. They are diverse in architecture, (pre)training distribution and training strategies. We consider 10 visual factors (e.g., shape and pattern), 5 types of out-of-distribution data, and 8 natural and challenging test conditions with different shift types, such as texture, style, and perturbation shifts. Our study has unveiled several previously unknown insights into CLIP models. For instance, they are not consistently more calibrated than other ImageNet models, which contradicts existing findings. Additionally, our analysis underscores the significance of training source design by showcasing its profound influence on the three safety-related properties. We believe our comprehensive study can shed light on and help guide the development of more robust and reliable CLIP models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07410v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比语言图像预训练（CLIP）模型在多个具有挑战性的分布变化中表现出了显著的泛化能力。然而，就其对特定视觉因素变化的鲁棒性而言，仍有很多需要探索的地方。在现实世界的应用中，可靠和安全的系统必须考虑分类准确性之外的其他安全目标，例如预测不确定性。然而，CLIP模型在此类安全相关功能上的有效性却很少被探索。在上述驱动下，这项工作全面研究了CLIP模型的安全目标，特别关注三个关键特性：对视觉因素变化的弹性、校准的不确定性估计和检测异常输入的能力。为此，我们研究了83个CLIP模型和127个ImageNet分类器。它们在架构、（培训前）分配和培训策略方面各不相同。我们考虑了10个视觉因素（如形状和图案）、5种类型的分布外数据，以及8种具有不同偏移类型（如纹理、风格和扰动偏移）的自然和具有挑战性的测试条件。我们的研究揭示了对CLIP模型的一些以前未知的见解。例如，它们并不总是比其他ImageNet模型校准得更高，这与现有的发现相矛盾。此外，我们的分析通过展示训练源设计对三个安全相关特性的深刻影响，强调了训练源设计的重要性。我们相信，我们的全面研究可以揭示并帮助指导更稳健和可靠的CLIP模型的开发。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07410v1" target="_blank">2402.07410v1</a>
                              </td>
                              <td>A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)</td>
                              <td>Weijie Tu</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07410v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07410v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07320v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07320v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07320v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07320v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This research explores the integration of language embeddings for active learning in autonomous driving datasets, with a focus on novelty detection. Novelty arises from unexpected scenarios that autonomous vehicles struggle to navigate, necessitating higher-level reasoning abilities. Our proposed method employs language-based representations to identify novel scenes, emphasizing the dual purpose of safety takeover responses and active learning. The research presents a clustering experiment using Contrastive Language-Image Pretrained (CLIP) embeddings to organize datasets and detect novelties. We find that the proposed algorithm effectively isolates novel scenes from a collection of subsets derived from two real-world driving datasets, one vehicle-mounted and one infrastructure-mounted. From the generated clusters, we further present methods for generating textual explanations of elements which differentiate scenes classified as novel from other scenes in the data pool, presenting qualitative examples from the clustered results. Our results demonstrate the effectiveness of language-driven embeddings in identifying novel elements and generating explanations of data, and we further discuss potential applications in safe takeovers, data curation, and multi-task active learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07320v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本研究探索了自动驾驶数据集中用于主动学习的语言嵌入的集成，重点是新颖性检测。新颖性源于自动驾驶汽车难以导航的意外场景，需要更高级别的推理能力。我们提出的方法使用基于语言的表示来识别新场景，强调安全接管响应和主动学习的双重目的。该研究提出了一个使用对比语言图像预训练（CLIP）嵌入来组织数据集和检测新颖性的聚类实验。我们发现，所提出的算法有效地将新场景与从两个真实世界的驾驶数据集（一个车载数据集和一个基础设施车载数据集）导出的子集集合隔离开来。从生成的聚类中，我们进一步提出了生成元素的文本解释的方法，这些方法将被分类为新颖的场景与数据池中的其他场景区分开来，并从聚类结果中提供了定性示例。我们的结果证明了语言驱动嵌入在识别新元素和生成数据解释方面的有效性，并进一步讨论了在安全接管、数据管理和多任务主动学习中的潜在应用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07320v1" target="_blank">2402.07320v1</a>
                              </td>
                              <td>Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets</td>
                              <td>Ross Greer</td>
                              <td>2024-02-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07320v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07320v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01163v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A New Learning Paradigm for Foundation Model-based Remote Sensing Change Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01163v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01163v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01163v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Change detection (CD) is a critical task to observe and analyze dynamic processes of land cover. Although numerous deep learning-based CD models have performed excellently, their further performance improvements are constrained by the limited knowledge extracted from the given labelled data. On the other hand, the foundation models that emerged recently contain a huge amount of knowledge by scaling up across data modalities and proxy tasks. In this paper, we propose a Bi-Temporal Adapter Network (BAN), which is a universal foundation model-based CD adaptation framework aiming to extract the knowledge of foundation models for CD. The proposed BAN contains three parts, i.e. frozen foundation model (e.g., CLIP), bi-temporal adapter branch (Bi-TAB), and bridging modules between them. Specifically, BAN extracts general features through a frozen foundation model, which are then selected, aligned, and injected into Bi-TAB via the bridging modules. Bi-TAB is designed as a model-agnostic concept to extract task/domain-specific features, which can be either an existing arbitrary CD model or some hand-crafted stacked blocks. Beyond current customized models, BAN is the first extensive attempt to adapt the foundation model to the CD task. Experimental results show the effectiveness of our BAN in improving the performance of existing CD methods (e.g., up to 4.08\% IoU improvement) with only a few additional learnable parameters. More importantly, these successful practices show us the potential of foundation models for remote sensing CD. The code is available at \url{https://github.com/likyoo/BAN} and will be supported in our Open-CD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01163v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>变化检测（CD）是观测和分析土地覆盖动态过程的一项关键任务。尽管许多基于深度学习的CD模型表现出色，但它们的进一步性能改进受到从给定标记数据中提取的有限知识的限制。另一方面，最近出现的基础模型通过扩展数据模式和代理任务，包含了大量知识。在本文中，我们提出了一个双时态适配器网络（BAN），这是一个通用的基于基础模型的CD自适应框架，旨在提取CD的基础模型知识。所提出的BAN包括三个部分，即冻结基础模型（如CLIP）、双时态适配器分支（Bi-TAB）以及它们之间的桥接模块。具体来说，BAN通过冻结的基础模型提取一般特征，然后通过桥接模块将这些特征选择、对齐并注入到Bi-TAB中。Bi-TAB被设计为一个与模型无关的概念，用于提取任务/领域特定的功能，这些功能可以是现有的任意CD模型，也可以是一些手工制作的堆叠块。除了当前的定制模型之外，BAN是第一次将基础模型适应CD任务的广泛尝试。实验结果表明，我们的BAN仅使用几个额外的可学习参数就可以有效地提高现有CD方法的性能（例如，高达4.08\%IoU的改进）。更重要的是，这些成功的实践向我们展示了遥感CD基础模型的潜力{https://github.com/likyoo/BAN}并将在我们的Open CD中得到支持。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01163v2" target="_blank">2312.01163v2</a>
                              </td>
                              <td>A New Learning Paradigm for Foundation Model-based Remote Sensing Change Detection</td>
                              <td>Kaiyu Li</td>
                              <td>2023-12-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01163v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01163v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/likyoo/ban" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07062v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07062v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07062v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07062v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this study, we propose a new method for constructing UCB-type algorithms for stochastic multi-armed bandits based on general convex optimization methods with an inexact oracle. We derive the regret bounds corresponding to the convergence rates of the optimization methods. We propose a new algorithm Clipped-SGD-UCB and show, both theoretically and empirically, that in the case of symmetric noise in the reward, we can achieve an $O(\log T\sqrt{KT\log T})$ regret bound instead of $O\left (T^{\frac{1}{1+\alpha}} K^{\frac{\alpha}{1+\alpha}} \right)$ for the case when the reward distribution satisfies $\mathbb{E}_{X \in D}[|X|^{1+\alpha}] \leq \sigma^{1+\alpha}$ ($\alpha \in (0, 1])$, i.e. perform better than it is assumed by the general lower bound for bandits with heavy-tails. Moreover, the same bound holds even when the reward distribution does not have the expectation, that is, when $\alpha<0$.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07062v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项研究中，我们提出了一种新的方法来构造随机多武装匪徒的UCB型算法，该方法基于具有不精确预言的一般凸优化方法。我们导出了与优化方法的收敛速度相对应的遗憾界。我们提出了一种新的算法Clipped SGD UCB，并从理论和经验上证明，在奖励中存在对称噪声的情况下，当奖励分布满足$\mathbb时，我们可以实现$O（\log T\sqrt｛KT\log T｝）$后悔界，而不是$O\left（T^{E}_｛X\in D｝[|X|^｛1+\alpha｝]\leq\sigma^｛1+1\alpha}$（$\alpha\in（0，1]）$，即表现比重尾土匪的一般下界所假设的要好。此外，即使当奖励分布不具有期望时，即当$\alpha<0$时，同样的界限也成立。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07062v1" target="_blank">2402.07062v1</a>
                              </td>
                              <td>Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise</td>
                              <td>Yuriy Dorn</td>
                              <td>2024-02-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07062v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07062v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_06978v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sophia-in-Audition: Virtual Production with a Robot Performer</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_06978v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_06978v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_06978v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present Sophia-in-Audition (SiA), a new frontier in virtual production, by employing the humanoid robot Sophia within an UltraStage environment composed of a controllable lighting dome coupled with multiple cameras. We demonstrate Sophia's capability to replicate iconic film segments, follow real performers, and perform a variety of motions and expressions, showcasing her versatility as a virtual actor. Key to this process is the integration of facial motion transfer algorithms and the UltraStage's controllable lighting and multi-camera setup, enabling dynamic performances that align with the director's vision. Our comprehensive user studies indicate positive audience reception towards Sophia's performances, highlighting her potential to reduce the uncanny valley effect in virtual acting. Additionally, the immersive lighting in dynamic clips was highly rated for its naturalness and its ability to mirror professional film standards. The paper presents a first-of-its-kind multi-view robot performance video dataset with dynamic lighting, offering valuable insights for future enhancements in humanoid robotic performers and virtual production techniques. This research contributes significantly to the field by presenting a unique virtual production setup, developing tools for sophisticated performance control, and providing a comprehensive dataset and user study analysis for diverse applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_06978v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们展示了Sophia in Audition（SiA），这是虚拟制作的一个新前沿，通过在由可控照明圆顶和多个摄像头组成的超舞台环境中使用人形机器人Sophia。我们展示了Sophia复制标志性电影片段、追随真实表演者、表演各种动作和表情的能力，展示了她作为虚拟演员的多才多艺。这一过程的关键是将面部运动传递算法与UltraStage的可控照明和多摄像头设置相结合，实现与导演视觉一致的动态表演。我们的全面用户研究表明，观众对索菲亚的表演有着积极的反应，这突出了她在减少虚拟表演中的诡异山谷效应方面的潜力。此外，动态剪辑中的沉浸式照明因其自然度和反映专业电影标准的能力而备受好评。本文提出了第一个具有动态照明的多视图机器人表演视频数据集，为未来增强人形机器人表演者和虚拟生产技术提供了有价值的见解。这项研究通过提供独特的虚拟生产设置、开发用于复杂性能控制的工具，以及为各种应用程序提供全面的数据集和用户研究分析，为该领域做出了重大贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.06978v1" target="_blank">2402.06978v1</a>
                              </td>
                              <td>Sophia-in-Audition: Virtual Production with a Robot Performer</td>
                              <td>Taotao Zhou</td>
                              <td>2024-02-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_06978v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.06978v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_06959v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SpeechCLIP+: Self-supervised multi-task representation learning for speech via CLIP and speech-image data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_06959v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_06959v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_06959v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recently proposed visually grounded speech model SpeechCLIP is an innovative framework that bridges speech and text through images via CLIP without relying on text transcription. On this basis, this paper introduces two extensions to SpeechCLIP. First, we apply the Continuous Integrate-and-Fire (CIF) module to replace a fixed number of CLS tokens in the cascaded architecture. Second, we propose a new hybrid architecture that merges the cascaded and parallel architectures of SpeechCLIP into a multi-task learning framework. Our experimental evaluation is performed on the Flickr8k and SpokenCOCO datasets. The results show that in the speech keyword extraction task, the CIF-based cascaded SpeechCLIP model outperforms the previous cascaded SpeechCLIP model using a fixed number of CLS tokens. Furthermore, through our hybrid architecture, cascaded task learning boosts the performance of the parallel branch in image-speech retrieval tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_06959v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近提出的基于视觉的语音模型SpeechCLIP是一种创新的框架，通过CLIP将语音和文本通过图像连接起来，而不依赖于文本转录。在此基础上，本文介绍了SpeechCLIP的两个扩展。首先，我们应用连续集成和激发（CIF）模块来替换级联架构中固定数量的CLS令牌。其次，我们提出了一种新的混合架构，将SpeechCLIP的级联和并行架构合并到一个多任务学习框架中。我们在Flickr8k和SpokenCOCO数据集上进行了实验评估。结果表明，在语音关键词提取任务中，基于CIF的级联SpeechCLIP模型优于之前使用固定数量CLS令牌的级联speech CLIP模型。此外，通过我们的混合架构，级联任务学习提高了并行分支在图像语音检索任务中的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.06959v1" target="_blank">2402.06959v1</a>
                              </td>
                              <td>SpeechCLIP+: Self-supervised multi-task representation learning for speech via CLIP and speech-image data</td>
                              <td>Hsuan-Fu Wang</td>
                              <td>2024-02-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_06959v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.06959v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_06798v1_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reasoning Grasping via Multimodal Large Language Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_06798v1_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_06798v1_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_06798v1_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite significant progress in robotic systems for operation within human-centric environments, existing models still heavily rely on explicit human commands to identify and manipulate specific objects. This limits their effectiveness in environments where understanding and acting on implicit human intentions are crucial. In this study, we introduce a novel task: reasoning grasping, where robots need to generate grasp poses based on indirect verbal instructions or intentions. To accomplish this, we propose an end-to-end reasoning grasping model that integrates a multi-modal Large Language Model (LLM) with a vision-based robotic grasping framework. In addition, we present the first reasoning grasping benchmark dataset generated from the GraspNet-1 billion, incorporating implicit instructions for object-level and part-level grasping, and this dataset will soon be available for public access. Our results show that directly integrating CLIP or LLaVA with the grasp detection model performs poorly on the challenging reasoning grasping tasks, while our proposed model demonstrates significantly enhanced performance both in the reasoning grasping benchmark and real-world experiments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_06798v1_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管在以人为中心的环境中操作的机器人系统取得了重大进展，但现有模型仍然严重依赖于明确的人类命令来识别和操纵特定对象。这限制了它们在理解和执行隐含的人类意图至关重要的环境中的有效性。在这项研究中，我们介绍了一项新任务：推理抓取，机器人需要根据间接的口头指令或意图生成抓取姿势。为了实现这一点，我们提出了一种端到端的推理抓取模型，该模型将多模态大语言模型（LLM）与基于视觉的机器人抓取框架相结合。此外，我们还提出了第一个由GraspNet-10亿生成的推理抓取基准数据集，该数据集包含了对象级和零件级抓取的隐含指令，该数据集很快将可供公众访问。我们的结果表明，将CLIP或LLaVA与抓取检测模型直接集成在具有挑战性的推理抓取任务中表现不佳，而我们提出的模型在推理抓取基准和真实世界实验中都表现出显著增强的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.06798v1" target="_blank">2402.06798v1</a>
                              </td>
                              <td>Reasoning Grasping via Multimodal Large Language Model</td>
                              <td>Shiyu Jin</td>
                              <td>2024-02-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_06798v1_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.06798v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_05301v2_4">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_05301v2_4_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_05301v2_4_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_05301v2_4_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family which includes thousands of mixed-representation human-designed bicycle models and several datasets quantifying design performance. The code and dataset can be found at: https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_05301v2_4_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文介绍了140万个程序生成的自行车设计的公共数据集，这些设计以JSON文件和光栅化图像的形式参数化表示。数据集是通过使用渲染引擎创建的，该引擎利用BikeCAD软件从参数化设计中生成矢量图形。该渲染引擎在论文中进行了讨论，并与数据集一起公开发布。尽管该数据集有许多应用，但主要动机是需要在参数化和基于图像的设计表示之间训练跨模态预测模型。例如，我们证明了预测模型可以直接从参数表示中准确估计对比语言图像预训练（CLIP）嵌入。这允许在参数化自行车设计和文本串或参考图像之间建立相似关系。经过训练的预测模型也被公开。该数据集加入了BIKED数据集家族，该家族包括数千个混合表示的人类设计的自行车模型和几个量化设计性能的数据集。代码和数据集位于：https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.05301v2" target="_blank">2402.05301v2</a>
                              </td>
                              <td>BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs</td>
                              <td>Lyle Regenwetter</td>
                              <td>2024-02-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_05301v2_4"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.05301v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/lyleregenwetter/biked_multimodal" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="DINO"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_13181v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13181v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13181v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13181v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its end-effector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13181v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了DINOBot，这是一种用于机器人操作的新型模仿学习框架，它利用了从用DINO训练的视觉变换器中提取的特征的图像级和像素级能力。当与新对象交互时，DINOBot首先使用这些特征来检索在人类演示过程中体验到的视觉上最相似的对象，然后使用该对象将其末端效应器与新对象对齐，以实现有效的交互。通过一系列关于日常任务的真实世界实验，我们表明，利用视觉基础模型的图像级和像素级特性，可以实现前所未有的学习效率和泛化能力。视频和代码可在https://www.robot-learning.uk/dinobot.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13181v1" target="_blank">2402.13181v1</a>
                              </td>
                              <td>DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models</td>
                              <td>Norman Di Palo</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13181v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13181v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10513v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Understanding Delays in AF\_XDP-based Applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10513v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10513v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10513v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Packet processing on Linux can be slow due to its complex network stack. To solve this problem, there are two main solutions: eXpress Data Path (XDP) and Data Plane Development Kit (DPDK). XDP and the AF XDP socket offer full interoperability with the legacy system and is being adopted by major internet players like Open vSwitch or Facebook. While the performance evaluation of AF XDP against the legacy protocol stack in the kernel or against DPDK has been studied in the literature, the impact of the multiple socket parameters and the system configuration on its latency has been left aside. To address this, we conduct an experimental study to understand the XDP/AF XDP ecosystem and detect microseconds delays to better architect future latency-sensitive applications. Since the performance of AF XDP depends on multiple parameters found in different layers, finding the configuration minimizing its latency is a challenging task. We rely on a classification algorithm to group the performance results, allowing us to easily identify parameters with the biggest impact on performance at different loads. Last, but not least, we show that some configurations can significantly decrease the benefits of AF XDP, leading to undesirable behaviors, while other configurations are able to reduce such round trip delays to an impressive value of 6.5 $\mu$s in the best case, including the tracing overhead. In summary, AF XDP is a promising solution, and careful selection of both application and socket parameters can significantly improve performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10513v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于其复杂的网络堆栈，Linux上的数据包处理可能会很慢。为了解决这个问题，有两种主要的解决方案：eXpress数据路径（XDP）和数据平面开发工具包（DPDK）。XDP和AF XDP插座提供了与传统系统的完全互操作性，并被Open vSwitch或Facebook等主要互联网玩家所采用。虽然文献中已经研究了AF XDP相对于内核中的遗留协议栈或DPDK的性能评估，但多个套接字参数和系统配置对其延迟的影响被忽略了。为了解决这一问题，我们进行了一项实验研究，以了解XDP/AF XDP生态系统，并检测微秒延迟，从而更好地构建未来对延迟敏感的应用程序。由于AF XDP的性能取决于在不同层中找到的多个参数，因此找到将其延迟最小化的配置是一项具有挑战性的任务。我们依靠分类算法对性能结果进行分组，使我们能够轻松识别在不同负载下对性能影响最大的参数。最后，但并非最不重要的是，我们表明，一些配置会显著降低AF XDP的好处，导致不理想的行为，而其他配置能够在最佳情况下将这种往返延迟减少到6.5$\mu$s的令人印象深刻的值，包括跟踪开销。总之，AF XDP是一个很有前途的解决方案，仔细选择应用程序和套接字参数可以显著提高性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10513v2" target="_blank">2402.10513v2</a>
                              </td>
                              <td>Understanding Delays in AF\_XDP-based Applications</td>
                              <td>Killian Castillon du Perron</td>
                              <td>2024-02-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10513v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10513v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10793v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Masked Attention is All You Need for Graphs</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10793v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10793v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10793v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly better transfer learning capabilities compared to GNNs and comparable or better time and memory scaling. MAG has sub-linear memory scaling in the number of nodes or edges, enabling learning on dense graphs and future-proofing the approach.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10793v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图神经网络（GNN）和消息传递算法的变体是在图上学习的主要手段，这在很大程度上是由于它们的灵活性、速度和令人满意的性能。然而，强大的通用GNN的设计需要大量的研究工作，并且通常依赖于手工制作、精心选择的消息传递运算符。受此启发，我们提出了一种非常简单的方法来学习完全依赖注意力的图。图被表示为节点集或边集，它们的连通性通过屏蔽注意力权重矩阵来实现，从而有效地为每个图创建自定义注意力模式。尽管图的掩蔽注意力（MAG）很简单，但它在远程任务上具有最先进的性能，在超过55个节点和图级别的任务上优于强消息传递基线和更多基于注意力的方法。与GNN相比，我们还显示出明显更好的迁移学习能力，以及相当或更好的时间和记忆缩放。MAG在节点或边的数量上具有亚线性内存缩放功能，能够在密集图上进行学习并证明该方法的未来性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10793v1" target="_blank">2402.10793v1</a>
                              </td>
                              <td>Masked Attention is All You Need for Graphs</td>
                              <td>David Buterez</td>
                              <td>2024-02-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10793v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10793v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10717v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10717v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10717v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10717v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VAE to feed into a self-attention network, generating patient-level features. Next, we develop a co-dual-cross-attention mechanism to combine the histopathological features with genetic data, enabling the model to capture the interplay between them. Additionally, clinical data is incorporated using a feed-forward network (FFN), further enhancing predictive performance and achieving comprehensive multimodal feature integration. Furthermore, we introduce a weighted Cox loss function, specifically designed to handle imbalanced survival data, which is a common challenge in the field. The proposed model achieves a mean concordance index (C-index) of 0.77 and a time-dependent area under the curve (AUC) of 0.84, outperforming state-of-the-art methods. It predicts risk (high versus low) with prognostic significance for overall survival (OS) in univariate analysis (HR=2.99, 95% CI: 1.88--4.78, p<0.005), and maintains independent significance in multivariate analysis incorporating standard clinicopathological variables (HR=2.91, 95% CI: 1.80--4.68, p<0.005). The proposed method not only improves model performance but also addresses a critical gap in handling imbalanced data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10717v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>癌症是影响全世界数百万妇女的一个重大健康问题。准确的生存风险分层在指导个性化治疗决策和改善患者预后方面发挥着至关重要的作用。在这里，我们介绍了BioFusionNet，这是一个深度学习框架，将图像衍生特征与遗传和临床数据融合在一起，以实现全面的患者概况，并对ER+乳腺癌症患者进行生存风险分层。我们使用多个自监督特征提取器，即DINO和MoCoV3，在组织病理学斑块上预训练，以捕捉详细的组织病理学图像特征。然后，我们利用变分自动编码器（VAE）融合这些特征，并利用VAE的潜在空间将其输入到自注意网络中，生成患者级特征。接下来，我们开发了一种共双重交叉注意机制，将组织病理学特征与遗传数据相结合，使模型能够捕捉它们之间的相互作用。此外，使用前馈网络（FFN）合并临床数据，进一步增强预测性能并实现全面的多模式特征集成。此外，我们引入了一个加权Cox损失函数，专门用于处理不平衡生存数据，这是该领域的一个常见挑战。所提出的模型实现了0.77的平均一致性指数（C指数）和0.84的随时间变化的曲线下面积（AUC），优于最先进的方法。在单变量分析中，它预测风险（高与低），并对总生存率（OS）具有预后意义（HR=2.99，95%CI:1.88-4.78，p<0.005），在纳入标准临床病理变量的多变量分析中保持独立意义（HR/2.91，95%CI:11.80-4.68，p<005）。所提出的方法不仅提高了模型性能，而且解决了处理不平衡数据的关键差距。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10717v1" target="_blank">2402.10717v1</a>
                              </td>
                              <td>BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion</td>
                              <td>Raktim Kumar Mondol</td>
                              <td>2024-02-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10717v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10717v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18237v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18237v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18237v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18237v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, "How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x, 4x and 15x reduction in pretraining compute cost when compared to task-agnostic VFM distillation, ImageNet pretraining and DINO pretraining, respectively, while outperforming them. We also show that the dataset used for transferring knowledge has a significant effect on the final target task performance, and introduce a retrieval-augmented knowledge transfer strategy that uses web-scale image retrieval to curate effective transfer sets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18237v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在海量数据集上预训练的视觉基础模型（VFM）在各种下游任务上表现出令人印象深刻的性能，尤其是在标记的目标数据有限的情况下。然而，由于它们的高推理计算成本，这些模型不能用于许多真实世界的应用。受此启发，我们提出了以下重要问题，“我们如何利用大型VFM的知识，用有限的标记训练数据为新的目标任务训练小型任务专用模型？”，并提出了一种简单的面向任务的知识转移方法，作为该问题的高效解决方案。我们在五个目标任务上的实验结果表明，所提出的方法分别比任务无关的VFM提取、网络级CLIP预训练、监督ImageNet预训练和自监督DINO预训练高达11.6%、22.1%、13.7%和29.8%。此外，与任务无关的VFM提取、ImageNet预训练和DINO预训练相比，所提出的方法还证明了预训练计算成本分别降低了9倍、4倍和15倍，同时优于它们。我们还表明，用于传递知识的数据集对最终目标任务的性能有显著影响，并引入了一种检索增强知识传递策略，该策略使用网络规模的图像检索来策划有效的传递集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18237v2" target="_blank">2311.18237v2</a>
                              </td>
                              <td>Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models</td>
                              <td>Raviteja Vemulapalli</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18237v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18237v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_09608v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_09608v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_09608v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_09608v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce squared neural Poisson point processes (SNEPPPs) by parameterising the intensity function by the squared norm of a two layer neural network. When the hidden layer is fixed and the second layer has a single neuron, our approach resembles previous uses of squared Gaussian process or kernel methods, but allowing the hidden layer to be learnt allows for additional flexibility. In many cases of interest, the integrated intensity function admits a closed form and can be computed in quadratic time in the number of hidden neurons. We enumerate a far more extensive number of such cases than has previously been discussed. Our approach is more memory and time efficient than naive implementations of squared or exponentiated kernel methods or Gaussian processes. Maximum likelihood and maximum a posteriori estimates in a reparameterisation of the final layer of the intensity function can be obtained by solving a (strongly) convex optimisation problem using projected gradient descent. We demonstrate SNEPPPs on real, and synthetic benchmarks, and provide a software implementation. https://github.com/RussellTsuchida/snefy</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_09608v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们通过两层神经网络的平方范数对强度函数进行参数化，引入了平方神经泊松点过程（SNEPPP）。当隐藏层是固定的，第二层只有一个神经元时，我们的方法类似于以前使用的平方高斯过程或核方法，但允许学习隐藏层可以提供额外的灵活性。在许多感兴趣的情况下，积分强度函数允许采用闭合形式，并且可以在隐藏神经元数量的二次时间内计算。我们列举的这类案件比以前讨论的要多得多。我们的方法比平方或指数核方法或高斯过程的简单实现更节省内存和时间。强度函数的最终层的重新表征中的最大似然和最大后验估计可以通过使用投影梯度下降求解（强）凸优化问题来获得。我们在真实基准和综合基准上演示了SNEPPP，并提供了软件实现。https://github.com/RussellTsuchida/snefy</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.09608v1" target="_blank">2402.09608v1</a>
                              </td>
                              <td>Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families</td>
                              <td>Russell Tsuchida</td>
                              <td>2024-02-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_09608v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.09608v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/russelltsuchida/snefy" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_06287v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_06287v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_06287v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_06287v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Everyday we increasingly rely on machine learning models to automate and support high-stake tasks and decisions. This growing presence means that humans are now constantly interacting with machine learning-based systems, training and using models everyday. Several different techniques in computer science literature account for the human interaction with machine learning systems, but their classification is sparse and the goals varied. This survey proposes a taxonomy of Hybrid Decision Making Systems, providing both a conceptual and technical framework for understanding how current computer science literature models interaction between humans and machines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_06287v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>每天，我们都越来越依赖机器学习模型来自动化和支持高风险的任务和决策。这种日益增长的存在意味着人类现在每天都在不断地与基于机器学习的系统交互，训练和使用模型。计算机科学文献中的几种不同技术解释了人类与机器学习系统的交互，但它们的分类很少，目标也各不相同。这项调查提出了混合决策系统的分类法，为理解当前计算机科学文献如何模拟人与机器之间的交互提供了概念和技术框架。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.06287v1" target="_blank">2402.06287v1</a>
                              </td>
                              <td>AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems</td>
                              <td>Clara Punzi</td>
                              <td>2024-02-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_06287v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.06287v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03138v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03138v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03138v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03138v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-trained biases into exploration. We evaluate our approach on the VizDoom and Habitat environments, demonstrating that our method surpasses other well-known exploration methods in these settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03138v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对强化学习中的探索采取了以表征为中心的观点，从根本上将探索视为一个密度估计问题。我们研究了聚类表示在三维环境中用于探索的有效性，基于这样的观察，即与二维环境相比，在三维环境下转换之间的像素变化的重要性不那么明显，在二维环境中转换之间的象素变化通常是明显的。我们提出了一种方法，该方法对随机表示和预训练的DINO表示执行情节和全局聚类，以计数状态，即估计伪计数。令人惊讶的是，即使是随机特征也可以有效地聚类，以计算三维环境中的状态，然而，当这些特征在视觉上变得更加复杂时，由于表示中预先训练的归纳偏差，预先训练的DINO表示更有效。总的来说，这为将预先训练的偏见融入探索提供了一条途径。我们在VizDoom和Habitat环境中评估了我们的方法，证明我们的方法在这些环境中超越了其他众所周知的探索方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03138v1" target="_blank">2402.03138v1</a>
                              </td>
                              <td>Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations</td>
                              <td>Stefan Sylvius Wagner</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03138v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03138v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02851v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Compositional Generalization via Compositional Feature Alignment</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02851v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02851v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02851v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen. We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models. We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02851v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器学习模型的真实世界应用经常面临数据分布的变化，其中训练和测试数据分布之间存在差异。在常见的多域多类设置中，随着类和域的数量增加，为每个域-类组合收集训练数据变得不可行。这一挑战自然导致了对具有组合泛化（CG）能力的模型的探索，其中模型可以泛化到看不见的领域类组合。为了深入研究CG挑战，我们开发了CG Bench，这是一套从现有的真实世界图像数据集中导出的CG基准，并观察到在基础模型（如CLIP和DINOv2）上流行的预训练微调范式难以应对这一挑战。为了应对这一挑战，我们提出了组合特征对齐（CFA），这是一种简单的两阶段微调技术，i）在预训练的编码器上学习关于类和域标签的两个正交线性头，以及ii）在新学习的头冻结的情况下微调编码器。我们从理论和经验上证明了CFA鼓励预训练模型的组成特征学习。我们进一步在CLIP和DINOv2这两个强大的预训练视觉基础模型的CG平台上进行了广泛的实验。实验结果表明，CFA在合成泛化方面优于常用的微调技术，证实了CFA在组合特征学习方面的有效性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02851v1" target="_blank">2402.02851v1</a>
                              </td>
                              <td>Enhancing Compositional Generalization via Compositional Feature Alignment</td>
                              <td>Haoxiang Wang</td>
                              <td>2024-02-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02851v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02851v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/haoxiang-wang/compositional-feature-alignment" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02352v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Region-Based Representations Revisited</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02352v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02352v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02352v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We investigate whether region-based representations are effective for recognition. Regions were once a mainstay in recognition approaches, but pixel and patch-based features are now used almost exclusively. We show that recent class-agnostic segmenters like SAM can be effectively combined with strong unsupervised representations like DINOv2 and used for a wide variety of tasks, including semantic segmentation, object-based image retrieval, and multi-image analysis. Once the masks and features are extracted, these representations, even with linear decoders, enable competitive performance, making them well suited to applications that require custom queries. The compactness of the representation also makes it well-suited to video analysis and other problems requiring inference across many images.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02352v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们研究基于区域的表示是否对识别有效。区域曾经是识别方法的支柱，但现在几乎完全使用基于像素和补丁的特征。我们表明，最近的类不可知分割器（如SAM）可以与强无监督表示（如DINOv2）有效结合，并用于各种任务，包括语义分割、基于对象的图像检索和多图像分析。一旦提取了掩码和特征，即使使用线性解码器，这些表示也能实现有竞争力的性能，使其非常适合需要自定义查询的应用程序。该表示的紧凑性也使其非常适合于视频分析和其他需要在许多图像上进行推理的问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02352v1" target="_blank">2402.02352v1</a>
                              </td>
                              <td>Region-Based Representations Revisited</td>
                              <td>Michal Shlapentokh-Rothman</td>
                              <td>2024-02-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02352v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02352v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_07193v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DINOv2: Learning Robust Visual Features without Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_07193v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_07193v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_07193v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_07193v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近在用于对大量数据进行模型预训练的自然语言处理方面取得的突破为计算机视觉中的类似基础模型开辟了道路。这些模型可以通过产生通用的视觉特征，即在不进行微调的情况下跨图像分布和任务工作的特征，极大地简化图像在任何系统中的使用。这项工作表明，如果在来自不同来源的足够精心策划的数据上进行训练，现有的预训练方法，特别是自监督方法，可以产生这样的特征。我们重新审视现有的方法，并结合不同的技术，在数据和模型大小方面扩展我们的预训练。大多数技术贡献旨在加速和稳定大规模培训。在数据方面，我们提出了一种自动管道，以建立一个专门的、多样化的、精心策划的图像数据集，而不是像在自我监督的文献中通常做的那样，建立未经处理的数据集。在模型方面，我们训练了一个具有1B参数的ViT模型（Dosovitskiy et al.，2020），并将其提取成一系列较小的模型，这些模型在图像和像素级别的大多数基准上超过了可用的最佳通用功能OpenCLIP（Ilharco et al.，2021）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.07193v2" target="_blank">2304.07193v2</a>
                              </td>
                              <td>DINOv2: Learning Robust Visual Features without Supervision</td>
                              <td>Maxime Oquab</td>
                              <td>2023-04-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_07193v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.07193v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/facebookresearch/dinov2" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17981v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17981v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17981v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17981v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a notable advancement in multimodal understanding. We release our codes to facilitate further exploration into the fine-grained multimodal dialogue capabilities of MLLMs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17981v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管多模式大型语言模型在整合文本和图像模式方面具有令人印象深刻的能力，但在准确解释详细的视觉元素方面仍然存在挑战。本文对使用最先进的（SOTA）对象检测和光学字符识别模型增强MLLMs进行了实证研究，以提高对细粒度图像的理解并减少响应中的幻觉。我们的研究调查了基于嵌入的检测信息注入，这种注入对MLLMs原始能力的影响，以及检测模型的互换性。我们对LLaVA-1.5、DINO和PaddleOCRv2等模型进行了系统实验，结果表明，我们的方法不仅提高了MLLM在特定视觉任务中的性能，而且保持了它们的原始优势。由此增强的MLLMs在10个基准中有9个优于SOTA模型，在归一化平均分数上提高了12.99%，标志着多模态理解的显著进步。我们发布代码是为了进一步探索MLLM的细粒度多模式对话功能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17981v1" target="_blank">2401.17981v1</a>
                              </td>
                              <td>Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study</td>
                              <td>Qirui Jiao</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17981v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17981v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17632v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">What Do Self-Supervised Speech and Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17632v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17632v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17632v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning (SSL) has attracted increased attention for learning meaningful speech representations. Speech SSL models, such as WavLM, employ masked prediction training to encode general-purpose representations. In contrast, speaker SSL models, exemplified by DINO-based models, adopt utterance-level training objectives primarily for speaker representation. Understanding how these models represent information is essential for refining model efficiency and effectiveness. Unlike the various analyses of speech SSL, there has been limited investigation into what information speaker SSL captures and how its representation differs from speech SSL or other fully-supervised speaker models. This paper addresses these fundamental questions. We explore the capacity to capture various speech properties by applying SUPERB evaluation probing tasks to speech and speaker SSL models. We also examine which layers are predominantly utilized for each task to identify differences in how speech is represented. Furthermore, we conduct direct comparisons to measure the similarities between layers within and across models. Our analysis unveils that 1) the capacity to represent content information is somewhat unrelated to enhanced speaker representation, 2) specific layers of speech SSL models would be partly specialized in capturing linguistic information, and 3) speaker SSL models tend to disregard linguistic information but exhibit more sophisticated speaker representation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17632v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>自监督学习（SSL）在学习有意义的语音表征方面引起了越来越多的关注。语音SSL模型，如WavLM，采用掩蔽预测训练来编码通用表示。相反，说话者SSL模型，以基于DINO的模型为例，主要针对说话者表示采用话语水平训练目标。了解这些模型如何表示信息对于提高模型的效率和有效性至关重要。与语音SSL的各种分析不同，对说话者SSL捕获的信息以及其表示与语音SSL或其他完全监督的说话者模型的不同之处的研究有限。本文解决了这些基本问题。我们通过将SUPERB评估探测任务应用于语音和说话者SSL模型来探索捕获各种语音属性的能力。我们还检查了每个任务主要使用哪些层来识别语音表示方式的差异。此外，我们进行直接比较，以衡量模型内和模型间各层之间的相似性。我们的分析表明，1）表示内容信息的能力与增强的说话者表示有些无关，2）语音SSL模型的特定层将部分专门用于捕获语言信息，3）说话者SSL模型倾向于忽略语言信息，但表现出更复杂的说话者表示。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17632v1" target="_blank">2401.17632v1</a>
                              </td>
                              <td>What Do Self-Supervised Speech and Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis</td>
                              <td>Takanori Ashihara</td>
                              <td>2024-01-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17632v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17632v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_08873v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_08873v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_08873v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_08873v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like "Can you pass through the curtains to deliver medicines to me?", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable objects such as curtains and grasses for verification by instructing the robot to traverse them. Besides, traversing curtains in a medical scenario was tested. All experimental results demonstrated the proposed framework's effectiveness and adaptability to diverse environments.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_08873v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文利用大型语言和视觉语言模型提出了一种交互式导航框架，使机器人能够在有可穿越障碍物的环境中导航。我们利用大型语言模型（GPT-3.5）和开放集视觉语言模型（Grounding DINO）创建一个动作感知成本图，在不进行微调的情况下执行有效的路径规划。使用大型模型，我们可以实现一个端到端的系统，从“你能穿过窗帘给我送药吗？”这样的文本说明，到具有动作感知属性的边界框（例如窗帘）。它们可以用于将激光雷达点云划分为两部分：可遍历部分和不可遍历部分，然后构建行动感知成本图以生成可行路径。经过预训练的大型模型具有很强的泛化能力，不需要额外的注释数据进行训练，允许在交互式导航任务中快速部署。我们选择使用多个可遍历对象，如窗帘和草，通过指示机器人遍历它们来进行验证。此外，还测试了在医疗场景中穿过窗帘的情况。所有实验结果都证明了所提出的框架的有效性和对不同环境的适应性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.08873v2" target="_blank">2310.08873v2</a>
                              </td>
                              <td>Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models</td>
                              <td>Zhen Zhang</td>
                              <td>2023-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_08873v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.08873v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05925v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with Dual Feature Fusion</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05925v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05925v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05925v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based segmentation methods have relied on time-consuming neural scene optimization. While recent 3D Gaussian Splatting has notably improved speed, existing Gaussian-based segmentation methods struggle to produce compact masks, especially in zero-shot segmentation. This issue probably stems from their straightforward assignment of learnable parameters to each Gaussian, resulting in a lack of robustness against cross-view inconsistent 2D machine-generated labels. Our method aims to address this problem by employing Dual Feature Fusion Network as Gaussians' segmentation field. Specifically, we first optimize 3D Gaussians under RGB supervision. After Gaussian Locating, DINO features extracted from images are applied through explicit unprojection, which are further incorporated with spatial features from the efficient point cloud processing network. Feature aggregation is utilized to fuse them in a global-to-local strategy for compact segmentation features. Experimental results show that our model outperforms baselines on both semantic and panoptic zero-shot segmentation task, meanwhile consumes less than 10% inference time compared to NeRF-based methods. Code and more results will be available at https://David-Dou.github.io/CoSSegGaussians</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05925v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了紧凑和快速分割3D高斯（CoSSegGaussians），这是一种在仅输入RGB图像的情况下以快速渲染速度进行紧凑的3D一致场景分割的方法。以前基于NeRF的分割方法依赖于耗时的神经场景优化。虽然最近的3D高斯Splatting显著提高了速度，但现有的基于高斯的分割方法很难产生紧凑的掩模，尤其是在零样本分割中。这个问题可能源于他们将可学习参数直接分配给每个高斯，导致对交叉视图不一致的2D机器生成标签缺乏鲁棒性。我们的方法旨在通过使用双特征融合网络作为高斯分割域来解决这个问题。具体来说，我们首先在RGB监督下优化3D高斯。在高斯定位之后，通过显式反投影应用从图像中提取的DINO特征，并将其与高效点云处理网络的空间特征进一步融合。利用特征聚合将它们融合在全局到局部的策略中，以实现紧凑的分割特征。实验结果表明，与基于NeRF的方法相比，我们的模型在语义和全景零样本分割任务上都优于基线，同时消耗不到10%的推理时间。代码和更多结果将在https://David-Dou.github.io/CoSSegGaussians</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05925v3" target="_blank">2401.05925v3</a>
                              </td>
                              <td>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with Dual Feature Fusion</td>
                              <td>Bin Dou</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05925v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05925v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14555v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Revisiting Active Learning in the Era of Vision Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14555v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14555v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14555v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zero- or few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We extensively test our strategy on many challenging image classification benchmarks, including natural images as well as out-of-domain biomedical images that are relatively understudied in the AL literature. Source code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14555v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基础视觉或视觉语言模型在大的未标记或有噪声的数据上进行训练，并学习稳健的表示，这些表示可以在不同的任务上实现令人印象深刻的零或少镜头性能。考虑到这些特性，它们自然适合主动学习（AL），其目的是最大限度地提高标签效率，但尚未在主动学习的背景下，特别是在低预算制度下，探索基础模型的全部潜力。在这项工作中，我们评估了基础模型如何影响有效AL的三个关键组成部分，即1）初始标记池选择，2）确保多样化采样，以及3）代表性采样和不确定性采样之间的权衡。我们系统地研究了基础模型（DINOv2，OpenCLIP）的鲁棒表示如何挑战主动学习中的现有发现。我们的观察结果为一种新的简单而优雅的AL策略的原则性构建提供了信息，该策略平衡了通过丢弃估计的不确定性和样本多样性。我们在许多具有挑战性的图像分类基准上广泛测试了我们的策略，包括自然图像以及AL文献中研究相对不足的领域外生物医学图像。将提供源代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14555v1" target="_blank">2401.14555v1</a>
                              </td>
                              <td>Revisiting Active Learning in the Era of Vision Foundation Models</td>
                              <td>Sanket Rajan Gupte</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14555v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14555v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tempconfx/al-foundation-models" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14159v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14159v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14159v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14159v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了Grounded SAM，它使用Grounding DINO作为开集对象检测器与分段任意模型（SAM）相结合。这种集成能够基于任意文本输入检测和分割任何区域，并为连接各种视觉模型打开了大门。如图1所示，通过使用通用的接地SAM管道，可以实现广泛的视觉任务。例如，可以通过结合诸如BLIP和Recognize Anything之类的模型来实现仅基于输入图像的自动注释流水线。此外，结合Stable Diffusion可以进行可控的图像编辑，而OSX的集成有助于快速进行3D人体运动分析。Grounded SAM在开放词汇基准测试中也表现出优异的性能，在SegInW（野外细分）零样本基准测试中，通过Grounding DINO-Base和SAM-Huge模型的组合，达到了48.7的平均AP。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14159v1" target="_blank">2401.14159v1</a>
                              </td>
                              <td>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</td>
                              <td>Tianhe Ren</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14159v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14159v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13987v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Domain Few-Shot Learning via Adaptive Transformer Networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13987v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13987v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13987v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13987v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>大多数少数镜头学习工作依赖于基础任务和目标任务之间的相同领域假设，阻碍了它们的实际应用。本文提出了一种自适应变换网络（ADAPTER），这是一种简单但有效的跨域少镜头学习解决方案，其中基本任务和目标任务之间存在较大的域偏移。ADAPTER建立在双向交叉注意力的思想之上，以学习两个领域之间的可转移特征。所提出的体系结构是用DINO训练的，以产生多样化的、较少偏见的特征，从而避免监督崩溃的问题。此外，还提出了标签平滑方法，通过考虑嵌入空间中紧密样本的预测标签来提高预测的一致性和可靠性。ADAPTER的性能在BSCD-FSL基准中得到了严格评估，在这些基准中，它以显著的优势优于现有技术。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13987v1" target="_blank">2401.13987v1</a>
                              </td>
                              <td>Cross-Domain Few-Shot Learning via Adaptive Transformer Networks</td>
                              <td>Naeem Paeedeh</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13987v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13987v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/naeem-paeedeh/adapter" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11673v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11673v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11673v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11673v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11673v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于学习的多视图立体（MVS）方法的最新进展突出了具有注意力机制的基于变换器的模型。然而，现有的方法没有彻底研究变压器对不同MVS模块的深刻影响，导致深度估计能力有限。在本文中，我们介绍了MVSFormer++，这是一种谨慎地最大化注意力的固有特性以增强MVS管道的各个组件的方法。从形式上讲，我们的方法包括将跨视图信息注入预先训练的DINOv2模型中，以促进MVS学习。此外，我们对特征编码器和代价体积正则化采用了不同的注意力机制，分别关注特征和空间聚合。此外，我们发现一些设计细节会显著影响MVS中转换器模块的性能，包括归一化的3D位置编码、自适应注意力缩放和层归一化的位置。在DTU、Tanks and Temples、BlendedMVS和ETH3D上进行的综合实验验证了该方法的有效性。值得注意的是，MVSFormer++在具有挑战性的DTU和坦克与神庙基准上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11673v1" target="_blank">2401.11673v1</a>
                              </td>
                              <td>MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo</td>
                              <td>Chenjie Cao</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11673v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11673v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/maybelx/mvsformerplusplus" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11311v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11311v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11311v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11311v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, the rapid evolution of computer vision has seen the emergence of various vision foundation models, each tailored to specific data types and tasks. While large language models often share a common pretext task, the diversity in vision foundation models arises from their varying training objectives. In this study, we delve into the quest for identifying the most effective vision foundation models for few-shot semantic segmentation, a critical task in computer vision. Specifically, we conduct a comprehensive comparative analysis of four prominent foundation models: DINO V2, Segment Anything, CLIP, Masked AutoEncoders, and a straightforward ResNet50 pre-trained on the COCO dataset. Our investigation focuses on their adaptability to new semantic segmentation tasks, leveraging only a limited number of segmented images. Our experimental findings reveal that DINO V2 consistently outperforms the other considered foundation models across a diverse range of datasets and adaptation methods. This outcome underscores DINO V2's superior capability to adapt to semantic segmentation tasks compared to its counterparts. Furthermore, our observations indicate that various adapter methods exhibit similar performance, emphasizing the paramount importance of selecting a robust feature extractor over the intricacies of the adaptation technique itself. This insight sheds light on the critical role of feature extraction in the context of few-shot semantic segmentation. This research not only contributes valuable insights into the comparative performance of vision foundation models in the realm of few-shot semantic segmentation but also highlights the significance of a robust feature extractor in this domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11311v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着计算机视觉的快速发展，出现了各种视觉基础模型，每种模型都适合特定的数据类型和任务。虽然大型语言模型通常有一个共同的借口任务，但视觉基础模型的多样性源于它们不同的训练目标。在这项研究中，我们深入探讨了为少镜头语义分割（计算机视觉中的一项关键任务）确定最有效的视觉基础模型的探索。具体而言，我们对四个突出的基础模型进行了全面的比较分析：DINO V2、Segment Anything、CLIP、Masked AutoEncoders和在COCO数据集上预训练的直接ResNet50。我们的研究重点是它们对新的语义分割任务的适应性，仅利用有限数量的分割图像。我们的实验结果表明，在各种数据集和适应方法中，DINO V2始终优于其他考虑的基础模型。这一结果突显了与同类产品相比，DINO V2在适应语义分割任务方面的卓越能力。此外，我们的观察结果表明，各种适配器方法表现出相似的性能，强调了选择鲁棒特征提取器的至关重要性，而不是自适应技术本身的复杂性。这一见解揭示了特征提取在少镜头语义分割中的关键作用。这项研究不仅为视觉基础模型在少镜头语义分割领域的比较性能提供了有价值的见解，而且突出了鲁棒特征提取器在该领域的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11311v1" target="_blank">2401.11311v1</a>
                              </td>
                              <td>A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</td>
                              <td>Reda Bensaid</td>
                              <td>2024-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11311v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11311v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_10815v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_10815v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_10815v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_10815v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Language-supervised pre-training has proven to be a valuable method for extracting semantically meaningful features from images, serving as a foundational element in multimodal systems within the computer vision and medical imaging domains. However, resulting features are limited by the information contained within the text. This is particularly problematic in medical imaging, where radiologists' written findings focus on specific observations; a challenge compounded by the scarcity of paired imaging-text data due to concerns over leakage of personal health information. In this work, we fundamentally challenge the prevailing reliance on language supervision for learning general purpose biomedical imaging encoders. We introduce RAD-DINO, a biomedical image encoder pre-trained solely on unimodal biomedical imaging data that obtains similar or greater performance than state-of-the-art biomedical language supervised models on a diverse range of benchmarks. Specifically, the quality of learned representations is evaluated on standard imaging tasks (classification and semantic segmentation), and a vision-language alignment task (text report generation from images). To further demonstrate the drawback of language supervision, we show that features from RAD-DINO correlate with other medical records (e.g., sex or age) better than language-supervised models, which are generally not mentioned in radiology reports. Finally, we conduct a series of ablations determining the factors in RAD-DINO's performance; notably, we observe that RAD-DINO's downstream performance scales well with the quantity and diversity of training data, demonstrating that image-only supervision is a scalable approach for training a foundational biomedical image encoder.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_10815v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语言监督预训练已被证明是从图像中提取语义上有意义的特征的一种有价值的方法，是计算机视觉和医学成像领域中多模式系统的基础元素。但是，生成的特征受到文本中包含的信息的限制。这在医学成像中尤其有问题，因为放射科医生的书面发现侧重于特定的观察结果；由于担心个人健康信息的泄露，配对成像文本数据的稀缺性加剧了这一挑战。在这项工作中，我们从根本上挑战了学习通用生物医学成像编码器时普遍依赖语言监督的现状。我们介绍了RAD-DINO，这是一种仅在单峰生物医学成像数据上预训练的生物医学图像编码器，在各种基准上获得与最先进的生物医学语言监督模型相似或更高的性能。具体而言，学习表示的质量是在标准成像任务（分类和语义分割）和视觉语言对齐任务（从图像生成文本报告）上评估的。为了进一步证明语言监督的缺点，我们发现RAD-DINO的特征与其他医疗记录（如性别或年龄）的相关性比语言监督模型更好，而语言监督模型通常在放射学报告中没有提及。最后，我们进行了一系列烧蚀，确定了影响RAD-DINO性能的因素；值得注意的是，我们观察到RAD-DINO的下游性能随着训练数据的数量和多样性而良好地扩展，这表明仅图像监督是训练基础生物医学图像编码器的一种可扩展方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.10815v1" target="_blank">2401.10815v1</a>
                              </td>
                              <td>RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision</td>
                              <td>Fernando Pérez-García</td>
                              <td>2024-01-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_10815v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.10815v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_07951v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Image Similarity using An Ensemble of Context-Sensitive Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_07951v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_07951v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_07951v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Image similarity has been extensively studied in computer vision. In recently years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling similarity, assigning a numerical score to a pair of images is less intuitive than determining if an image A is closer to a reference image R than another image B. In this work, we present a novel approach for building an image similarity model based on labelled data in the form of A:R vs B:R. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. In particular, we employed two ML techniques to construct such an ensemble model, namely dimensionality reduction and MLP regressors. Our testing results show that the ensemble model constructed performs ~5% better than the best individual context-sensitive models. They also performed better than the model trained with mixed imagery data as well as existing similarity models, e.g., CLIP and DINO. This work demonstrate that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_07951v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图像相似性在计算机视觉中得到了广泛的研究。近年来，机器学习模型已经显示出其比传统的多元度量编码更多语义的能力。然而，在标记相似性时，将数字分数分配给一对图像不如确定一个图像a是否比另一个图像B更接近参考图像R直观。在这项工作中，我们提出了一种新的方法，用于基于a:R与B:R形式的标记数据建立图像相似性模型。我们解决了图像空间（R，a，B）中稀疏采样的挑战以及通过使用集成模型用基于上下文的数据训练的模型中的偏差。特别地，我们使用了两种ML技术来构建这样的集成模型，即降维和MLP回归。我们的测试结果表明，所构建的集成模型的性能比最好的单个上下文敏感模型好约5%。它们的性能也优于用混合图像数据训练的模型以及现有的相似性模型，例如CLIP和DINO。这项工作表明，当使用适当的集成方法来缓解稀疏采样带来的限制时，基于上下文的标记和模型训练是有效的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.07951v1" target="_blank">2401.07951v1</a>
                              </td>
                              <td>Image Similarity using An Ensemble of Context-Sensitive Models</td>
                              <td>Zukang Liao</td>
                              <td>2024-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_07951v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.07951v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_06013v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Surgical-DINO: Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_06013v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_06013v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_06013v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically show that Surgical-DINO significantly outperforms all the state-of-the-art models in endoscopic depth estimation tasks. The analysis with ablation studies has shown evidence of the remarkable effect of our LoRA layers and adaptation. Conclusion: Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain for depth estimation. There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision datasets or naive fine-tuning is not sufficient to use the foundation model in the surgical domain directly. Code is available at https://github.com/BeileiCui/SurgicalDINO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_06013v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目的：机器人手术中的深度估计在三维重建、手术导航和增强现实可视化中至关重要。尽管基础模型在许多视觉任务中表现出出色的性能，包括深度估计（例如，DINOv2），但最近的工作观察到其在医学和外科领域特定应用中的局限性。这项工作提出了一种用于手术深度估计的基础模型的低阶自适应（LoRA）。方法：我们设计了一种基于基础模型的深度估计方法，称为Surgical DINO，这是对DINOv2的低阶自适应，用于内窥镜手术中的深度估计。我们构建了LoRA层，并将其集成到DINO中，以适应手术特定领域的知识，而不是传统的微调。在训练过程中，我们冻结了显示出出色视觉表示能力的DINO图像编码器，并仅优化了LoRA层和深度解码器，以集成来自手术场景的特征。结果：我们的模型在SCARED的MICCAI挑战数据集上得到了广泛验证，该数据集是从达芬奇Xi内窥镜手术中收集的。我们的经验表明，外科DINO在内窥镜深度估计任务中显著优于所有最先进的模型。消融研究的分析表明，我们的LoRA层和适应具有显著效果。结论：外科DINO为基础模型成功适应外科领域进行深度估计提供了一些启示。结果中有明确证据表明，对计算机视觉数据集中预先训练的权重进行零样本预测或简单微调不足以直接在外科领域使用基础模型。代码位于https://github.com/BeileiCui/SurgicalDINO.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.06013v2" target="_blank">2401.06013v2</a>
                              </td>
                              <td>Surgical-DINO: Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery</td>
                              <td>Beilei Cui</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_06013v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.06013v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/beileicui/surgicaldino" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_14093v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly Supervised 3D Open-vocabulary Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_14093v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_14093v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_14093v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_14093v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>三维场景的开放式词汇分割是人类感知的基本功能，因此也是计算机视觉研究的重要目标。然而，由于缺乏用于训练健壮和可推广模型的大规模和多样化的3D开放词汇分割数据集，这项任务受到了严重阻碍。从预先训练的2D开放词汇分割模型中提取知识有帮助，但它损害了开放词汇的特征，因为2D模型大多是用紧密的词汇数据集进行微调的。我们通过以弱监督的方式利用预先训练的基础模型CLIP和DINO来解决3D开放词汇分割中的挑战。具体而言，仅给定场景中对象的开放词汇文本描述，我们将CLIP和DINO的开放词汇多模态知识和对象推理能力提取到神经辐射场（NeRF）中，这有效地将2D特征提升到视图一致的3D分割中。我们的方法的一个值得注意的方面是，它不需要对基础模型或蒸馏过程进行任何手动分割注释。大量实验表明，在某些场景中，我们的方法甚至优于用分割注释训练的完全监督模型，这表明可以从2D图像和文本图像对中有效地学习3D开放词汇分割。代码位于\url{https://github.com/Kunhao-Liu/3D-OVS}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.14093v4" target="_blank">2305.14093v4</a>
                              </td>
                              <td>Weakly Supervised 3D Open-vocabulary Segmentation</td>
                              <td>Kunhao Liu</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_14093v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.14093v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/kunhao-liu/3d-ovs" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02957v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Denoising Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02957v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02957v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02957v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We delve into a nuanced but significant challenge inherent to Vision Transformers (ViTs): feature maps of these models exhibit grid-like artifacts, which detrimentally hurt the performance of ViTs in downstream tasks. Our investigations trace this fundamental issue down to the positional embeddings at the input stage. To address this, we propose a novel noise model, which is universally applicable to all ViTs. Specifically, the noise model dissects ViT outputs into three components: a semantics term free from noise artifacts and two artifact-related terms that are conditioned on pixel locations. Such a decomposition is achieved by enforcing cross-view feature consistency with neural fields in a per-image basis. This per-image optimization process extracts artifact-free features from raw ViT outputs, providing clean features for offline applications. Expanding the scope of our solution to support online functionality, we introduce a learnable denoiser to predict artifact-free features directly from unprocessed ViT outputs, which shows remarkable generalization capabilities to novel data without the need for per-image optimization. Our two-stage approach, termed Denoising Vision Transformers (DVT), does not require re-training existing pre-trained ViTs and is immediately applicable to any Transformer-based architecture. We evaluate our method on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP, DINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT consistently and significantly improves existing state-of-the-art general-purpose models in semantic and geometric tasks across multiple datasets (e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT design, especially regarding the naive use of positional embeddings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02957v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们深入研究了视觉转换器（ViTs）固有的一个细微但重大的挑战：这些模型的特征图显示出网格状的伪影，这对ViTs在下游任务中的性能造成了不利影响。我们的研究将这个基本问题追溯到输入阶段的位置嵌入。为了解决这一问题，我们提出了一种新的噪声模型，该模型普遍适用于所有的ViT。具体来说，噪声模型将ViT输出分解为三个部分：一个没有噪声伪影的语义术语和两个以像素位置为条件的伪影相关术语。这种分解是通过在每幅图像的基础上加强与神经场的交叉视图特征一致性来实现的。这种逐图像优化过程从原始ViT输出中提取无伪影特征，为离线应用程序提供干净的特征。扩大了我们的解决方案的范围，以支持在线功能，我们引入了一种可学习的去噪器，直接从未处理的ViT输出中预测无伪影特征，这显示出对新数据的显著泛化能力，而无需对每张图像进行优化。我们的两阶段方法，称为去噪视觉转换器（DVT），不需要重新训练现有的预先训练的ViT，并且立即适用于任何基于转换器的架构。我们在各种具有代表性的ViT（DINO、MAE、DeiT III、EVA02、CLIP、DINOv2、DINOv2-reg）上评估了我们的方法。广泛的评估表明，我们的DVT在多个数据集（例如+3.84mIoU）的语义和几何任务中持续显著地改进了现有的最先进的通用模型。我们希望我们的研究将鼓励对ViT设计进行重新评估，特别是关于位置嵌入的天真使用。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02957v1" target="_blank">2401.02957v1</a>
                              </td>
                              <td>Denoising Vision Transformers</td>
                              <td>Jiawei Yang</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02957v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02957v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02361v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02361v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02361v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02361v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounding-DINO is a state-of-the-art open-set detection model that tackles multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness has led to its widespread adoption as a mainstream architecture for various downstream applications. However, despite its significance, the original Grounding-DINO model lacks comprehensive public technical details due to the unavailability of its training code. To bridge this gap, we present MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline, which is built with the MMDetection toolbox. It adopts abundant vision datasets for pre-training and various detection and grounding datasets for fine-tuning. We give a comprehensive analysis of each reported result and detailed settings for reproduction. The extensive experiments on the benchmarks mentioned demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny baseline. We release all our models to the research community. Codes and trained models are released at https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02361v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Grounding DINO是一种最先进的开放集检测模型，可处理多种视觉任务，包括开放词汇检测（OVD）、短语基础（PG）和参考表达理解（REC）。它的有效性导致它被广泛采用为各种下游应用程序的主流架构。然而，尽管其意义重大，但由于其培训代码的不可用，最初的Grounding DINO模型缺乏全面的公共技术细节。为了弥补这一差距，我们推出了MM Grounding DINO，这是一个开源、全面、用户友好的基线，它是用MMDetection工具箱构建的。它采用丰富的视觉数据集进行预训练，并采用各种检测和基础数据集进行微调。我们对每一个报告的结果进行了全面的分析，并对复制进行了详细的设置。对上述基准的广泛实验表明，我们的MM Grounding DINO Tiny优于Grounding DINO Tiny基线。我们向研究界发布所有模型。代码和经过训练的模型发布于https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02361v2" target="_blank">2401.02361v2</a>
                              </td>
                              <td>An Open and Comprehensive Pipeline for Unified Object Grounding and Detection</td>
                              <td>Xiangyu Zhao</td>
                              <td>2024-01-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02361v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02361v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/open-mmlab/mmdetection" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_12735v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_12735v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_12735v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_12735v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose integrally pre-trained transformer pyramid network (iTPN), towards jointly optimizing the network backbone and the neck, so that transfer gap between representation models and downstream tasks is minimal. iTPN is born with two elaborated designs: 1) The first pre-trained feature pyramid upon vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing computational memory overhead and accelerating inference through two flexible designs. 1) Token migration: dropping redundant tokens of the backbone while replenishing them in the feature pyramid without attention operations. 2) Token gathering: reducing computation cost caused by global attention by introducing few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1 accuracy on ImageNet-1K. With 1x training schedule using DINO, the base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object detection, and a 57.5%/58.7% mIoU on ADE20K semantic segmentation using MaskDINO. Fast-iTPN can accelerate the inference procedure by up to 70%, with negligible performance loss, demonstrating the potential to be a powerful backbone for downstream vision tasks. The code is available at: github.com/sunsmarterjie/iTPN.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_12735v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了整体预训练的变换金字塔网络（iTPN），以联合优化网络主干和瓶颈，从而使表示模型和下游任务之间的传输间隙最小。iTPN诞生于两个精心设计：1）第一个预先训练的视觉转换器上的特征金字塔（ViT）。2） 使用掩蔽特征建模（MFM）对特征金字塔进行多阶段监督。iTPN更新为Fast iTPN，通过两种灵活的设计减少了计算内存开销并加速了推理。1） 令牌迁移：丢弃主干的冗余令牌，同时在功能金字塔中补充它们，而无需注意操作。2） 代币采集：通过引入少量采集代币，降低全球关注带来的计算成本。基本/大级别Fast iTPN在ImageNet-1K上实现了88.75%/89.5%的前1级精度。在使用DINO的1x训练计划的情况下，基本/大级别Fast iTPN在COCO对象检测上实现了58.4%/58.8%的box AP，在使用MaskDINO的ADE20K语义分割上实现了57.5%/58.7%的mIoU。快速iTPN可以将推理过程加速70%，而性能损失可以忽略不计，这表明它有潜力成为下游视觉任务的强大支柱。该代码位于：github.com/sunsmarterjie/iTPN。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.12735v2" target="_blank">2211.12735v2</a>
                              </td>
                              <td>Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration</td>
                              <td>Yunjie Tian</td>
                              <td>2022-11-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_12735v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.12735v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sunsmarterjie/itpn" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_01013v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_01013v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_01013v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_01013v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU) has revealed that traditional machine learning methods, such as semi-supervised label propagation and K-nearest neighbors, outperform Transformer-based models in artifact detection from PPG signals, mainly when data is limited. This study addresses the underutilization of abundant unlabeled data by employing self-supervised learning (SSL) to extract latent features from these data, followed by fine-tuning on labeled data. Our experiments demonstrate that SSL significantly enhances the Transformer model's ability to learn representations, improving its robustness in artifact classification tasks. Among various SSL techniques, including masking, contrastive learning, and DINO (self-distillation with no labels)-contrastive learning exhibited the most stable and superior performance in small PPG datasets. Further, we delve into optimizing contrastive loss functions, which are crucial for contrastive SSL. Inspired by InfoNCE, we introduce a novel contrastive loss function that facilitates smoother training and better convergence, thereby enhancing performance in artifact classification. In summary, this study establishes the efficacy of SSL in leveraging unlabeled data, particularly in enhancing the capabilities of the Transformer model. This approach holds promise for broader applications in PICU environments, where annotated data is often limited.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_01013v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>CHU Sainte Justine儿科重症监护室（PICU）最近的研究表明，传统的机器学习方法，如半监督标签传播和K近邻，在PPG信号的伪影检测方面，主要是在数据有限的情况下，优于基于Transformer的模型。这项研究通过使用自监督学习（SSL）从这些数据中提取潜在特征，然后对标记数据进行微调，解决了大量未标记数据利用不足的问题。我们的实验表明，SSL显著增强了Transformer模型学习表示的能力，提高了其在工件分类任务中的稳健性。在各种SSL技术中，包括掩蔽、对比学习和DINO（无标签的自蒸馏），对比学习在小型PPG数据集中表现出最稳定和优越的性能。此外，我们深入研究了优化对比损失函数，这对对比SSL至关重要。受InfoNCE的启发，我们引入了一种新的对比损失函数，该函数有助于更平滑的训练和更好的收敛，从而提高伪像分类的性能。总之，本研究确定了SSL在利用未标记数据方面的有效性，特别是在增强Transformer模型的能力方面。这种方法有望在PICU环境中获得更广泛的应用，在PICU中，注释数据通常是有限的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.01013v1" target="_blank">2401.01013v1</a>
                              </td>
                              <td>Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning</td>
                              <td>Thanh-Dung Le</td>
                              <td>2024-01-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_01013v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.01013v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_00463v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analyzing Local Representations of Self-supervised Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_00463v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_00463v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_00463v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present a comparative analysis of various self-supervised Vision Transformers (ViTs), focusing on their local representative power. Inspired by large language models, we examine the abilities of ViTs to perform various computer vision tasks with little to no fine-tuning. We design an evaluation framework to analyze the quality of local, i.e. patch-level, representations in the context of few-shot semantic segmentation, instance identification, object retrieval, and tracking. We discover that contrastive learning based methods like DINO produce more universal patch representations that can be immediately applied for downstream tasks with no parameter tuning, compared to masked image modeling. The embeddings learned using the latter approach, e.g. in masked autoencoders, have high variance features that harm distance-based algorithms, such as k-NN, and do not contain useful information for most downstream tasks. Furthermore, we demonstrate that removing these high-variance features enhances k-NN by providing an analysis of the benchmarks for this work and for Scale-MAE, a recent extension of masked autoencoders. Finally, we find an object instance retrieval setting where DINOv2, a model pretrained on two orders of magnitude more data, performs worse than its less compute-intensive counterpart DINO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_00463v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对各种自监督视觉变压器（ViT）进行了比较分析，重点分析了它们的局部代表性。受大型语言模型的启发，我们研究了ViT在几乎没有微调的情况下执行各种计算机视觉任务的能力。我们设计了一个评估框架来分析局部（即补丁级别）表示在少镜头语义分割、实例识别、对象检索和跟踪背景下的质量。我们发现，与掩蔽图像建模相比，基于对比学习的方法（如DINO）产生了更通用的补丁表示，可以立即应用于下游任务，而无需参数调整。使用后一种方法学习的嵌入，例如在掩码自动编码器中，具有高方差特征，这会损害基于距离的算法，例如k-NN，并且不包含用于大多数下游任务的有用信息。此外，我们证明，通过为这项工作和Scale MAE（掩蔽自动编码器的最近扩展）提供基准分析，去除这些高方差特征可以增强k-NN。最后，我们找到了一个对象实例检索设置，其中DINOv2，一个在两个数量级以上的数据上预训练的模型，其性能比计算密集度较低的对应DINO差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.00463v1" target="_blank">2401.00463v1</a>
                              </td>
                              <td>Analyzing Local Representations of Self-supervised Vision Transformers</td>
                              <td>Ani Vanyan</td>
                              <td>2023-12-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_00463v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.00463v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03940v4_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hard View Selection for Self-Supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03940v4_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03940v4_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03940v4_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Many Self-Supervised Learning (SSL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during SSL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward passes for each view pair on the currently trained model, 3) adversarially select the pair yielding the worst loss, and 4) run the backward pass with the selected pair. In our empirical analysis we show that under the hood, HVS increases task difficulty by controlling the Intersection over Union of views during pretraining. With only 300-epoch pretraining, HVS is able to closely rival the 800-epoch DINO baseline which remains very favorable even when factoring in the slowdown induced by the additional forwards of HVS. Additionally, HVS consistently achieves accuracy improvements on ImageNet between 0.4% and 1.9% on linear evaluation and similar improvements on transfer tasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03940v4_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>许多自监督学习（SSL）方法将其模型训练为对图像输入的不同“视图”保持不变，而良好的数据增强管道对图像输入至关重要。虽然在改进文本前任务、架构或稳健性（例如，连体网络或教师softmax居中）方面做出了相当大的努力，但这些方法中的大多数仍然强烈依赖于图像增强管道内的操作的随机采样，例如随机调整大小的裁剪或颜色失真操作。在本文中，我们认为到目前为止，视图生成的作用及其对性能的影响还没有得到足够的关注。为了解决这一问题，我们提出了一种简单、无需学习但功能强大的硬视图选择（HVS）策略，旨在扩展随机视图生成，以便在SSL训练期间将预训练的模型暴露给更硬的样本。它包括以下迭代步骤：1）随机采样多个视图并创建两个视图对，2）在当前训练的模型上为每个视图对运行前向通道，3）对抗性地选择产生最差损失的一对，以及4）使用所选的一对运行后向通道。在我们的实证分析中，我们发现在引擎盖下，HVS通过在预训练过程中控制视图并集上的交集来增加任务难度。只有300个历元的预训练，HVS能够与800个历元DINO基线相媲美，即使考虑到HVS额外前锋导致的速度减慢，这一基线仍然非常有利。此外，HVS在ImageNet上的线性评估准确率持续提高0.4%至1.9%，在多种SSL方法（如DINO、SimSiam、iBOT和SimCLR）的传输任务上也实现了类似的提高。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03940v4" target="_blank">2310.03940v4</a>
                              </td>
                              <td>Hard View Selection for Self-Supervised Learning</td>
                              <td>Fabio Ferreira</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03940v4_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03940v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_18628v2_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Lightweight Clustering Framework for Unsupervised Semantic Segmentation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_18628v2_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_18628v2_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_18628v2_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Unsupervised semantic segmentation aims to categorize each pixel in an image into a corresponding class without the use of annotated data. It is a widely researched area as obtaining labeled datasets is expensive. While previous works in the field have demonstrated a gradual improvement in model accuracy, most required neural network training. This made segmentation equally expensive, especially when dealing with large-scale datasets. We thus propose a lightweight clustering framework for unsupervised semantic segmentation. We discovered that attention features of the self-supervised Vision Transformer exhibit strong foreground-background differentiability. Therefore, clustering can be employed to effectively separate foreground and background image patches. In our framework, we first perform multilevel clustering across the Dataset-level, Category-level, and Image-level, and maintain consistency throughout. Then, the binary patch-level pseudo-masks extracted are upsampled, refined and finally labeled. Furthermore, we provide a comprehensive analysis of the self-supervised Vision Transformer features and a detailed comparison between DINO and DINOv2 to justify our claims. Our framework demonstrates great promise in unsupervised semantic segmentation and achieves state-of-the-art results on PASCAL VOC and MS COCO datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_18628v2_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无监督语义分割旨在将图像中的每个像素分类到相应的类别中，而不使用注释数据。这是一个广泛研究的领域，因为获得标记的数据集是昂贵的。虽然该领域先前的工作已经证明模型精度逐渐提高，但大多数都需要神经网络训练。这使得分割同样昂贵，尤其是在处理大规模数据集时。因此，我们提出了一种用于无监督语义分割的轻量级聚类框架。我们发现，自监督视觉转换器的注意力特征表现出很强的前景-背景可微性。因此，可以采用聚类来有效地分离前景和背景图像块。在我们的框架中，我们首先在数据集级别、类别级别和图像级别执行多级聚类，并始终保持一致性。然后，对提取的二进制补丁级伪掩码进行上采样、细化和最终标记。此外，我们对自监督视觉转换器的功能进行了全面分析，并对DINO和DINOv2进行了详细比较，以证明我们的说法是正确的。我们的框架在无监督语义分割方面表现出了巨大的前景，并在PASCAL VOC和MS COCO数据集上取得了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.18628v2" target="_blank">2311.18628v2</a>
                              </td>
                              <td>A Lightweight Clustering Framework for Unsupervised Semantic Segmentation</td>
                              <td>Yau Shing Jonathan Cheung</td>
                              <td>2023-11-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_18628v2_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.18628v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17742v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Vision from Models Rivals Learning Vision from Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17742v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17742v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17742v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce SynCLR, a novel approach for learning visual representations exclusively from synthetic images and synthetic captions, without any real data. We synthesize a large dataset of image captions using LLMs, then use an off-the-shelf text-to-image model to generate multiple images corresponding to each synthetic caption. We perform visual representation learning on these synthetic images via contrastive learning, treating images sharing the same caption as positive pairs. The resulting representations transfer well to many downstream tasks, competing favorably with other general-purpose visual representation learners such as CLIP and DINO v2 in image classification tasks. Furthermore, in dense prediction tasks such as semantic segmentation, SynCLR outperforms previous self-supervised methods by a significant margin, e.g., improving over MAE and iBOT by 6.2 and 4.3 mIoU on ADE20k for ViT-B/16.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17742v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍了SynCLR，这是一种专门从合成图像和合成字幕中学习视觉表示的新方法，无需任何真实数据。我们使用LLM合成了一个大型的图像字幕数据集，然后使用现成的文本到图像模型来生成与每个合成字幕相对应的多个图像。我们通过对比学习对这些合成图像进行视觉表征学习，将共享同一字幕的图像视为正对。由此产生的表示很好地转移到许多下游任务，在图像分类任务中与其他通用视觉表示学习器（如CLIP和DINO v2）竞争。此外，在语义分割等密集预测任务中，SynCLR显著优于以前的自监督方法，例如，在ViT-B/16的ADE20k上比MAE和iBOT提高了6.2和4.3mIoU。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17742v1" target="_blank">2312.17742v1</a>
                              </td>
                              <td>Learning Vision from Models Rivals Learning Vision from Data</td>
                              <td>Yonglong Tian</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17742v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17742v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/syn-rep-learn" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_02366v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_02366v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_02366v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_02366v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The integration of deep learning systems into healthcare has been hindered by the resource-intensive process of data annotation and the inability of these systems to generalize to different data distributions. Foundation models, which are models pre-trained on large datasets, have emerged as a solution to reduce reliance on annotated data and enhance model generalizability and robustness. DINOv2 is an open-source foundation model pre-trained with self-supervised learning on 142 million curated natural images that exhibits promising capabilities across various vision tasks. Nevertheless, a critical question remains unanswered regarding DINOv2's adaptability to radiological imaging, and whether its features are sufficiently general to benefit radiology image analysis. Therefore, this study comprehensively evaluates DINOv2 for radiology, conducting over 100 experiments across diverse modalities (X-ray, CT, and MRI). To measure the effectiveness and generalizability of DINOv2's feature representations, we analyze the model across medical image analysis tasks including disease classification and organ segmentation on both 2D and 3D images, and under different settings like kNN, few-shot learning, linear-probing, end-to-end fine-tuning, and parameter-efficient fine-tuning. Comparative analyses with established supervised, self-supervised, and weakly-supervised models reveal DINOv2's superior performance and cross-task generalizability. The findings contribute insights to potential avenues for optimizing pre-training strategies for medical imaging and enhancing the broader understanding of DINOv2's role in bridging the gap between natural and radiological image analysis. Our code is available at https://github.com/MohammedSB/DINOv2ForRadiology</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_02366v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>数据注释的资源密集型过程以及这些系统无法推广到不同的数据分布，阻碍了深度学习系统与医疗保健的集成。基础模型是在大型数据集上预先训练的模型，已成为减少对注释数据的依赖并增强模型可推广性和稳健性的解决方案。DINOv2是一个开源的基础模型，通过对1.42亿张策划的自然图像进行自我监督学习进行预训练，在各种视觉任务中表现出有希望的能力。然而，关于DINOv2对放射学成像的适应性，以及其特征是否足够通用以有利于放射学图像分析，一个关键问题仍未得到解答。因此，本研究全面评估了DINOv2的放射学，在不同的模式（X射线、CT和MRI）下进行了100多项实验。为了衡量DINOv2特征表示的有效性和可推广性，我们分析了医学图像分析任务中的模型，包括2D和3D图像上的疾病分类和器官分割，以及在不同的设置下，如kNN、少镜头学习、线性探测、端到端微调和参数有效微调。与已建立的监督、自监督和弱监督模型的比较分析揭示了DINOv2的优越性能和跨任务可推广性。这些发现有助于深入了解优化医学成像预训练策略的潜在途径，并增进对DINOv2在弥合自然图像分析和放射学图像分析之间差距方面的作用的更广泛理解。我们的代码可在https://github.com/MohammedSB/DINOv2ForRadiology</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.02366v3" target="_blank">2312.02366v3</a>
                              </td>
                              <td>Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks</td>
                              <td>Mohammed Baharoon</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_02366v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.02366v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/mohammedsb/dinov2forradiology" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_17116v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalizable Visual Reinforcement Learning with Segment Anything Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_17116v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_17116v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_17116v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning policies that can generalize to unseen environments is a fundamental challenge in visual reinforcement learning (RL). While most current methods focus on acquiring robust visual representations through auxiliary supervision, pre-training, or data augmentation, the potential of modern vision foundation models remains underleveraged. In this work, we introduce Segment Anything Model for Generalizable visual RL (SAM-G), a novel framework that leverages the promptable segmentation ability of Segment Anything Model (SAM) to enhance the generalization capabilities of visual RL agents. We utilize image features from DINOv2 and SAM to find correspondence as point prompts to SAM, and then SAM produces high-quality masked images for agents directly. Evaluated across 8 DMControl tasks and 3 Adroit tasks, SAM-G significantly improves the visual generalization ability without altering the RL agents' architecture but merely their observations. Notably, SAM-G achieves 44% and 29% relative improvements on the challenging video hard setting on DMControl and Adroit respectively, compared to state-of-the-art methods. Video and code: https://yanjieze.com/SAM-G/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_17116v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>能够推广到看不见的环境中的学习策略是视觉强化学习（RL）中的一个基本挑战。虽然目前的大多数方法都侧重于通过辅助监督、预训练或数据增强来获取稳健的视觉表示，但现代视觉基础模型的潜力仍然不足。在这项工作中，我们介绍了可泛化视觉RL的分段任意模型（SAM-G），这是一个新的框架，利用分段任意模型的可提示分割能力来增强视觉RL代理的泛化能力。我们利用DINOv2和SAM的图像特征来寻找与SAM的点提示对应关系，然后SAM直接为代理生成高质量的掩码图像。在8个DMControl任务和3个Adroit任务中进行评估后，SAM-G显著提高了视觉泛化能力，而不会改变RL代理的架构，而只是改变他们的观察结果。值得注意的是，与最先进的方法相比，SAM-G在DMControl和Adroit上具有挑战性的视频硬设置上分别实现了44%和29%的相对改进。视频和代码：https://yanjieze.com/SAM-G/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.17116v1" target="_blank">2312.17116v1</a>
                              </td>
                              <td>Generalizable Visual Reinforcement Learning with Segment Anything Model</td>
                              <td>Ziyu Wang</td>
                              <td>2023-12-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_17116v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.17116v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wadiuvatzy/sam-g" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16084v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">LangSplat: 3D Language Gaussian Splatting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16084v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16084v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16084v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Human lives in a 3D world and commonly uses natural language to interact with a 3D scene. Modeling a 3D language field to support open-ended language queries in 3D has gained increasing attention recently. This paper introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces. Unlike existing methods that ground CLIP language embeddings in a NeRF model, LangSplat advances the field by utilizing a collection of 3D Gaussians, each encoding language features distilled from CLIP, to represent the language field. By employing a tile-based splatting technique for rendering language features, we circumvent the costly rendering process inherent in NeRF. Instead of directly learning CLIP embeddings, LangSplat first trains a scene-wise language autoencoder and then learns language features on the scene-specific latent space, thereby alleviating substantial memory demands imposed by explicit modeling. Existing methods struggle with imprecise and vague 3D language fields, which fail to discern clear boundaries between objects. We delve into this issue and propose to learn hierarchical semantics using SAM, thereby eliminating the need for extensively querying the language field across various scales and the regularization of DINO features. Extensive experiments on open-vocabulary 3D object localization and semantic segmentation demonstrate that LangSplat significantly outperforms the previous state-of-the-art method LERF by a large margin. Notably, LangSplat is extremely efficient, achieving a {\speed} $\times$ speedup compared to LERF at the resolution of 1440 $\times$ 1080. We strongly recommend readers to check out our video results at https://langsplat.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16084v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人类生活在3D世界中，通常使用自然语言与3D场景进行交互。最近，对3D语言字段进行建模以支持3D中的开放式语言查询越来越受到关注。本文介绍了LangSplat，它构建了一个三维语言字段，可以在三维空间中进行精确高效的开放式词汇查询。与现有的将CLIP语言嵌入NeRF模型的方法不同，LangSplat通过利用3D高斯集合来表示语言领域，从而推进了这一领域的发展，每个高斯集合都对从CLIP中提取的语言特征进行编码。通过使用基于瓦片的飞溅技术来渲染语言特征，我们避免了NeRF中固有的昂贵的渲染过程。LangSplat不是直接学习CLIP嵌入，而是首先训练场景式语言自动编码器，然后在特定场景的潜在空间上学习语言特征，从而减轻显式建模带来的大量内存需求。现有的方法难以处理不精确和模糊的3D语言字段，这些字段无法辨别对象之间的清晰边界。我们深入研究了这个问题，并建议使用SAM学习分层语义，从而消除了在各种规模上广泛查询语言字段和DINO特征正则化的需要。在开放词汇三维对象定位和语义分割方面的大量实验表明，LangSplat显著优于先前最先进的方法LERF。值得注意的是，LangSplat非常高效，与分辨率为1440$\times$1080的LERF相比，它实现了｛\speed｝$\times$的加速。我们强烈建议读者在上查看我们的视频结果https://langsplat.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16084v1" target="_blank">2312.16084v1</a>
                              </td>
                              <td>LangSplat: 3D Language Gaussian Splatting</td>
                              <td>Minghan Qin</td>
                              <td>2023-12-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16084v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16084v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/minghanqin/LangSplat" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_06709v3_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AM-RADIO: Agglomerative Model -- Reduce All Domains Into One</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_06709v3_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_06709v3_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_06709v3_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>A handful of visual foundation models (VFMs) have recently emerged as the backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are trained with distinct objectives, exhibiting unique characteristics for various downstream tasks. We find that despite their conceptual differences, these models can be effectively merged into a unified model through multi-teacher distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All Domains Into One). This integrative approach not only surpasses the performance of individual teacher models but also amalgamates their distinctive features, such as zero-shot vision-language comprehension, detailed pixel-level understanding, and open vocabulary segmentation capabilities. In pursuit of the most hardware-efficient backbone, we evaluated numerous architectures in our multi-teacher distillation pipeline using the same training recipe. This led to the development of a novel architecture (E-RADIO) that exceeds the performance of its predecessors and is at least 7x faster than the teacher models. Our comprehensive benchmarking process covers downstream tasks including ImageNet classification, ADE20k semantic segmentation, COCO object detection and LLaVa-1.5 framework.   Code: https://github.com/NVlabs/RADIO</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_06709v3_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近出现了一些视觉基础模型（VFM）作为许多下游任务的骨干。像CLIP、DINOv2、SAM这样的VFM都是以不同的目标进行训练的，在各种下游任务中表现出独特的特征。我们发现，尽管这些模型在概念上存在差异，但通过多教师提炼，它们可以有效地合并为一个统一的模型。我们将这种方法命名为AM-RADIO（聚集模型——将所有域归一）。这种综合方法不仅超越了个别教师模型的性能，而且融合了它们的独特特征，如零样本视觉语言理解、详细的像素级理解和开放的词汇分割能力。为了追求硬件效率最高的主干，我们使用相同的培训方法评估了多教师蒸馏管道中的许多架构。这导致了一种新型架构（E-RADIO）的开发，其性能超过了其前身，并且至少比教师模型快7倍。我们的全面基准测试流程涵盖下游任务，包括ImageNet分类、ADE20k语义分割、COCO对象检测和LLaVa-1.5框架。代码：https://github.com/NVlabs/RADIO</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.06709v3" target="_blank">2312.06709v3</a>
                              </td>
                              <td>AM-RADIO: Agglomerative Model -- Reduce All Domains Into One</td>
                              <td>Mike Ranzinger</td>
                              <td>2023-12-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_06709v3_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.06709v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/nvlabs/radio" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16211v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16211v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16211v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16211v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Causal networks are widely used in many fields, including epidemiology, social science, medicine, and engineering, to model the complex relationships between variables. While it can be convenient to algorithmically infer these models directly from observational data, the resulting networks are often plagued with erroneous edges. Auditing and correcting these networks may require domain expertise frequently unavailable to the analyst. We propose the use of large language models such as ChatGPT as an auditor for causal networks. Our method presents ChatGPT with a causal network, one edge at a time, to produce insights about edge directionality, possible confounders, and mediating variables. We ask ChatGPT to reflect on various aspects of each causal link and we then produce visualizations that summarize these viewpoints for the human analyst to direct the edge, gather more data, or test further hypotheses. We envision a system where large language models, automated causal inference, and the human analyst and domain expert work hand in hand as a team to derive holistic and comprehensive causal models for any given case scenario. This paper presents first results obtained with an emerging prototype.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16211v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>因果网络被广泛应用于许多领域，包括流行病学、社会科学、医学和工程，以对变量之间的复杂关系进行建模。虽然直接从观测数据中用算法推断这些模型很方便，但由此产生的网络往往存在错误边缘。审核和纠正这些网络可能需要分析员经常无法获得的领域专业知识。我们建议使用大型语言模型（如ChatGPT）作为因果网络的审计员。我们的方法为ChatGPT提供了一个因果网络，一次一个边缘，以产生关于边缘方向性、可能的混杂因素和中介变量的见解。我们要求ChatGPT反思每个因果关系的各个方面，然后我们生成可视化结果，总结这些观点，供人类分析师指导边缘、收集更多数据或测试进一步的假设。我们设想一个系统，在该系统中，大型语言模型、自动因果推理、人类分析师和领域专家作为一个团队携手合作，为任何给定的案例场景推导出整体和全面的因果模型。本文介绍了使用新兴原型获得的第一个结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16211v1" target="_blank">2312.16211v1</a>
                              </td>
                              <td>An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development</td>
                              <td>Yanming Zhang</td>
                              <td>2023-12-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16211v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16211v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_14810v1_5">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Accelerating Bayesian Optimal Experimental Design with Derivative-Informed Neural Operators</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_14810v1_5_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_14810v1_5_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_14810v1_5_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We consider optimal experimental design (OED) for nonlinear Bayesian inverse problems governed by large-scale partial differential equations (PDEs). For the optimality criteria of Bayesian OED, we consider both expected information gain and summary statistics including the trace and determinant of the information matrix that involves the evaluation of the parameter-to-observable (PtO) map and its derivatives. However, it is prohibitive to compute and optimize these criteria when the PDEs are very expensive to solve, the parameters to estimate are high-dimensional, and the optimization problem is combinatorial, high-dimensional, and non-convex. To address these challenges, we develop an accurate, scalable, and efficient computational framework to accelerate the solution of Bayesian OED. In particular, the framework is developed based on derivative-informed neural operator (DINO) surrogates with proper dimension reduction techniques and a modified swapping greedy algorithm. We demonstrate the high accuracy of the DINO surrogates in the computation of the PtO map and the optimality criteria compared to high-fidelity finite element approximations. We also show that the proposed method is scalable with increasing parameter dimensions. Moreover, we demonstrate that it achieves high efficiency with over 1000X speedup compared to a high-fidelity Bayesian OED solution for a three-dimensional PDE example with tens of thousands of parameters, including both online evaluation and offline construction costs of the surrogates.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_14810v1_5_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们考虑由大规模偏微分方程（PDE）控制的非线性贝叶斯反问题的最优实验设计（OED）。对于贝叶斯OED的最优性标准，我们考虑了预期信息增益和汇总统计，包括信息矩阵的迹和行列式，该信息矩阵涉及对参数-可观测（PtO）图及其导数的评估。然而，当偏微分方程的求解非常昂贵，要估计的参数是高维的，并且优化问题是组合的、高维的和非凸的时，计算和优化这些准则是禁止的。为了应对这些挑战，我们开发了一个准确、可扩展和高效的计算框架来加速贝叶斯OED的解决方案。特别是，该框架是基于导数知情神经算子（DINO）代理，采用适当的降维技术和改进的交换贪婪算法开发的。与高保真有限元近似相比，我们证明了DINO替代物在PtO映射计算中的高精度和最优性标准。我们还证明了所提出的方法随着参数维数的增加是可扩展的。此外，我们证明，对于具有数万个参数的三维PDE示例，与高保真度贝叶斯OED解决方案相比，它实现了超过1000倍的加速，包括代理的在线评估和离线构建成本。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.14810v1" target="_blank">2312.14810v1</a>
                              </td>
                              <td>Accelerating Bayesian Optimal Experimental Design with Derivative-Informed Neural Operators</td>
                              <td>Jinwoo Go</td>
                              <td>2023-12-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_14810v1_5"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.14810v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="MAEVAD"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_13217v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VideoPrism: A Foundational Visual Encoder for Video Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13217v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13217v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13217v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13217v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍VideoPrism，这是一种通用视频编码器，可通过单个冻结模型处理各种视频理解任务。我们在异构语料库上预训练VideoPrism，该语料库包含36M个高质量视频字幕对和582M个具有噪声并行文本的视频片段（例如ASR转录本）。预训练方法通过语义视频嵌入的全局局部提取和令牌混洗方案改进了掩蔽自动编码，使VideoPrism能够主要关注视频模态，同时利用与视频相关的宝贵文本。我们在四大类视频理解任务上广泛测试了VideoPrism，从网络视频问答到科学简历，在33个视频理解基准中的30个上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13217v1" target="_blank">2402.13217v1</a>
                              </td>
                              <td>VideoPrism: A Foundational Visual Encoder for Video Understanding</td>
                              <td>Long Zhao</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13217v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13217v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11337v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning by Reconstruction Produces Uninformative Features For Perception</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11337v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11337v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11337v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Input space reconstruction is an attractive representation learning paradigm. Despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90\% of the pixel variance can be solved with 45\% test accuracy. Using the bottom subspace instead, accounting for only 20\% of the pixel variance, reaches 55\% test accuracy. The features for perception being learned last explains the need for long training time, e.g., with Masked Autoencoders. Learning by denoising is a popular strategy to alleviate that misalignment. We prove that while some noise strategies such as masking are indeed beneficial, others such as additive Gaussian noise are not. Yet, even in the case of masking, we find that the benefits vary as a function of the mask's shape, ratio, and the considered dataset. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide first clues on how to detect if a noise strategy is never beneficial regardless of the perception task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11337v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>输入空间重构是一种极具吸引力的表征学习范式。尽管重建和生成是可解释的，但我们发现了通过重建学习和感知学习之间的错位。我们证明，前者将模型的容量分配给解释观测方差的数据子空间——后者的子空间具有无信息特征。例如，将图像投影到顶部子空间上解释90%像素方差的监督TinyImagenet任务可以以45%的测试精度解决。使用底部子空间，只占像素方差的20%，达到55%的测试精度。正在学习的感知功能最后解释了对长训练时间的需求，例如，使用掩蔽自动编码器。通过去噪学习是一种流行的策略来缓解这种错位。我们证明，虽然一些噪声策略（如掩蔽）确实是有益的，但其他策略（如加性高斯噪声）则不然。然而，即使在掩蔽的情况下，我们也发现其益处随着掩蔽的形状、比率和所考虑的数据集的变化而变化。虽然在不了解感知任务的情况下调整噪声策略似乎很有挑战性，但我们提供了关于如何检测噪声策略是否无论感知任务如何都是有益的第一条线索。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11337v1" target="_blank">2402.11337v1</a>
                              </td>
                              <td>Learning by Reconstruction Produces Uninformative Features For Perception</td>
                              <td>Randall Balestriero</td>
                              <td>2024-02-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11337v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11337v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_09797v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_09797v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_09797v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_09797v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we propose a novel cross-talk rejection framework for a multi-channel multi-talker setup for a live multiparty interactive show. Our far-field audio setup is required to be hands-free during live interaction and comprises four adjacent talkers with directional microphones in the same space. Such setups often introduce heavy cross-talk between channels, resulting in reduced automatic speech recognition (ASR) and natural language understanding (NLU) performance. To address this problem, we propose voice activity detection (VAD) model for all talkers using multichannel information, which is then used to filter audio for downstream tasks. We adopt a synthetic training data generation approach through playback and re-recording for such scenarios, simulating challenging speech overlap conditions. We train our models on this synthetic data and demonstrate that our approach outperforms single-channel VAD models and energy-based multi-channel VAD algorithm in various acoustic environments. In addition to VAD results, we also present multiparty ASR evaluation results to highlight the impact of using our VAD model for filtering audio in downstream tasks by significantly reducing the insertion error.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_09797v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们提出了一种新的串扰抑制框架，用于现场多方互动节目的多通道多扬声器设置。我们的远场音频设置要求在现场互动过程中免提，并包括四个相邻的扬声器和位于同一空间的定向麦克风。这种设置通常会在通道之间引入严重的串扰，导致自动语音识别（ASR）和自然语言理解（NLU）性能降低。为了解决这个问题，我们提出了使用多通道信息的所有说话者的语音活动检测（VAD）模型，然后将其用于过滤下游任务的音频。我们采用了一种合成训练数据生成方法，通过回放和重新记录这些场景，模拟具有挑战性的语音重叠条件。我们根据这些合成数据训练我们的模型，并证明我们的方法在各种声学环境中优于单通道VAD模型和基于能量的多通道VAD算法。除了VAD结果外，我们还提供了多方ASR评估结果，以突出使用我们的VAD模型通过显著减少插入误差来过滤下游任务中的音频的影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.09797v1" target="_blank">2402.09797v1</a>
                              </td>
                              <td>A cross-talk robust multichannel VAD model for multiparty agent interactions trained using synthetic re-recordings</td>
                              <td>Hyewon Han</td>
                              <td>2024-02-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_09797v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.09797v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08035v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multiple Random Masking Autoencoder Ensembles for Robust Multimodal Semi-supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08035v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08035v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08035v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>There is an increasing number of real-world problems in computer vision and machine learning requiring to take into consideration multiple interpretation layers (modalities or views) of the world and learn how they relate to each other. For example, in the case of Earth Observations from satellite data, it is important to be able to predict one observation layer (e.g. vegetation index) from other layers (e.g. water vapor, snow cover, temperature etc), in order to best understand how the Earth System functions and also be able to reliably predict information for one layer when the data is missing (e.g. due to measurement failure or error).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08035v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在计算机视觉和机器学习中，越来越多的现实世界问题需要考虑世界的多个解释层（模态或视图），并了解它们之间的关系。例如，在根据卫星数据进行地球观测的情况下，能够从其他层（如水蒸气、积雪、温度等）预测一个观测层（如植被指数）是很重要的，以便最好地理解地球系统是如何工作的，并且当数据丢失（例如由于测量故障或误差）时也能够可靠地预测一层的信息。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08035v1" target="_blank">2402.08035v1</a>
                              </td>
                              <td>Multiple Random Masking Autoencoder Ensembles for Robust Multimodal Semi-supervised Learning</td>
                              <td>Alexandru-Raul Todoran</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08035v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08035v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08023v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UGMAE: A Unified Framework for Graph Masked Autoencoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08023v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08023v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08023v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generative self-supervised learning on graphs, particularly graph masked autoencoders, has emerged as a popular learning paradigm and demonstrated its efficacy in handling non-Euclidean data. However, several remaining issues limit the capability of existing methods: 1) the disregard of uneven node significance in masking, 2) the underutilization of holistic graph information, 3) the ignorance of semantic knowledge in the representation space due to the exclusive use of reconstruction loss in the output space, and 4) the unstable reconstructions caused by the large volume of masked contents. In light of this, we propose UGMAE, a unified framework for graph masked autoencoders to address these issues from the perspectives of adaptivity, integrity, complementarity, and consistency. Specifically, we first develop an adaptive feature mask generator to account for the unique significance of nodes and sample informative masks (adaptivity). We then design a ranking-based structure reconstruction objective joint with feature reconstruction to capture holistic graph information and emphasize the topological proximity between neighbors (integrity). After that, we present a bootstrapping-based similarity module to encode the high-level semantic knowledge in the representation space, complementary to the low-level reconstruction in the output space (complementarity). Finally, we build a consistency assurance module to provide reconstruction objectives with extra stabilized consistency targets (consistency). Extensive experiments demonstrate that UGMAE outperforms both contrastive and generative state-of-the-art baselines on several tasks across multiple datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08023v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图上的生成自监督学习，特别是图屏蔽自动编码器，已经成为一种流行的学习范式，并证明了其在处理非欧几里得数据方面的有效性。然而，剩下的几个问题限制了现有方法的能力：1）在掩蔽中忽略了不均匀的节点显著性，2）整体图信息的利用不足，3）由于在输出空间中独占使用重构损失而忽略了表示空间中的语义知识，以及4）大量掩蔽内容导致的不稳定重构。有鉴于此，我们提出了UGMAE，这是一个用于图屏蔽自动编码器的统一框架，从自适应、完整性、互补性和一致性的角度来解决这些问题。具体来说，我们首先开发了一个自适应特征掩码生成器，以考虑节点和样本信息掩码的独特意义（自适应性）。然后，我们设计了一个基于排序的结构重建目标与特征重建相结合，以捕获整体图信息，并强调邻居之间的拓扑接近性（完整性）。之后，我们提出了一个基于自举的相似性模块来对表示空间中的高级语义知识进行编码，与输出空间中的低级重构（互补性）互补。最后，我们构建了一个一致性保证模块，为重建目标提供额外稳定的一致性目标（一致性）。大量实验表明，UGMAE在多个数据集的几个任务上都优于对比和生成最先进的基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08023v1" target="_blank">2402.08023v1</a>
                              </td>
                              <td>UGMAE: A Unified Framework for Graph Masked Autoencoders</td>
                              <td>Yijun Tian</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08023v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08023v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07370v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07370v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07370v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07370v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Face swapping has gained significant attention for its varied applications. The majority of previous face swapping approaches have relied on the seesaw game training scheme, which often leads to the instability of the model training and results in undesired samples with blended identities due to the target identity leakage problem. This paper introduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach designed to enhance face swapping model training. Our training scheme addresses the limitations of traditional training methods by circumventing the conventional seesaw game and introducing clear ground truth through its self-reconstruction training regime. It effectively mitigates identity leakage by masking facial regions of the input images and utilizing learned disentangled identity and non-identity features. Additionally, we tackle the shape misalignment problem with new techniques including perforation confusion and random mesh scaling, and establishes a new state-of-the-art, surpassing other baseline methods, preserving both identity and non-identity attributes, without sacrificing on either aspect.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07370v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人脸交换因其应用的多样性而备受关注。以前的大多数人脸交换方法都依赖于跷跷板游戏训练方案，这往往导致模型训练的不稳定性，并由于目标身份泄漏问题而导致混合身份的不期望样本。本文介绍了形状不可知掩模自动编码器（SAMAE）训练方案，这是一种新的自监督方法，旨在增强人脸交换模型的训练。我们的训练方案解决了传统训练方法的局限性，绕过了传统的拉锯游戏，并通过其自我重建训练机制引入了明确的基本事实。它通过掩蔽输入图像的面部区域并利用学习到的解纠缠的身份和非身份特征，有效地缓解了身份泄漏。此外，我们使用新技术解决了形状错位问题，包括穿孔混淆和随机网格缩放，并建立了一种新的最先进技术，超越了其他基线方法，在不牺牲任何一个方面的情况下保留了同一性和非同一性属性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07370v1" target="_blank">2402.07370v1</a>
                              </td>
                              <td>SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder</td>
                              <td>Jaeseong Lee</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07370v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07370v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07225v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rethinking Graph Masked Autoencoders through Alignment and Uniformity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07225v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07225v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07225v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning on graphs can be bifurcated into contrastive and generative methods. Contrastive methods, also known as graph contrastive learning (GCL), have dominated graph self-supervised learning in the past few years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles the momentum behind generative methods. Despite the empirical success of GraphMAE, there is still a dearth of theoretical understanding regarding its efficacy. Moreover, while both generative and contrastive methods have been shown to be effective, their connections and differences have yet to be thoroughly investigated. Therefore, we theoretically build a bridge between GraphMAE and GCL, and prove that the node-level reconstruction objective in GraphMAE implicitly performs context-level GCL. Based on our theoretical analysis, we further identify the limitations of the GraphMAE from the perspectives of alignment and uniformity, which have been considered as two key properties of high-quality representations in GCL. We point out that GraphMAE's alignment performance is restricted by the masking strategy, and the uniformity is not strictly guaranteed. To remedy the aforementioned limitations, we propose an Alignment-Uniformity enhanced Graph Masked AutoEncoder, named AUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy to provide hard-to-align samples, which improves the alignment performance. Meanwhile, we introduce an explicit uniformity regularizer to ensure the uniformity of the learned representations. Experimental results on benchmark datasets demonstrate the superiority of our model over existing state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07225v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图上的自监督学习可以分为对比和生成两种方法。对比方法，也称为图对比学习（GCL），在过去几年中主导了图自监督学习，但最近出现的图屏蔽自动编码器（GraphMAE）重新点燃了生成方法背后的动力。尽管GraphMAE在经验上取得了成功，但对其功效仍缺乏理论理解。此外，尽管生成方法和对比方法都被证明是有效的，但它们之间的联系和差异还有待深入研究。因此，我们在理论上搭建了GraphMAE和GCL之间的桥梁，并证明了GraphMAE中的节点级重构目标隐含地执行上下文级GCL。基于我们的理论分析，我们从对齐和一致性的角度进一步确定了GraphMAE的局限性，这被认为是GCL中高质量表示的两个关键特性。我们指出，GraphMAE的对准性能受到掩蔽策略的限制，并且不能严格保证一致性。为了弥补上述限制，我们提出了一种增强对齐一致性的图掩码自动编码器，名为AUG-MAE。具体来说，我们提出了一种易到难的对抗性掩蔽策略来提供难以对准的样本，这提高了对准性能。同时，我们引入了一个显式一致性正则化子来确保学习表示的一致性。在基准数据集上的实验结果表明，我们的模型优于现有的最先进的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07225v1" target="_blank">2402.07225v1</a>
                              </td>
                              <td>Rethinking Graph Masked Autoencoders through Alignment and Uniformity</td>
                              <td>Liang Wang</td>
                              <td>2024-02-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07225v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07225v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/azureleon1/aug-mae" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_06986v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cacophony: An Improved Contrastive Audio-Text Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_06986v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_06986v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_06986v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite recent improvements in audio-text modeling, audio-text contrastive models still lag behind their image-text counterparts in scale and performance. We propose a method to improve both the scale and the training of audio-text contrastive models. Specifically, we craft a large-scale audio-text dataset consisting of over 13,000 hours of text-labeled audio, aided by large language model (LLM) processing and audio captioning. Further, we employ an masked autoencoder (MAE) pre-pretraining phase with random patch dropout, which allows us to both scale unlabeled audio datasets and train efficiently with variable length audio. After MAE pre-pretraining of our audio encoder, we train a contrastive model with an auxiliary captioning objective. Our final model, which we name Cacophony, achieves state-of-the-art performance on audio-text retrieval tasks, and exhibits competitive results on other downstream tasks such as zero-shot classification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_06986v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管最近在音频文本建模方面有所改进，但音频文本对比模型在规模和性能上仍落后于图像文本对比模型。我们提出了一种提高音频文本对比模型的规模和训练的方法。具体来说，我们在大语言模型（LLM）处理和音频字幕的帮助下，制作了一个由13000多小时的文本标记音频组成的大规模音频文本数据集。此外，我们采用了具有随机补丁丢弃的掩蔽自动编码器（MAE）预预训练阶段，这使我们能够缩放未标记的音频数据集，并使用可变长度的音频进行有效训练。在对我们的音频编码器进行MAE预预训练后，我们训练了一个具有辅助字幕目标的对比模型。我们的最终模型，我们将其命名为Cacophony，在音频文本检索任务上实现了最先进的性能，并在其他下游任务（如零样本分类）上显示了具有竞争力的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.06986v1" target="_blank">2402.06986v1</a>
                              </td>
                              <td>Cacophony: An Improved Contrastive Audio-Text Model</td>
                              <td>Ge Zhu</td>
                              <td>2024-02-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_06986v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.06986v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/gzhu06/cacophony" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06881v2_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unmasking Deepfakes: Masked Autoencoding Spatiotemporal Transformers for Enhanced Video Forgery Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06881v2_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06881v2_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06881v2_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel approach for the detection of deepfake videos using a pair of vision transformers pre-trained by a self-supervised masked autoencoding setup. Our method consists of two distinct components, one of which focuses on learning spatial information from individual RGB frames of the video, while the other learns temporal consistency information from optical flow fields generated from consecutive frames. Unlike most approaches where pre-training is performed on a generic large corpus of images, we show that by pre-training on smaller face-related datasets, namely Celeb-A (for the spatial learning component) and YouTube Faces (for the temporal learning component), strong results can be obtained. We perform various experiments to evaluate the performance of our method on commonly used datasets namely FaceForensics++ (Low Quality and High Quality, along with a new highly compressed version named Very Low Quality) and Celeb-DFv2 datasets. Our experiments show that our method sets a new state-of-the-art on FaceForensics++ (LQ, HQ, and VLQ), and obtains competitive results on Celeb-DFv2. Moreover, our method outperforms other methods in the area in a cross-dataset setup where we fine-tune our model on FaceForensics++ and test on CelebDFv2, pointing to its strong cross-dataset generalization ability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06881v2_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的检测深度伪造视频的方法，使用一对由自监督掩蔽自动编码装置预训练的视觉转换器。我们的方法由两个不同的组件组成，其中一个专注于从视频的各个RGB帧中学习空间信息，而另一个则从连续帧生成的光流场中学习时间一致性信息。与大多数方法不同的是，预训练是在通用的大型图像语料库上进行的，我们表明，通过在较小的人脸相关数据集上进行预训练，即Celeb-a（用于空间学习组件）和YouTube人脸（用于时间学习组件），可以获得很强的结果。我们进行了各种实验，以评估我们的方法在常用数据集上的性能，即FaceForensics++（低质量和高质量，以及名为Very Low Quality的新的高度压缩版本）和Celeb-DFv2数据集。我们的实验表明，我们的方法在FaceForensics++（LQ、HQ和VLQ）上建立了新的最先进的技术，并在Celeb-DFv2上获得了有竞争力的结果。此外，在跨数据集设置中，我们的方法优于该领域的其他方法，我们在FaceForensics++上微调了我们的模型，并在CelebDFv2上进行了测试，这表明它具有强大的跨数据集泛化能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06881v2" target="_blank">2306.06881v2</a>
                              </td>
                              <td>Unmasking Deepfakes: Masked Autoencoding Spatiotemporal Transformers for Enhanced Video Forgery Detection</td>
                              <td>Sayantan Das</td>
                              <td>2023-06-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06881v2_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06881v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_05382v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_05382v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_05382v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_05382v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\% on average. It also obtains new state-of-the-art self-supervised learning results on detection and segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_05382v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩蔽自动编码器（MAE）是一种流行的自监督学习方法，在模型预训练中取得了良好的效果。然而，当各种下游任务的数据分布与预训练数据不同时，语义无关的预训练信息可能会导致负迁移，阻碍MAE的可扩展性。为了解决这个问题，我们提出了一种新的基于MAE的预训练范式，即集群条件专家混合（MoCE），它可以训练一次，但为不同的下游任务提供定制的预训练模型。与专家混合（MoE）不同，我们的MoCE通过使用聚类条件门仅用语义相关的图像来训练每个专家。因此，每个下游任务可以被分配到其定制的模型，该模型用与下游数据最相似的数据预训练。在11个下游任务的集合上进行的实验表明，MoCE平均比香草MAE高2.45%。它还在检测和分割方面获得了最先进的自监督学习结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.05382v1" target="_blank">2402.05382v1</a>
                              </td>
                              <td>Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts</td>
                              <td>Zhili Liu</td>
                              <td>2024-02-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_05382v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.05382v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_17152v3_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mixed Autoencoder for Self-supervised Visual Representation Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_17152v3_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_17152v3_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_17152v3_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on different downstream tasks with significant efficiency. Specifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9 AP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base. Moreover, MixedAE surpasses iBOT, a strong MIM method combined with instance discrimination, while accelerating training by 2x. To our best knowledge, this is the very first work to consider mixing for MIM from the perspective of pretext task design. Code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_17152v3_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩模自动编码器（MAE）通过随机掩模图像块和重建，在各种视觉任务中表现出卓越的性能。然而，与作为最重要部分的对比学习不同，MAE的有效数据扩充策略仍然是悬而未决的问题。本文研究了MAE的主要混合增强。我们首先证明，由于互信息（MI）的增加，天真混合将使模型性能退化。为了解决这个问题，我们提出了同源识别，这是一种辅助借口任务，不仅通过明确要求每个补丁识别同源补丁来缓解MI的增加，而且还可以执行对象感知自监督预训练，以获得更好的下游密集感知性能。通过广泛的实验，我们证明了我们提出的混合自动编码器（MixedAE）在不同下游任务的掩模图像建模（MIM）增强之间以显著的效率实现了最先进的传输结果。具体而言，我们的MixedAE在ImageNet-1K、ADE20K和COCO上的准确率分别比MAE高出+0.3%、+1.7mIoU和+0.9AP。此外，MixedAE超越了iBOT，这是一种结合实例判别的强大MIM方法，同时将训练速度提高了2倍。据我们所知，这是第一个从借口任务设计的角度考虑MIM混合的工作。将提供代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.17152v3" target="_blank">2303.17152v3</a>
                              </td>
                              <td>Mixed Autoencoder for Self-supervised Visual Representation Learning</td>
                              <td>Kai Chen</td>
                              <td>2023-03-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_17152v3_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.17152v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02088v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DCS-Net: Pioneering Leakage-Free Point Cloud Pretraining Framework with Global Insights</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02088v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02088v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02088v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked autoencoding and generative pretraining have achieved remarkable success in computer vision and natural language processing, and more recently, they have been extended to the point cloud domain. Nevertheless, existing point cloud models suffer from the issue of information leakage due to the pre-sampling of center points, which leads to trivial proxy tasks for the models. These approaches primarily focus on local feature reconstruction, limiting their ability to capture global patterns within point clouds. In this paper, we argue that the reduced difficulty of pretext tasks hampers the model's capacity to learn expressive representations. To address these limitations, we introduce a novel solution called the Differentiable Center Sampling Network (DCS-Net). It tackles the information leakage problem by incorporating both global feature reconstruction and local feature reconstruction as non-trivial proxy tasks, enabling simultaneous learning of both the global and local patterns within point cloud. Experimental results demonstrate that our method enhances the expressive capacity of existing point cloud models and effectively addresses the issue of information leakage.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02088v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩蔽自动编码和生成预训练在计算机视觉和自然语言处理领域取得了显著的成功，最近还扩展到了点云领域。然而，由于中心点的预采样，现有的点云模型存在信息泄漏问题，这导致模型的代理任务微不足道。这些方法主要侧重于局部特征重建，限制了它们在点云中捕获全局模式的能力。在本文中，我们认为借口任务的难度降低阻碍了模型学习表达表征的能力。为了解决这些限制，我们引入了一种新的解决方案，称为可微分中心采样网络（DCS-Net）。它通过将全局特征重构和局部特征重构作为非平凡的代理任务来解决信息泄漏问题，从而能够同时学习点云中的全局和局部模式。实验结果表明，该方法提高了现有点云模型的表达能力，有效地解决了信息泄漏问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02088v1" target="_blank">2402.02088v1</a>
                              </td>
                              <td>DCS-Net: Pioneering Leakage-Free Point Cloud Pretraining Framework with Global Insights</td>
                              <td>Zhe Li</td>
                              <td>2024-02-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02088v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02088v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_05087v3_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_05087v3_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_05087v3_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_05087v3_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Anomaly Detection (VAD) serves as a pivotal technology in the intelligent surveillance systems, enabling the temporal or spatial identification of anomalous events within videos. While existing reviews predominantly concentrate on conventional unsupervised methods, they often overlook the emergence of weakly-supervised and fully-unsupervised approaches. To address this gap, this survey extends the conventional scope of VAD beyond unsupervised methods, encompassing a broader spectrum termed Generalized Video Anomaly Event Detection (GVAED). By skillfully incorporating recent advancements rooted in diverse assumptions and learning frameworks, this survey introduces an intuitive taxonomy that seamlessly navigates through unsupervised, weakly-supervised, supervised and fully-unsupervised VAD methodologies, elucidating the distinctions and interconnections within these research trajectories. In addition, this survey facilitates prospective researchers by assembling a compilation of research resources, including public datasets, available codebases, programming tools, and pertinent literature. Furthermore, this survey quantitatively assesses model performance, delves into research challenges and directions, and outlines potential avenues for future exploration.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_05087v3_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）是智能监控系统中的一项关键技术，能够在时间或空间上识别视频中的异常事件。虽然现有的综述主要集中在传统的无监督方法上，但它们往往忽视了弱监督和完全无监督方法的出现。为了解决这一差距，这项调查将VAD的传统范围扩展到了无监督方法之外，涵盖了更广泛的范围，称为广义视频异常事件检测（GVAED）。通过巧妙地结合植根于不同假设和学习框架的最新进展，这项调查引入了一种直观的分类法，该分类法可以无缝地浏览无监督、弱监督、有监督和完全无监督的VAD方法，阐明了这些研究轨迹中的区别和相互联系。此外，这项调查通过汇编研究资源，包括公共数据集、可用代码库、编程工具和相关文献，为未来的研究人员提供了便利。此外，这项调查定量评估了模型性能，深入研究了研究挑战和方向，并概述了未来探索的潜在途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.05087v3" target="_blank">2302.05087v3</a>
                              </td>
                              <td>Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models</td>
                              <td>Yang Liu</td>
                              <td>2023-02-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_05087v3_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.05087v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fudanyliu/gvaed" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17497v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Visual Syntactical Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17497v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17497v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17497v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Syntax is usually studied in the realm of linguistics and refers to the arrangement of words in a sentence. Similarly, an image can be considered as a visual 'sentence', with the semantic parts of the image acting as 'words'. While visual syntactic understanding occurs naturally to humans, it is interesting to explore whether deep neural networks (DNNs) are equipped with such reasoning. To that end, we alter the syntax of natural images (e.g. swapping the eye and nose of a face), referred to as 'incorrect' images, to investigate the sensitivity of DNNs to such syntactic anomaly. Through our experiments, we discover an intriguing property of DNNs where we observe that state-of-the-art convolutional neural networks, as well as vision transformers, fail to discriminate between syntactically correct and incorrect images when trained on only correct ones. To counter this issue and enable visual syntactic understanding with DNNs, we propose a three-stage framework- (i) the 'words' (or the sub-features) in the image are detected, (ii) the detected words are sequentially masked and reconstructed using an autoencoder, (iii) the original and reconstructed parts are compared at each location to determine syntactic correctness. The reconstruction module is trained with BERT-like masked autoencoding for images, with the motivation to leverage language model inspired training to better capture the syntax. Note, our proposed approach is unsupervised in the sense that the incorrect images are only used during testing and the correct versus incorrect labels are never used for training. We perform experiments on CelebA, and AFHQ datasets and obtain classification accuracy of 92.10%, and 90.89%, respectively. Notably, the approach generalizes well to ImageNet samples which share common classes with CelebA and AFHQ without explicitly training on them.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17497v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语法通常在语言学领域研究，指的是句子中单词的排列。同样，图像可以被视为视觉“句子”，图像的语义部分充当“单词”。虽然视觉句法理解是人类自然产生的，但探索深度神经网络（DNN）是否具备这种推理是很有趣的。为此，我们改变了被称为“不正确”图像的自然图像的语法（例如，交换人脸的眼睛和鼻子），以研究DNN对这种语法异常的敏感性。通过我们的实验，我们发现了DNN的一个有趣的特性，即我们观察到最先进的卷积神经网络以及视觉转换器，当只在正确的图像上训练时，无法区分语法正确和不正确的图像。为了解决这个问题并实现DNN的视觉句法理解，我们提出了一个三阶段框架——（i）检测图像中的“单词”（或子特征），（ii）使用自动编码器顺序屏蔽和重建检测到的单词，（iii）在每个位置比较原始部分和重建部分，以确定句法正确性。重建模块使用类似BERT的图像掩蔽自动编码进行训练，目的是利用语言模型启发的训练来更好地捕捉语法。注意，我们提出的方法是无监督的，因为不正确的图像只在测试期间使用，而正确与不正确的标签从不用于训练。我们在CelebA和AFHQ数据集上进行了实验，分别获得了92.10%和90.89%的分类准确率。值得注意的是，该方法很好地推广到与CelebA和AFHQ共享公共类的ImageNet样本，而无需对它们进行明确的训练。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17497v1" target="_blank">2401.17497v1</a>
                              </td>
                              <td>Towards Visual Syntactical Understanding</td>
                              <td>Sayeed Shafayet Chowdhury</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17497v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17497v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_16402v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_16402v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_16402v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_16402v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the concept of normality in visual data, widely applied across diverse domains, e.g., industrial defect inspection, and medical lesion detection. This survey comprehensively examines recent advancements in VAD by identifying three primary challenges: 1) scarcity of training data, 2) diversity of visual modalities, and 3) complexity of hierarchical anomalies. Starting with a brief overview of the VAD background and its generic concept definitions, we progressively categorize, emphasize, and discuss the latest VAD progress from the perspective of sample number, data modality, and anomaly hierarchy. Through an in-depth analysis of the VAD field, we finally summarize future developments for VAD and conclude the key findings and contributions of this survey.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_16402v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉异常检测（VAD）致力于找出视觉数据中与正态性概念的偏差，广泛应用于不同领域，例如工业缺陷检测和医疗损伤检测。这项调查通过确定三个主要挑战，全面审查了VAD的最新进展：1）训练数据的稀缺性，2）视觉模式的多样性，以及3）层次异常的复杂性。从对VAD背景及其一般概念定义的简要概述开始，我们从样本数量、数据形态和异常层次的角度对VAD的最新进展进行了逐步分类、强调和讨论。通过对VAD领域的深入分析，我们最终总结了VAD的未来发展，并总结了本次调查的主要发现和贡献。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.16402v1" target="_blank">2401.16402v1</a>
                              </td>
                              <td>A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect</td>
                              <td>Yunkang Cao</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_16402v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.16402v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15900v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MV2MAE: Multi-View Video Masked Autoencoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15900v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15900v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15900v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Videos captured from multiple viewpoints can help in perceiving the 3D structure of the world and benefit computer vision tasks such as action recognition, tracking, etc. In this paper, we present a method for self-supervised learning from synchronized multi-view videos. We use a cross-view reconstruction task to inject geometry information in the model. Our approach is based on the masked autoencoder (MAE) framework. In addition to the same-view decoder, we introduce a separate cross-view decoder which leverages cross-attention mechanism to reconstruct a target viewpoint video using a video from source viewpoint, to help representations robust to viewpoint changes. For videos, static regions can be reconstructed trivially which hinders learning meaningful representations. To tackle this, we introduce a motion-weighted reconstruction loss which improves temporal modeling. We report state-of-the-art results on the NTU-60, NTU-120 and ETRI datasets, as well as in the transfer learning setting on NUCLA, PKU-MMD-II and ROCOG-v2 datasets, demonstrating the robustness of our approach. Code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15900v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从多个视点捕获的视频可以帮助感知世界的3D结构，并有利于计算机视觉任务，如动作识别、跟踪等。在本文中，我们提出了一种从同步多视点视频中进行自我监督学习的方法。我们使用跨视图重建任务在模型中注入几何信息。我们的方法基于掩蔽自动编码器（MAE）框架。除了相同的视图解码器外，我们还引入了一个单独的交叉视图解码器，该解码器利用交叉注意力机制，使用来自源视点的视频重建目标视点视频，以帮助表示对视点变化具有鲁棒性。对于视频，静态区域可以被琐碎地重建，这阻碍了学习有意义的表示。为了解决这个问题，我们引入了一种运动加权重建损失，它改进了时间建模。我们在NTU-60、NTU-120和ETRI数据集上，以及在NUCLA、PKU-MMD-II和ROCOG-v2数据集上的迁移学习环境中报告了最先进的结果，证明了我们方法的稳健性。将提供代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15900v1" target="_blank">2401.15900v1</a>
                              </td>
                              <td>MV2MAE: Multi-View Video Masked Autoencoders</td>
                              <td>Ketul Shah</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15900v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15900v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2207_13532v3_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Contrastive Masked Autoencoders are Stronger Vision Learners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2207_13532v3_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2207_13532v3_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2207_13532v3_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked image modeling (MIM) has achieved promising results on various vision tasks. However, the limited discriminability of learned representation manifests there is still plenty to go for making a stronger vision learner. Towards this goal, we propose Contrastive Masked Autoencoders (CMAE), a new self-supervised pre-training method for learning more comprehensive and capable vision representations. By elaboratively unifying contrastive learning (CL) and masked image model (MIM) through novel designs, CMAE leverages their respective advantages and learns representations with both strong instance discriminability and local perceptibility. Specifically, CMAE consists of two branches where the online branch is an asymmetric encoder-decoder and the momentum branch is a momentum updated encoder. During training, the online encoder reconstructs original images from latent representations of masked images to learn holistic features. The momentum encoder, fed with the full images, enhances the feature discriminability via contrastive learning with its online counterpart. To make CL compatible with MIM, CMAE introduces two new components, i.e. pixel shifting for generating plausible positive views and feature decoder for complementing features of contrastive pairs. Thanks to these novel designs, CMAE effectively improves the representation quality and transfer performance over its MIM counterpart. CMAE achieves the state-of-the-art performance on highly competitive benchmarks of image classification, semantic segmentation and object detection. Notably, CMAE-Base achieves $85.3\%$ top-1 accuracy on ImageNet and $52.5\%$ mIoU on ADE20k, surpassing previous best results by $0.7\%$ and $1.8\%$ respectively. The source code is publicly accessible at \url{https://github.com/ZhichengHuang/CMAE}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2207_13532v3_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩模图像建模（MIM）在各种视觉任务中取得了良好的效果。然而，学习表征的有限可分辨性表明，要培养一个更强的视觉学习者，还有很多路要走。为了实现这一目标，我们提出了对比掩模自动编码器（CMAE），这是一种新的自监督预训练方法，用于学习更全面、更有能力的视觉表示。通过新颖的设计，将对比学习（CL）和掩蔽图像模型（MIM）精细地统一起来，CMAE利用了它们各自的优势，学习了具有强实例可分辨性和局部可感知性的表示。具体而言，CMAE由两个分支组成，其中在线分支是非对称编码器-解码器，动量分支是动量更新编码器。在训练过程中，在线编码器从掩蔽图像的潜在表示重建原始图像，以学习整体特征。动量编码器提供了完整的图像，通过与在线编码器的对比学习增强了特征的可分辨性。为了使CL与MIM兼容，CMAE引入了两个新的组件，即用于生成可信正视图的像素移位和用于补充对比对的特征的特征解码器。得益于这些新颖的设计，CMAE比MIM有效地提高了表示质量和传输性能。CMAE在极具竞争力的图像分类、语义分割和对象检测基准上实现了最先进的性能。值得注意的是，CMAE Base在ImageNet上实现了85.3\%%$的前1精度，在ADE20k上实现了52.5\%%$mIoU，分别比以前的最佳结果高出0.7\%%$和1.8\%%$。源代码可在\url上公开访问{https://github.com/ZhichengHuang/CMAE}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2207.13532v3" target="_blank">2207.13532v3</a>
                              </td>
                              <td>Contrastive Masked Autoencoders are Stronger Vision Learners</td>
                              <td>Zhicheng Huang</td>
                              <td>2022-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2207_13532v3_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2207.13532v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhichenghuang/cmae" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14391v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rethinking Patch Dependence for Masked Autoencoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14391v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14391v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14391v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we re-examine inter-patch dependencies in the decoding mechanism of masked autoencoders (MAE). We decompose this decoding mechanism for masked patch reconstruction in MAE into self-attention and cross-attention. Our investigations suggest that self-attention between mask patches is not essential for learning good representations. To this end, we propose a novel pretraining framework: Cross-Attention Masked Autoencoders (CrossMAE). CrossMAE's decoder leverages only cross-attention between masked and visible tokens, with no degradation in downstream performance. This design also enables decoding only a small subset of mask tokens, boosting efficiency. Furthermore, each decoder block can now leverage different encoder features, resulting in improved representation learning. CrossMAE matches MAE in performance with 2.5 to 3.7$\times$ less decoding compute. It also surpasses MAE on ImageNet classification and COCO instance segmentation under the same compute. Code and models: https://crossmae.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14391v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们重新审视了屏蔽自动编码器（MAE）解码机制中的补丁间依赖关系。我们将这种用于MAE中掩蔽补丁重建的解码机制分解为自注意和交叉注意。我们的研究表明，掩模贴片之间的自我关注对于学习良好的表征并不重要。为此，我们提出了一种新的预训练框架：交叉注意力掩蔽自动编码器（CrossMAE）。CrossMAE的解码器仅利用掩码和可见令牌之间的交叉关注，不会降低下游性能。这种设计还能够仅解码掩码令牌的一小部分，从而提高效率。此外，每个解码器块现在可以利用不同的编码器特征，从而改进表示学习。CrossMAE在性能上与MAE相匹配，解码计算量减少了2.5到3.7$\倍。在同一计算下，它在ImageNet分类和COCO实例分割上也超过了MAE。代码和型号：https://crossmae.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14391v1" target="_blank">2401.14391v1</a>
                              </td>
                              <td>Rethinking Patch Dependence for Masked Autoencoders</td>
                              <td>Letian Fu</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14391v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14391v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13551v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13551v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13551v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13551v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Without human annotations, a typical Unsupervised Video Anomaly Detection (UVAD) method needs to train two models that generate pseudo labels for each other. In previous work, the two models are closely entangled with each other, and it is not known how to upgrade their method without modifying their training framework significantly. Second, previous work usually adopts fixed thresholding to obtain pseudo labels, however the user-specified threshold is not reliable which inevitably introduces errors into the training process. To alleviate these two problems, we propose a novel interleaved framework that alternately trains a One-Class Classification (OCC) model and a Weakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can be easily replaced with other OCC or WS models, which facilitates our method to upgrade with the most recent developments in both fields. For handling the fixed thresholding problem, we break through the conventional cognitive boundary and propose a weighted OCC model that can be trained on both normal and abnormal data. We also propose an adaptive mechanism for automatically finding the optimal threshold for the WS model in a loose to strict manner. Experiments demonstrate that the proposed UVAD method outperforms previous approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13551v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在没有人工注释的情况下，典型的无监督视频异常检测（UVAD）方法需要训练两个为彼此生成伪标签的模型。在以前的工作中，这两个模型彼此紧密纠缠，不知道如何在不显著修改训练框架的情况下升级它们的方法。其次，以前的工作通常采用固定的阈值来获得伪标签，但用户指定的阈值是不可靠的，这不可避免地会在训练过程中引入错误。为了缓解这两个问题，我们提出了一种新的交错框架，该框架交替训练UVAD的一类分类（OCC）模型和弱监督（WS）模型。我们方法中的OCC或WS模型可以很容易地被其他OCC或WS模式取代，这有助于我们的方法随着这两个领域的最新发展而升级。为了处理固定阈值问题，我们突破了传统的认知边界，提出了一种可以在正常和异常数据上训练的加权OCC模型。我们还提出了一种自适应机制，用于以宽松到严格的方式自动找到WS模型的最佳阈值。实验表明，所提出的UVAD方法优于以前的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13551v1" target="_blank">2401.13551v1</a>
                              </td>
                              <td>Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection</td>
                              <td>Yongwei Nie</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13551v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13551v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16613v2_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16613v2_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16613v2_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16613v2_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we propose the use of self-supervised pretraining on a large unlabelled data set to improve the performance of a personalized voice activity detection (VAD) model in adverse conditions. We pretrain a long short-term memory (LSTM)-encoder using the autoregressive predictive coding (APC) framework and fine-tune it for personalized VAD. We also propose a denoising variant of APC, with the goal of improving the robustness of personalized VAD. The trained models are systematically evaluated on both clean speech and speech contaminated by various types of noise at different SNR-levels and compared to a purely supervised model. Our experiments show that self-supervised pretraining not only improves performance in clean conditions, but also yields models which are more robust to adverse conditions compared to purely supervised learning.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16613v2_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们提出在大型未标记数据集上使用自监督预训练来提高个性化语音活动检测（VAD）模型在不利条件下的性能。我们使用自回归预测编码（APC）框架预训练长短期记忆（LSTM）编码器，并对其进行微调以实现个性化VAD。我们还提出了APC的去噪变体，目的是提高个性化VAD的鲁棒性。在不同SNR水平下，对干净语音和被各种类型的噪声污染的语音系统地评估训练的模型，并将其与纯监督模型进行比较。我们的实验表明，自监督预训练不仅提高了在干净条件下的性能，而且与纯监督学习相比，产生的模型对不利条件更具鲁棒性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16613v2" target="_blank">2312.16613v2</a>
                              </td>
                              <td>Self-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions</td>
                              <td>Holger Severin Bovbjerg</td>
                              <td>2023-12-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16613v2_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16613v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14451v2_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Prompt-Enhanced Context Features for Weakly-Supervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14451v2_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14451v2_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14451v2_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection under weak supervision presents significant challenges, particularly due to the lack of frame-level annotations during training. While prior research has utilized graph convolution networks and self-attention mechanisms alongside multiple instance learning (MIL)-based classification loss to model temporal relations and learn discriminative features, these methods often employ multi-branch architectures to capture local and global dependencies separately, resulting in increased parameters and computational costs. Moreover, the coarse-grained interclass separability provided by the binary constraint of MIL-based loss neglects the fine-grained discriminability within anomalous classes. In response, this paper introduces a weakly supervised anomaly detection framework that focuses on efficient context modeling and enhanced semantic discriminability. We present a Temporal Context Aggregation (TCA) module that captures comprehensive contextual information by reusing the similarity matrix and implementing adaptive fusion. Additionally, we propose a Prompt-Enhanced Learning (PEL) module that integrates semantic priors using knowledge-based prompts to boost the discriminative capacity of context features while ensuring separability between anomaly sub-classes. Extensive experiments validate the effectiveness of our method's components, demonstrating competitive performance with reduced parameters and computational effort on three challenging benchmarks: UCF-Crime, XD-Violence, and ShanghaiTech datasets. Notably, our approach significantly improves the detection accuracy of certain anomaly sub-classes, underscoring its practical value and efficacy. Our code is available at: https://github.com/yujiangpu20/PEL4VAD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14451v2_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>弱监督下的视频异常检测带来了重大挑战，特别是由于训练过程中缺乏帧级注释。虽然先前的研究已经利用图卷积网络和自注意机制以及基于多实例学习（MIL）的分类损失来建模时间关系和学习判别特征，但这些方法通常使用多分支架构来分别捕获局部和全局依赖性，导致参数和计算成本增加。此外，基于MIL的损失的二元约束所提供的粗粒度类间可分性忽略了异常类内的细粒度可分辨性。作为回应，本文介绍了一种弱监督异常检测框架，该框架侧重于高效的上下文建模和增强的语义可分辨性。我们提出了一个时间上下文聚合（TCA）模块，该模块通过重用相似度矩阵和实现自适应融合来捕获全面的上下文信息。此外，我们提出了一个提示增强学习（PEL）模块，该模块使用基于知识的提示集成语义先验，以提高上下文特征的判别能力，同时确保异常子类之间的可分性。大量实验验证了我们方法组件的有效性，在三个具有挑战性的基准上展示了具有竞争力的性能，减少了参数和计算工作量：UCF犯罪、XD暴力和ShanghaiTech数据集。值得注意的是，我们的方法显著提高了某些异常子类的检测精度，突出了其实用价值和有效性。我们的代码位于：https://github.com/yujiangpu20/PEL4VAD.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14451v2" target="_blank">2306.14451v2</a>
                              </td>
                              <td>Learning Prompt-Enhanced Context Features for Weakly-Supervised Video Anomaly Detection</td>
                              <td>Yujiang Pu</td>
                              <td>2023-06-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14451v2_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14451v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yujiangpu20/pel4vad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_12344v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_12344v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_12344v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_12344v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite the revolutionary impact of AI and the development of locally trained algorithms, achieving widespread generalized learning from multi-modal data in medical AI remains a significant challenge. This gap hinders the practical deployment of scalable medical AI solutions. Addressing this challenge, our research contributes a self-supervised robust machine learning framework, OCT-SelfNet, for detecting eye diseases using optical coherence tomography (OCT) images. In this work, various data sets from various institutions are combined enabling a more comprehensive range of representation. Our method addresses the issue using a two-phase training approach that combines self-supervised pretraining and supervised fine-tuning with a mask autoencoder based on the SwinV2 backbone by providing a solution for real-world clinical deployment. Extensive experiments on three datasets with different encoder backbones, low data settings, unseen data settings, and the effect of augmentation show that our method outperforms the baseline model, Resnet-50 by consistently attaining AUC-ROC performance surpassing 77% across all tests, whereas the baseline model exceeds 54%. Moreover, in terms of the AUC-PR metric, our proposed method exceeded 42%, showcasing a substantial increase of at least 10% in performance compared to the baseline, which exceeded only 33%. This contributes to our understanding of our approach's potential and emphasizes its usefulness in clinical settings.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_12344v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管人工智能和本地训练算法的发展产生了革命性的影响，但在医学人工智能中实现从多模态数据的广泛广义学习仍然是一个重大挑战。这一差距阻碍了可扩展医疗人工智能解决方案的实际部署。为了应对这一挑战，我们的研究提供了一个自我监督的稳健机器学习框架OCT SelfNet，用于使用光学相干断层扫描（OCT）图像检测眼病。在这项工作中，来自不同机构的各种数据集被结合在一起，从而实现更全面的代表性。我们的方法使用两阶段训练方法来解决这个问题，该方法将自监督预训练和监督微调与基于SwinV2主干的掩码自动编码器相结合，为现实世界的临床部署提供了解决方案。在具有不同编码器主干、低数据设置、看不见的数据设置和增强效果的三个数据集上进行的广泛实验表明，我们的方法优于基线模型Resnet-50，在所有测试中始终获得超过77%的AUC-ROC性能，而基线模型超过54%。此外，就AUC-PR指标而言，我们提出的方法超过了42%，与仅超过33%的基线相比，性能显著提高了至少10%。这有助于我们理解我们的方法的潜力，并强调其在临床环境中的有用性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.12344v1" target="_blank">2401.12344v1</a>
                              </td>
                              <td>OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection</td>
                              <td>Fatema-E Jannat</td>
                              <td>2024-01-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_12344v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.12344v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_05758v2_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BEV-MAE: Bird's Eye View Masked Autoencoders for Point Cloud Pre-training in Autonomous Driving Scenarios</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_05758v2_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_05758v2_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_05758v2_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Existing LiDAR-based 3D object detection methods for autonomous driving scenarios mainly adopt the training-from-scratch paradigm. Unfortunately, this paradigm heavily relies on large-scale labeled data, whose collection can be expensive and time-consuming. Self-supervised pre-training is an effective and desirable way to alleviate this dependence on extensive annotated data. In this work, we present BEV-MAE, an efficient masked autoencoder pre-training framework for LiDAR-based 3D object detection in autonomous driving. Specifically, we propose a bird's eye view (BEV) guided masking strategy to guide the 3D encoder learning feature representation in a BEV perspective and avoid complex decoder design during pre-training. Furthermore, we introduce a learnable point token to maintain a consistent receptive field size of the 3D encoder with fine-tuning for masked point cloud inputs. Based on the property of outdoor point clouds in autonomous driving scenarios, i.e., the point clouds of distant objects are more sparse, we propose point density prediction to enable the 3D encoder to learn location information, which is essential for object detection. Experimental results show that BEV-MAE surpasses prior state-of-the-art self-supervised methods and achieves a favorably pre-training efficiency. Furthermore, based on TransFusion-L, BEV-MAE achieves new state-of-the-art LiDAR-based 3D object detection results, with 73.6 NDS and 69.6 mAP on the nuScenes benchmark. The source code will be released at https://github.com/VDIGPKU/BEV-MAE</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_05758v2_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的基于激光雷达的自动驾驶场景三维物体检测方法主要采用从头开始的训练范式。不幸的是，这种范式严重依赖于大规模的标记数据，而这些数据的收集可能既昂贵又耗时。自我监督的预训练是一种有效且可取的方式，可以减轻对大量注释数据的依赖。在这项工作中，我们提出了BEV-MAE，这是一种用于自动驾驶中基于激光雷达的3D目标检测的高效掩蔽自动编码器预训练框架。具体而言，我们提出了一种鸟瞰图（BEV）引导的掩蔽策略，以从BEV的角度指导3D编码器学习特征表示，并避免在预训练期间进行复杂的解码器设计。此外，我们引入了一种可学习的点标记，以保持3D编码器的感受野大小一致，并对屏蔽的点云输入进行微调。基于自动驾驶场景中室外点云的特性，即远处物体的点云更稀疏，我们提出了点密度预测，以使3D编码器能够学习位置信息，这对物体检测至关重要。实验结果表明，BEV-MAE超越了现有的最先进的自监督方法，并实现了良好的预训练效率。此外，基于TransFusion-L，BEV-MAE实现了最先进的基于激光雷达的3D物体检测结果，在nuScenes基准上具有73.6 NDS和69.6 mAP。源代码将于https://github.com/VDIGPKU/BEV-MAE</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.05758v2" target="_blank">2212.05758v2</a>
                              </td>
                              <td>BEV-MAE: Bird's Eye View Masked Autoencoders for Point Cloud Pre-training in Autonomous Driving Scenarios</td>
                              <td>Zhiwei Lin</td>
                              <td>2022-12-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_05758v2_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.05758v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/vdigpku/bev-mae" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2205_14540v3_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2205_14540v3_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2205_14540v3_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2205_14540v3_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, self-supervised Masked Autoencoders (MAE) have attracted unprecedented attention for their impressive representation learning ability. However, the pretext task, Masked Image Modeling (MIM), reconstructs the missing local patches, lacking the global understanding of the image. This paper extends MAE to a fully supervised setting by adding a supervised classification branch, thereby enabling MAE to learn global features from golden labels effectively. The proposed Supervised MAE (SupMAE) only exploits a visible subset of image patches for classification, unlike the standard supervised pre-training where all image patches are used. Through experiments, we demonstrate that SupMAE is not only more training efficient but it also learns more robust and transferable features. Specifically, SupMAE achieves comparable performance with MAE using only 30% of compute when evaluated on ImageNet with the ViT-B/16 model. SupMAE's robustness on ImageNet variants and transfer learning performance outperforms MAE and standard supervised pre-training counterparts. Codes are available at https://github.com/enyac-group/supmae.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2205_14540v3_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，自监督掩模自动编码器（MAE）以其令人印象深刻的表示学习能力引起了前所未有的关注。然而，掩模图像建模（MIM）这一借口任务重建了缺失的局部补丁，缺乏对图像的全局理解。本文通过添加监督分类分支，将MAE扩展到完全监督环境，从而使MAE能够有效地从黄金标签中学习全局特征。与使用所有图像块的标准监督预训练不同，所提出的监督MAE（SupMAE）仅利用图像块的可见子集进行分类。通过实验，我们证明了SupMAE不仅训练效率更高，而且可以学习更健壮和可转移的特征。具体而言，当在ImageNet上使用ViT-B/16模型进行评估时，SupMAE仅使用30%的计算就实现了与MAE相当的性能。SupMAE在ImageNet变体上的鲁棒性和迁移学习性能优于MAE和标准监督预训练对手。代码可在https://github.com/enyac-group/supmae.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2205.14540v3" target="_blank">2205.14540v3</a>
                              </td>
                              <td>SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners</td>
                              <td>Feng Liang</td>
                              <td>2022-05-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2205_14540v3_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2205.14540v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/enyac-group/supmae" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_11311v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_11311v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_11311v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_11311v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, the rapid evolution of computer vision has seen the emergence of various vision foundation models, each tailored to specific data types and tasks. While large language models often share a common pretext task, the diversity in vision foundation models arises from their varying training objectives. In this study, we delve into the quest for identifying the most effective vision foundation models for few-shot semantic segmentation, a critical task in computer vision. Specifically, we conduct a comprehensive comparative analysis of four prominent foundation models: DINO V2, Segment Anything, CLIP, Masked AutoEncoders, and a straightforward ResNet50 pre-trained on the COCO dataset. Our investigation focuses on their adaptability to new semantic segmentation tasks, leveraging only a limited number of segmented images. Our experimental findings reveal that DINO V2 consistently outperforms the other considered foundation models across a diverse range of datasets and adaptation methods. This outcome underscores DINO V2's superior capability to adapt to semantic segmentation tasks compared to its counterparts. Furthermore, our observations indicate that various adapter methods exhibit similar performance, emphasizing the paramount importance of selecting a robust feature extractor over the intricacies of the adaptation technique itself. This insight sheds light on the critical role of feature extraction in the context of few-shot semantic segmentation. This research not only contributes valuable insights into the comparative performance of vision foundation models in the realm of few-shot semantic segmentation but also highlights the significance of a robust feature extractor in this domain.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_11311v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，随着计算机视觉的快速发展，出现了各种视觉基础模型，每种模型都适合特定的数据类型和任务。虽然大型语言模型通常有一个共同的借口任务，但视觉基础模型的多样性源于它们不同的训练目标。在这项研究中，我们深入探讨了为少镜头语义分割（计算机视觉中的一项关键任务）确定最有效的视觉基础模型的探索。具体而言，我们对四个突出的基础模型进行了全面的比较分析：DINO V2、Segment Anything、CLIP、Masked AutoEncoders和在COCO数据集上预训练的直接ResNet50。我们的研究重点是它们对新的语义分割任务的适应性，仅利用有限数量的分割图像。我们的实验结果表明，在各种数据集和适应方法中，DINO V2始终优于其他考虑的基础模型。这一结果突显了与同类产品相比，DINO V2在适应语义分割任务方面的卓越能力。此外，我们的观察结果表明，各种适配器方法表现出相似的性能，强调了选择鲁棒特征提取器的至关重要性，而不是自适应技术本身的复杂性。这一见解揭示了特征提取在少镜头语义分割中的关键作用。这项研究不仅为视觉基础模型在少镜头语义分割领域的比较性能提供了有价值的见解，而且突出了鲁棒特征提取器在该领域的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.11311v1" target="_blank">2401.11311v1</a>
                              </td>
                              <td>A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models</td>
                              <td>Reda Bensaid</td>
                              <td>2024-01-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_11311v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.11311v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_00114v4_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_00114v4_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_00114v4_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_00114v4_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The existing methods for video anomaly detection mostly utilize videos containing identifiable facial and appearance-based features. The use of videos with identifiable faces raises privacy concerns, especially when used in a hospital or community-based setting. Appearance-based features can also be sensitive to pixel-based noise, straining the anomaly detection methods to model the changes in the background and making it difficult to focus on the actions of humans in the foreground. Structural information in the form of skeletons describing the human motion in the videos is privacy-protecting and can overcome some of the problems posed by appearance-based features. In this paper, we present a survey of privacy-protecting deep learning anomaly detection methods using skeletons extracted from videos. We present a novel taxonomy of algorithms based on the various learning approaches. We conclude that skeleton-based approaches for anomaly detection can be a plausible privacy-protecting alternative for video anomaly detection. Lastly, we identify major open research questions and provide guidelines to address them.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_00114v4_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的视频异常检测方法大多利用包含可识别的面部和基于外观的特征的视频。使用可识别人脸的视频会引发隐私问题，尤其是在医院或社区环境中使用时。基于外观的特征也可能对基于像素的噪声敏感，这使异常检测方法难以对背景变化进行建模，并使其难以关注前景中人类的行为。视频中描述人体运动的骨架形式的结构信息是隐私保护，可以克服基于外观的特征带来的一些问题。在本文中，我们对使用从视频中提取的骨架的隐私保护深度学习异常检测方法进行了调查。我们提出了一种基于各种学习方法的新的算法分类法。我们得出的结论是，基于骨架的异常检测方法可以成为视频异常检测的一种合理的隐私保护替代方案。最后，我们确定了主要的开放研究问题，并提供了解决这些问题的指导方针。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.00114v4" target="_blank">2301.00114v4</a>
                              </td>
                              <td>Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions</td>
                              <td>Pratik K. Mishra</td>
                              <td>2022-12-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_00114v4_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.00114v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_17547v3_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Translatotron 3: Speech to Speech Translation with Monolingual Data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_17547v3_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_17547v3_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_17547v3_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper presents Translatotron 3, a novel approach to unsupervised direct speech-to-speech translation from monolingual speech-text datasets by combining masked autoencoder, unsupervised embedding mapping, and back-translation. Experimental results in speech-to-speech translation tasks between Spanish and English show that Translatotron 3 outperforms a baseline cascade system, reporting $18.14$ BLEU points improvement on the synthesized Unpaired-Conversational dataset. In contrast to supervised approaches that necessitate real paired data, or specialized modeling to replicate para-/non-linguistic information such as pauses, speaking rates, and speaker identity, Translatotron 3 showcases its capability to retain it. Audio samples can be found at http://google-research.github.io/lingvo-lab/translatotron3</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_17547v3_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文提出了Translatotron 3，这是一种新的方法，通过结合掩蔽自动编码器、无监督嵌入映射和反翻译，从单语语音文本数据集进行无监督直接语音到语音的翻译。在西班牙语和英语之间的语音到语音翻译任务中的实验结果表明，Translatotron 3的性能优于基线级联系统，在合成的非配对会话数据集上提高了18.14$BLEU点。与需要真实配对数据或专门建模来复制准/非语言信息（如停顿、语速和说话者身份）的监督方法不同，Translatotron 3展示了其保留信息的能力。音频样本可在http://google-research.github.io/lingvo-lab/translatotron3</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.17547v3" target="_blank">2305.17547v3</a>
                              </td>
                              <td>Translatotron 3: Speech to Speech Translation with Monolingual Data</td>
                              <td>Eliya Nachmani</td>
                              <td>2023-05-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_17547v3_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.17547v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_07892v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Deep Fuzzy Framework for Emotion Recognition using EEG Signals and Emotion Representation in Type-2 Fuzzy VAD Space</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_07892v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_07892v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_07892v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Recently, the representation of emotions in the Valence, Arousal and Dominance (VAD) space has drawn enough attention. However, the complex nature of emotions and the subjective biases in self-reported values of VAD make the emotion model too specific to a particular experiment. This study aims to develop a generic model representing emotions using a fuzzy VAD space and improve emotion recognition by utilizing this representation. We partitioned the crisp VAD space into a fuzzy VAD space using low, medium and high type-2 fuzzy dimensions to represent emotions. A framework that integrates fuzzy VAD space with EEG data has been developed to recognize emotions. The EEG features were extracted using spatial and temporal feature vectors from time-frequency spectrograms, while the subject-reported values of VAD were also considered. The study was conducted on the DENS dataset, which includes a wide range of twenty-four emotions, along with EEG data and subjective ratings. The study was validated using various deep fuzzy framework models based on type-2 fuzzy representation, cuboid probabilistic lattice representation and unsupervised fuzzy emotion clusters. These models resulted in emotion recognition accuracy of 96.09\%, 95.75\% and 95.31\%, respectively, for the classes of 24 emotions. The study also included an ablation study, one with crisp VAD space and the other without VAD space. The result with crisp VAD space performed better, while the deep fuzzy framework outperformed both models. The model was extended to predict cross-subject cases of emotions, and the results with 78.37\% accuracy are promising, proving the generality of our model. The generic nature of the developed model, along with its successful cross-subject predictions, gives direction for real-world applications in the areas such as affective computing, human-computer interaction, and mental health monitoring.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_07892v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近，情绪在价、唤起和支配（VAD）空间中的表征引起了足够的关注。然而，情绪的复杂性质和VAD自我报告值的主观偏见使情绪模型对特定实验过于具体。本研究旨在开发一个使用模糊VAD空间表示情绪的通用模型，并通过使用该表示来提高情绪识别。我们使用低、中、高类型2模糊维度来表示情绪，将清晰的VAD空间划分为模糊VAD空间。已经开发了一种将模糊VAD空间与脑电图数据相结合的框架来识别情绪。使用时频频谱图中的空间和时间特征向量提取EEG特征，同时还考虑了受试者报告的VAD值。这项研究是在DENS数据集上进行的，该数据集包括24种广泛的情绪，以及脑电图数据和主观评级。使用基于2型模糊表示、长方体概率格表示和无监督模糊情感聚类的各种深度模糊框架模型对该研究进行了验证。这些模型对24种情绪类别的情绪识别准确率分别为96.09%、95.75%和95.31%。该研究还包括一项消融研究，一项有清晰的VAD空间，另一项没有VAD空间。具有清晰VAD空间的结果表现更好，而深度模糊框架的表现优于这两个模型。将该模型扩展到跨学科情感案例的预测中，结果的准确率为78.37%，证明了该模型的通用性。所开发模型的通用性及其成功的跨学科预测为情感计算、人机交互和心理健康监测等领域的现实应用提供了方向。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.07892v1" target="_blank">2401.07892v1</a>
                              </td>
                              <td>Deep Fuzzy Framework for Emotion Recognition using EEG Signals and Emotion Representation in Type-2 Fuzzy VAD Space</td>
                              <td>Mohammad Asif</td>
                              <td>2024-01-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_07892v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.07892v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05702v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Video Anomaly Detection and Explanation via Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05702v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05702v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05702v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Anomaly Detection (VAD) aims to localize abnormal events on the timeline of long-range surveillance videos. Anomaly-scoring-based methods have been prevailing for years but suffer from the high complexity of thresholding and low explanability of detection results. In this paper, we conduct pioneer research on equipping video-based large language models (VLLMs) in the framework of VAD, making the VAD model free from thresholds and able to explain the reasons for the detected anomalies. We introduce a novel network module Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range context modeling. We design a three-phase training method to improve the efficiency of fine-tuning VLLMs by substantially minimizing the requirements for VAD data and lowering the costs of annotating instruction-tuning data. Our trained model achieves the top performance on the anomaly videos of the UCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\% and +4.96\%, respectively. More impressively, our approach can provide textual explanations for detected anomalies.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05702v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）旨在定位远程监控视频时间线上的异常事件。基于异常评分的方法已经盛行多年，但存在阈值处理的高复杂性和检测结果的低可解释性的问题。在本文中，我们对在VAD框架下配备基于视频的大型语言模型（VLLMs）进行了开创性的研究，使VAD模型不受阈值的限制，能够解释检测到异常的原因。我们引入了一种新的网络模块长期上下文（LTC），以减轻VLLM在长期上下文建模中的无能。我们设计了一种三阶段训练方法，通过显著降低对VAD数据的要求和降低注释指令调整数据的成本，来提高VLLM的微调效率。我们训练的模型在UCF犯罪和TAD基准的异常视频上实现了最高性能，AUC分别提高了+3.86%和+4.96%。更令人印象深刻的是，我们的方法可以为检测到的异常提供文本解释。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05702v1" target="_blank">2401.05702v1</a>
                              </td>
                              <td>Video Anomaly Detection and Explanation via Large Language Models</td>
                              <td>Hui Lv</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05702v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05702v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05698v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual Emotion Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05698v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05698v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05698v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Audio-Visual Emotion Recognition (AVER) has garnered increasing attention in recent years for its critical role in creating emotion-ware intelligent machines. Previous efforts in this area are dominated by the supervised learning paradigm. Despite significant progress, supervised learning is meeting its bottleneck due to the longstanding data scarcity issue in AVER. Motivated by recent advances in self-supervised learning, we propose Hierarchical Contrastive Masked Autoencoder (HiCMAE), a novel self-supervised framework that leverages large-scale self-supervised pre-training on vast unlabeled audio-visual data to promote the advancement of AVER. Following prior arts in self-supervised audio-visual representation learning, HiCMAE adopts two primary forms of self-supervision for pre-training, namely masked data modeling and contrastive learning. Unlike them which focus exclusively on top-layer representations while neglecting explicit guidance of intermediate layers, HiCMAE develops a three-pronged strategy to foster hierarchical audio-visual feature learning and improve the overall quality of learned representations. To verify the effectiveness of HiCMAE, we conduct extensive experiments on 9 datasets covering both categorical and dimensional AVER tasks. Experimental results show that our method significantly outperforms state-of-the-art supervised and self-supervised audio-visual methods, which indicates that HiCMAE is a powerful audio-visual emotion representation learner. Codes and models will be publicly available at https://github.com/sunlicai/HiCMAE.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05698v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视听情感识别（AVER）近年来因其在创建情感软件智能机器中的关键作用而越来越受到关注。以前在这一领域的努力主要是由监督学习范式主导的。尽管取得了重大进展，但由于AVER中长期存在的数据稀缺问题，监督学习正面临瓶颈。受自监督学习最新进展的启发，我们提出了分层对比掩蔽自动编码器（HiCMAE），这是一种新的自监督框架，利用对大量未标记视听数据的大规模自监督预训练来促进AVER的发展。继现有技术的自监督视听表示学习之后，HiCMAE采用了两种主要的自监督形式进行预训练，即掩蔽数据建模和对比学习。与只关注顶层表征而忽视中间层的明确指导不同，HiCMAE开发了一种三管齐下的策略来促进分层视听特征学习，并提高学习表征的整体质量。为了验证HiCMAE的有效性，我们在9个数据集上进行了广泛的实验，涵盖了分类和维度AVER任务。实验结果表明，我们的方法显著优于最先进的监督和自监督视听方法，这表明HiCMAE是一种强大的视听情绪表征学习器。代码和模型将在https://github.com/sunlicai/HiCMAE.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05698v1" target="_blank">2401.05698v1</a>
                              </td>
                              <td>HiCMAE: Hierarchical Contrastive Masked Autoencoder for Self-Supervised Audio-Visual Emotion Recognition</td>
                              <td>Licai Sun</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05698v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05698v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/sunlicai/hicmae" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04942v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04942v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04942v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04942v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the past several years, road anomaly segmentation is actively explored in the academia and drawing growing attention in the industry. The rationale behind is straightforward: if the autonomous car can brake before hitting an anomalous object, safety is promoted. However, this rationale naturally calls for a temporally informed setting while existing methods and benchmarks are designed in an unrealistic frame-wise manner. To bridge this gap, we contribute the first video anomaly segmentation dataset for autonomous driving. Since placing various anomalous objects on busy roads and annotating them in every frame are dangerous and expensive, we resort to synthetic data. To improve the relevance of this synthetic dataset to real-world applications, we train a generative adversarial network conditioned on rendering G-buffers for photorealism enhancement. Our dataset consists of 120,000 high-resolution frames at a 60 FPS framerate, as recorded in 7 different towns. As an initial benchmarking, we provide baselines using latest supervised and unsupervised road anomaly segmentation methods. Apart from conventional ones, we focus on two new metrics: temporal consistency and latencyaware streaming accuracy. We believe the latter is valuable as it measures whether an anomaly segmentation algorithm can truly prevent a car from crashing in a temporally informed setting.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04942v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几年里，道路异常分割在学术界得到了积极的探索，并引起了业界越来越多的关注。背后的理由很简单：如果自动驾驶汽车能够在撞上异常物体之前刹车，那么安全性就会得到提高。然而，这一原理自然需要一个时间知情的设置，而现有的方法和基准是以不切实际的框架方式设计的。为了弥补这一差距，我们贡献了第一个用于自动驾驶的视频异常分割数据集。由于在繁忙的道路上放置各种异常对象并在每一帧中对其进行注释既危险又昂贵，因此我们求助于合成数据。为了提高这个合成数据集与现实世界应用的相关性，我们训练了一个生成对抗性网络，该网络以渲染G缓冲区为条件，用于增强真实感。我们的数据集由120000个高分辨率帧组成，帧速率为60 FPS，记录在7个不同的城镇。作为初始基准，我们使用最新的有监督和无监督道路异常分割方法提供基线。除了传统的指标外，我们还关注两个新的指标：时间一致性和感知延迟的流媒体准确性。我们认为后者是有价值的，因为它可以衡量异常分割算法是否能够真正防止汽车在时间知情的情况下发生碰撞。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04942v1" target="_blank">2401.04942v1</a>
                              </td>
                              <td>Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics</td>
                              <td>Beiwen Tian</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04942v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04942v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04741v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Masked AutoEncoder for Graph Clustering without Pre-defined Cluster Number k</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04741v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04741v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04741v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Graph clustering algorithms with autoencoder structures have recently gained popularity due to their efficient performance and low training cost. However, for existing graph autoencoder clustering algorithms based on GCN or GAT, not only do they lack good generalization ability, but also the number of clusters clustered by such autoencoder models is difficult to determine automatically. To solve this problem, we propose a new framework called Graph Clustering with Masked Autoencoders (GCMA). It employs our designed fusion autoencoder based on the graph masking method for the fusion coding of graph. It introduces our improved density-based clustering algorithm as a second decoder while decoding with multi-target reconstruction. By decoding the mask embedding, our model can capture more generalized and comprehensive knowledge. The number of clusters and clustering results can be output end-to-end while improving the generalization ability. As a nonparametric class method, extensive experiments demonstrate the superiority of \textit{GCMA} over state-of-the-art baselines.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04741v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>具有自动编码器结构的图聚类算法由于其高效的性能和低的训练成本而最近受到欢迎。然而，对于现有的基于GCN或GAT的图自动编码器聚类算法，它们不仅缺乏良好的泛化能力，而且这种自动编码器模型聚类的聚类数量也很难自动确定。为了解决这个问题，我们提出了一个新的框架，称为带掩码自动编码器的图聚类（GCMA）。它采用我们设计的基于图屏蔽方法的融合自动编码器对图进行融合编码。它介绍了我们改进的基于密度的聚类算法，作为多目标重建解码时的第二解码器。通过对掩模嵌入进行解码，我们的模型可以获取更广泛、更全面的知识。聚类数量和聚类结果可以端到端输出，同时提高了泛化能力。作为一种非参数类方法，大量实验证明了\textit｛GCMA｝优于最先进的基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04741v1" target="_blank">2401.04741v1</a>
                              </td>
                              <td>Masked AutoEncoder for Graph Clustering without Pre-defined Cluster Number k</td>
                              <td>Yuanchi Ma</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04741v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04741v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_03325v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_03325v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_03325v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_03325v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Models trained on a labeled source domain (e.g., labeled images from wildlife camera traps) often generalize poorly when deployed on an out-of-distribution (OOD) target domain (e.g., images from new camera trap locations). In the domain adaptation setting where unlabeled target data is available, self-supervised pretraining (e.g., masked autoencoding or contrastive learning) is a promising method to mitigate this performance drop. Pretraining improves OOD error when the generic data augmentations used (e.g., masking or cropping) connect the source and target domains, which may be far apart in the input space. In this paper, we show on real-world tasks that standard fine-tuning after pretraining does not consistently improve OOD error over simply training from scratch on labeled source data. To better leverage pretraining for distribution shifts, we propose Connect Later: after pretraining with generic augmentations, fine-tune with targeted augmentations designed with knowledge of the distribution shift. Pretraining learns good representations within the source and target domains, while targeted augmentations connect the domains better during fine-tuning. Connect Later improves average OOD error over standard fine-tuning and supervised learning with targeted augmentations on 4 real-world datasets: Connect Later achieves the state-of-the-art on astronomical time-series classification (AstroClassification) by 2.5%, wildlife species identification (iWildCam-WILDS) with ResNet-50 by 0.9%, and tumor identification (Camelyon17-WILDS) with DenseNet121 by 1.1%; as well as best performance on a new dataset for astronomical time-series redshift prediction (Redshifts) by 0.03 RMSE (11% relative). Code and datasets are available at https://github.com/helenqu/connect-later.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_03325v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在标记源域（例如，来自野生动物相机陷阱的标记图像）上训练的模型在部署在分布外（OOD）目标域（例如来自新相机陷阱位置的图像）上时，通常泛化较差。在未标记目标数据可用的领域自适应设置中，自监督预训练（例如，掩蔽自动编码或对比学习）是缓解这种性能下降的一种很有前途的方法。当所使用的通用数据增强（例如，掩蔽或裁剪）连接源域和目标域时，预训练改进了OOD错误，这两个域在输入空间中可能相距很远。在本文中，我们在真实世界的任务中表明，与简单地在标记的源数据上从头开始训练相比，预训练后的标准微调并不能持续改善OOD错误。为了更好地利用分布变化的预训练，我们建议稍后连接：在使用通用增强进行预训练后，使用根据分布变化知识设计的目标增强进行微调。预训练学习源域和目标域内的良好表示，而目标增强在微调过程中更好地连接域。与标准微调和监督学习相比，Connect Later在4个真实世界数据集上通过有针对性的增强提高了平均OOD误差：Connect Later将天文时间序列分类（AstroClassification）提高了2.5%，将ResNet-50的野生动物物种识别（iWildCam WILDS）提高了0.9%，将肿瘤识别（Camelyon17 WILDS）DenseNet121增加1.1%；以及在天文时间序列红移预测（红移）的新数据集上的最佳性能为0.03RMSE（相对11%）。代码和数据集可在https://github.com/helenqu/connect-later.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.03325v1" target="_blank">2402.03325v1</a>
                              </td>
                              <td>Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations</td>
                              <td>Helen Qu</td>
                              <td>2024-01-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_03325v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.03325v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_06583v2_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Improving Masked Autoencoders by Learning Where to Mask</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_06583v2_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_06583v2_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_06583v2_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked image modeling is a promising self-supervised learning method for visual data. It is typically built upon image patches with random masks, which largely ignores the variation of information density between them. The question is: Is there a better masking strategy than random sampling and how can we learn it? We empirically study this problem and initially find that introducing object-centric priors in mask sampling can significantly improve the learned representations. Inspired by this observation, we present AutoMAE, a fully differentiable framework that uses Gumbel-Softmax to interlink an adversarially-trained mask generator and a mask-guided image modeling process. In this way, our approach can adaptively find patches with higher information density for different images, and further strike a balance between the information gain obtained from image reconstruction and its practical training difficulty. In our experiments, AutoMAE is shown to provide effective pretraining models on standard self-supervised benchmarks and downstream tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_06583v2_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩模图像建模是一种很有前途的视觉数据自监督学习方法。它通常建立在具有随机掩模的图像块上，这在很大程度上忽略了它们之间信息密度的变化。问题是：有比随机采样更好的掩蔽策略吗？我们如何学习它？我们对这个问题进行了实证研究，最初发现在掩模采样中引入以对象为中心的先验可以显著提高学习到的表示。受这一观察结果的启发，我们提出了AutoMAE，这是一个完全可微的框架，使用Gumbel Softmax将对抗性训练的掩模生成器和掩模引导的图像建模过程互连起来。这样，我们的方法可以自适应地为不同的图像找到具有更高信息密度的补丁，并进一步在图像重建获得的信息增益与其实际训练难度之间取得平衡。在我们的实验中，AutoMAE被证明可以在标准的自监督基准和下游任务上提供有效的预训练模型。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.06583v2" target="_blank">2303.06583v2</a>
                              </td>
                              <td>Improving Masked Autoencoders by Learning Where to Mask</td>
                              <td>Haijian Chen</td>
                              <td>2023-03-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_06583v2_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.06583v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_02764v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Fus-MAE: A cross-attention-based data fusion approach for Masked Autoencoders in remote sensing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_02764v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_02764v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_02764v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised frameworks for representation learning have recently stirred up interest among the remote sensing community, given their potential to mitigate the high labeling costs associated with curating large satellite image datasets. In the realm of multimodal data fusion, while the often used contrastive learning methods can help bridging the domain gap between different sensor types, they rely on data augmentations techniques that require expertise and careful design, especially for multispectral remote sensing data. A possible but rather scarcely studied way to circumvent these limitations is to use a masked image modelling based pretraining strategy. In this paper, we introduce Fus-MAE, a self-supervised learning framework based on masked autoencoders that uses cross-attention to perform early and feature-level data fusion between synthetic aperture radar and multispectral optical data - two modalities with a significant domain gap. Our empirical findings demonstrate that Fus-MAE can effectively compete with contrastive learning strategies tailored for SAR-optical data fusion and outperforms other masked-autoencoders frameworks trained on a larger corpus.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_02764v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>表示学习的自监督框架最近引起了遥感界的兴趣，因为它们有可能降低与管理大型卫星图像数据集相关的高标记成本。在多模式数据融合领域，虽然经常使用的对比学习方法可以帮助弥合不同传感器类型之间的领域差距，但它们依赖于需要专业知识和仔细设计的数据增强技术，尤其是对于多光谱遥感数据。一种可能但几乎没有研究过的规避这些限制的方法是使用基于掩蔽图像建模的预训练策略。在本文中，我们介绍了Fus-MAE，这是一种基于掩蔽自动编码器的自监督学习框架，它使用交叉注意力在合成孔径雷达和多光谱光学数据之间进行早期和特征级数据融合，这两种模式具有显著的域差距。我们的经验发现表明，Fus-MAE可以有效地与为SAR光学数据融合量身定制的对比学习策略竞争，并优于在更大语料库上训练的其他掩蔽自动编码器框架。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.02764v1" target="_blank">2401.02764v1</a>
                              </td>
                              <td>Fus-MAE: A cross-attention-based data fusion approach for Masked Autoencoders in remote sensing</td>
                              <td>Hugo Chan-To-Hing</td>
                              <td>2024-01-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_02764v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.02764v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_03937v2_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Diffusion Models as Masked Audio-Video Learners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_03937v2_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_03937v2_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_03937v2_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Over the past several years, the synchronization between audio and visual signals has been leveraged to learn richer audio-visual representations. Aided by the large availability of unlabeled videos, many unsupervised training frameworks have demonstrated impressive results in various downstream audio and video tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a state-of-the-art audio-video pre-training framework. MAViL couples contrastive learning with masked autoencoding to jointly reconstruct audio spectrograms and video frames by fusing information from both modalities. In this paper, we study the potential synergy between diffusion models and MAViL, seeking to derive mutual benefits from these two frameworks. The incorporation of diffusion into MAViL, combined with various training efficiency methodologies that include the utilization of a masking ratio curriculum and adaptive batch sizing, results in a notable 32% reduction in pre-training Floating-Point Operations (FLOPS) and an 18% decrease in pre-training wall clock time. Crucially, this enhanced efficiency does not compromise the model's performance in downstream audio-classification tasks when compared to MAViL's performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_03937v2_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几年里，音频和视觉信号之间的同步被用来学习更丰富的视听表示。在未标记视频的大量可用性的帮助下，许多无监督训练框架在各种下游音频和视频任务中表现出了令人印象深刻的结果。最近，掩蔽音视频学习器（MAViL）已经成为一种最先进的音视频预训练框架。MAViL将对比学习与掩蔽自动编码相结合，通过融合两种模式的信息来联合重建音频频谱图和视频帧。在本文中，我们研究了扩散模型和MAViL之间的潜在协同作用，试图从这两个框架中获得互惠互利。将扩散纳入MAViL，再加上各种训练效率方法，包括使用掩蔽比课程和自适应批量大小，使训练前浮点运算（FLOPS）显著减少32%，训练前挂钟时间减少18%。至关重要的是，与MAViL的性能相比，这种增强的效率不会影响模型在下游音频分类任务中的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.03937v2" target="_blank">2310.03937v2</a>
                              </td>
                              <td>Diffusion Models as Masked Audio-Video Learners</td>
                              <td>Elvis Nunez</td>
                              <td>2023-10-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_03937v2_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.03937v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_05411v2_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">R-MAE: Regions Meet Masked Autoencoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_05411v2_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_05411v2_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_05411v2_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we explore regions as a potential visual analogue of words for self-supervised image representation learning. Inspired by Masked Autoencoding (MAE), a generative pre-training baseline, we propose masked region autoencoding to learn from groups of pixels or regions. Specifically, we design an architecture which efficiently addresses the one-to-many mapping between images and regions, while being highly effective especially with high-quality regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks, with negligible computational overheads. Beyond the quantitative evaluation, our analysis indicates the models pre-trained with masked region autoencoding unlock the potential for interactive segmentation. The code is provided at https://github.com/facebookresearch/r-mae.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_05411v2_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们探索了区域作为单词的潜在视觉模拟，用于自我监督的图像表示学习。受生成预训练基线掩蔽自动编码（MAE）的启发，我们提出了掩蔽区域自动编码来从像素或区域组中学习。具体来说，我们设计了一种架构，它有效地解决了图像和区域之间的一对多映射，同时非常有效，尤其是在高质量区域中。当与MAE集成时，我们的方法（R-MAE）在各种预训练数据集和下游检测和分割基准上表现出了一致的改进，计算开销可以忽略不计。除了定量评估之外，我们的分析表明，用掩蔽区域自动编码预训练的模型释放了交互式分割的潜力。代码提供于https://github.com/facebookresearch/r-mae.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.05411v2" target="_blank">2306.05411v2</a>
                              </td>
                              <td>R-MAE: Regions Meet Masked Autoencoders</td>
                              <td>Duy-Kien Nguyen</td>
                              <td>2023-06-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_05411v2_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.05411v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/facebookresearch/r-mae" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_05922v3_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Audiovisual Masked Autoencoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_05922v3_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_05922v3_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_05922v3_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Can we leverage the audiovisual information already present in video to improve self-supervised representation learning? To answer this question, we study various pretraining architectures and objectives within the masked autoencoding framework, motivated by the success of similar methods in natural language and image understanding. We show that we can achieve significant improvements on audiovisual downstream classification tasks, surpassing the state-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage our audiovisual pretraining scheme for multiple unimodal downstream tasks using a single audiovisual pretrained model. We additionally demonstrate the transferability of our representations, achieving state-of-the-art audiovisual results on Epic Kitchens without pretraining specifically for this dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_05922v3_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们能利用视频中已经存在的视听信息来改善自我监督的表征学习吗？为了回答这个问题，我们在掩蔽自动编码框架内研究了各种预训练架构和目标，其动机是类似方法在自然语言和图像理解方面的成功。我们表明，我们可以在视听下游分类任务上实现显著改进，超过VGGSound和AudioSet上的最先进技术。此外，我们可以使用单个视听预训练模型，将我们的视听预训练方案用于多个单峰下游任务。我们还展示了我们的表示的可转移性，在Epic Kitchens上实现了最先进的视听结果，而无需专门针对该数据集进行预训练。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.05922v3" target="_blank">2212.05922v3</a>
                              </td>
                              <td>Audiovisual Masked Autoencoders</td>
                              <td>Mariana-Iuliana Georgescu</td>
                              <td>2022-12-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_05922v3_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.05922v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/google-research/scenic" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_00463v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Analyzing Local Representations of Self-supervised Vision Transformers</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_00463v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_00463v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_00463v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this paper, we present a comparative analysis of various self-supervised Vision Transformers (ViTs), focusing on their local representative power. Inspired by large language models, we examine the abilities of ViTs to perform various computer vision tasks with little to no fine-tuning. We design an evaluation framework to analyze the quality of local, i.e. patch-level, representations in the context of few-shot semantic segmentation, instance identification, object retrieval, and tracking. We discover that contrastive learning based methods like DINO produce more universal patch representations that can be immediately applied for downstream tasks with no parameter tuning, compared to masked image modeling. The embeddings learned using the latter approach, e.g. in masked autoencoders, have high variance features that harm distance-based algorithms, such as k-NN, and do not contain useful information for most downstream tasks. Furthermore, we demonstrate that removing these high-variance features enhances k-NN by providing an analysis of the benchmarks for this work and for Scale-MAE, a recent extension of masked autoencoders. Finally, we find an object instance retrieval setting where DINOv2, a model pretrained on two orders of magnitude more data, performs worse than its less compute-intensive counterpart DINO.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_00463v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在本文中，我们对各种自监督视觉变压器（ViT）进行了比较分析，重点分析了它们的局部代表性。受大型语言模型的启发，我们研究了ViT在几乎没有微调的情况下执行各种计算机视觉任务的能力。我们设计了一个评估框架来分析局部（即补丁级别）表示在少镜头语义分割、实例识别、对象检索和跟踪背景下的质量。我们发现，与掩蔽图像建模相比，基于对比学习的方法（如DINO）产生了更通用的补丁表示，可以立即应用于下游任务，而无需参数调整。使用后一种方法学习的嵌入，例如在掩码自动编码器中，具有高方差特征，这会损害基于距离的算法，例如k-NN，并且不包含用于大多数下游任务的有用信息。此外，我们证明，通过为这项工作和Scale MAE（掩蔽自动编码器的最近扩展）提供基准分析，去除这些高方差特征可以增强k-NN。最后，我们找到了一个对象实例检索设置，其中DINOv2，一个在两个数量级以上的数据上预训练的模型，其性能比计算密集度较低的对应DINO差。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.00463v1" target="_blank">2401.00463v1</a>
                              </td>
                              <td>Analyzing Local Representations of Self-supervised Vision Transformers</td>
                              <td>Ani Vanyan</td>
                              <td>2023-12-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_00463v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.00463v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_20704v2_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_20704v2_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_20704v2_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_20704v2_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite their success, ViTs lack inductive biases, which can make it difficult to train them with limited data. To address this challenge, prior studies suggest training ViTs with self-supervised learning (SSL) and fine-tuning sequentially. However, we observe that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the amount of training data is limited. We explore the appropriate SSL tasks that can be optimized alongside the primary task, the training schemes for these tasks, and the data scale at which they can be most effective. Our findings reveal that SSAT is a powerful technique that enables ViTs to leverage the unique characteristics of both the self-supervised and primary tasks, achieving better performance than typical ViTs pre-training with SSL and fine-tuning sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT significantly improves ViT performance while reducing carbon footprint. We also confirm the effectiveness of SSAT in the video domain for deepfake detection, showcasing its generalizability. Our code is available at https://github.com/dominickrei/Limited-data-vits.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_20704v2_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视觉转换器（ViTs）已经在计算机视觉中无处不在。尽管取得了成功，但ViTs缺乏归纳偏见，这可能会使其难以用有限的数据进行训练。为了应对这一挑战，先前的研究建议用自我监督学习（SSL）和顺序微调来训练ViT。然而，我们观察到，当训练数据量有限时，联合优化主任务和自监督辅助任务（SSAT）的ViT是令人惊讶的有益的。我们探索了可以与主要任务一起优化的适当SSL任务、这些任务的训练方案以及它们最有效的数据规模。我们的研究结果表明，SSAT是一种强大的技术，使ViTs能够利用自我监督和主要任务的独特特征，实现比使用SSL进行典型ViTs预训练和顺序微调更好的性能。我们在10个数据集上进行的实验表明，SSAT显著提高了ViT的性能，同时减少了碳足迹。我们还证实了SSAT在视频领域用于深度伪造检测的有效性，展示了其可推广性。我们的代码可在https://github.com/dominickrei/Limited-data-vits.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.20704v2" target="_blank">2310.20704v2</a>
                              </td>
                              <td>Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders</td>
                              <td>Srijan Das</td>
                              <td>2023-10-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_20704v2_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.20704v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/dominickrei/limited-data-vits" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_08738v2_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AV-MaskEnhancer: Enhancing Video Representations through Audio-Visual Masked Autoencoder</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_08738v2_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_08738v2_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_08738v2_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Learning high-quality video representation has shown significant applications in computer vision and remains challenging. Previous work based on mask autoencoders such as ImageMAE and VideoMAE has proven the effectiveness of learning representations in images and videos through reconstruction strategy in the visual modality. However, these models exhibit inherent limitations, particularly in scenarios where extracting features solely from the visual modality proves challenging, such as when dealing with low-resolution and blurry original videos. Based on this, we propose AV-MaskEnhancer for learning high-quality video representation by combining visual and audio information. Our approach addresses the challenge by demonstrating the complementary nature of audio and video features in cross-modality content. Moreover, our result of the video classification task on the UCF101 dataset outperforms the existing work and reaches the state-of-the-art, with a top-1 accuracy of 98.8% and a top-5 accuracy of 99.9%.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_08738v2_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>学习高质量的视频表示在计算机视觉中已经显示出重要的应用，并且仍然具有挑战性。先前基于ImageMAE和VideoMAE等掩模自动编码器的工作已经证明了通过视觉模态中的重建策略来学习图像和视频中的表示的有效性。然而，这些模型表现出固有的局限性，特别是在仅从视觉模态提取特征具有挑战性的情况下，例如在处理低分辨率和模糊的原始视频时。基于此，我们提出了AV掩码增强器，用于通过结合视觉和音频信息来学习高质量的视频表示。我们的方法通过展示跨模态内容中音频和视频功能的互补性来应对这一挑战。此外，我们在UCF101数据集上的视频分类任务的结果优于现有工作，达到了最先进的水平，前1名的准确率为98.8%，前5名的准确度为99.9%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.08738v2" target="_blank">2309.08738v2</a>
                              </td>
                              <td>AV-MaskEnhancer: Enhancing Video Representations through Audio-Visual Masked Autoencoder</td>
                              <td>Xingjian Diao</td>
                              <td>2023-09-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_08738v2_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.08738v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_12480v1_6">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adaptive Distribution Masked Autoencoders for Continual Test-Time Adaptation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_12480v1_6_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_12480v1_6_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_12480v1_6_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Continual Test-Time Adaptation (CTTA) is proposed to migrate a source pre-trained model to continually changing target distributions, addressing real-world dynamism. Existing CTTA methods mainly rely on entropy minimization or teacher-student pseudo-labeling schemes for knowledge extraction in unlabeled target domains. However, dynamic data distributions cause miscalibrated predictions and noisy pseudo-labels in existing self-supervised learning methods, hindering the effective mitigation of error accumulation and catastrophic forgetting problems during the continual adaptation process. To tackle these issues, we propose a continual self-supervised method, Adaptive Distribution Masked Autoencoders (ADMA), which enhances the extraction of target domain knowledge while mitigating the accumulation of distribution shifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism to adaptively sample masked positions, followed by establishing consistency constraints between the masked target samples and the original target samples. Additionally, for masked tokens, we utilize an efficient decoder to reconstruct a hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients), leveraging its invariant properties to boost task-relevant representations. Through conducting extensive experiments on four widely recognized benchmarks, our proposed method attains state-of-the-art performance in both classification and segmentation CTTA tasks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_12480v1_6_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>提出了连续测试时间自适应（CTTA），将源预训练模型迁移到不断变化的目标分布中，以解决现实世界的动态问题。现有的CTTA方法主要依靠熵最小化或师生伪标记方案来提取未标记目标领域的知识。然而，在现有的自监督学习方法中，动态数据分布会导致预测错误和伪标签噪声，阻碍了在持续适应过程中有效缓解错误积累和灾难性遗忘问题。为了解决这些问题，我们提出了一种连续的自监督方法，即自适应分布屏蔽自动编码器（ADMA），它增强了目标领域知识的提取，同时减少了分布偏移的积累。具体来说，我们提出了一种分布感知掩蔽（DaM）机制来自适应地对掩蔽位置进行采样，然后在掩蔽目标样本和原始目标样本之间建立一致性约束。此外，对于掩码令牌，我们利用高效的解码器来重建手工制作的特征描述符（例如，面向梯度的直方图），利用其不变特性来增强任务相关的表示。通过在四个公认的基准上进行广泛的实验，我们提出的方法在分类和分割CTTA任务中都达到了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.12480v1" target="_blank">2312.12480v1</a>
                              </td>
                              <td>Adaptive Distribution Masked Autoencoders for Continual Test-Time Adaptation</td>
                              <td>Jiaming Liu</td>
                              <td>2023-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_12480v1_6"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.12480v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="VAD"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2302_06018v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Optimizing Floors in First Price Auctions: an Empirical Study of Yahoo Advertising</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_06018v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_06018v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_06018v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Floors (also known as reserve prices) help publishers to increase the expected revenue of their ad space, which is usually sold via auctions. Floors are defined as the minimum bid that a seller (it can be a publisher or an ad exchange) is willing to accept for the inventory opportunity. In this paper, we present a model to set floors in first price auctions, and discuss the impact of its implementation on Yahoo sites. The model captures important characteristics of the online advertising industry. For instance, some bidders impose restrictions on how ad exchanges can handle data from bidders, conditioning the model choice to set reserve prices. Our solution induces bidders to change their bidding behavior as a response to the floors enclosed in the bid request, helping online publishers to increase their ad revenue.   The outlined methodology has been implemented at Yahoo with remarkable results. The annualized incremental revenue is estimated at +1.3% on Yahoo display inventory, and +2.5% on video ad inventory. These are non-negligible numbers in the multi-million Yahoo ad business.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_06018v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>底价（也称为底价）有助于出版商增加广告空间的预期收入，广告空间通常通过拍卖出售。下限被定义为卖家（可以是出版商或广告交易所）愿意接受的库存机会的最低出价。在本文中，我们提出了一个在首次价格拍卖中设置下限的模型，并讨论了它的实现对雅虎网站的影响。该模型捕捉到了在线广告行业的重要特征。例如，一些竞标者对广告交易所如何处理竞标者的数据施加了限制，将模式选择作为设定底价的条件。我们的解决方案促使投标人改变其投标行为，以响应投标请求中包含的楼层，帮助在线出版商增加广告收入。概述的方法已在雅虎实施，取得了显著的效果。雅虎显示器库存的年化增量收入估计为+1.3%，视频广告库存的年增长率估计为+2.5%。在数百万的雅虎广告业务中，这些数字是不可忽视的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.06018v2" target="_blank">2302.06018v2</a>
                              </td>
                              <td>Optimizing Floors in First Price Auctions: an Empirical Study of Yahoo Advertising</td>
                              <td>Miguel Alcobendas</td>
                              <td>2023-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_06018v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.06018v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_05087v3_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_05087v3_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_05087v3_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_05087v3_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Anomaly Detection (VAD) serves as a pivotal technology in the intelligent surveillance systems, enabling the temporal or spatial identification of anomalous events within videos. While existing reviews predominantly concentrate on conventional unsupervised methods, they often overlook the emergence of weakly-supervised and fully-unsupervised approaches. To address this gap, this survey extends the conventional scope of VAD beyond unsupervised methods, encompassing a broader spectrum termed Generalized Video Anomaly Event Detection (GVAED). By skillfully incorporating recent advancements rooted in diverse assumptions and learning frameworks, this survey introduces an intuitive taxonomy that seamlessly navigates through unsupervised, weakly-supervised, supervised and fully-unsupervised VAD methodologies, elucidating the distinctions and interconnections within these research trajectories. In addition, this survey facilitates prospective researchers by assembling a compilation of research resources, including public datasets, available codebases, programming tools, and pertinent literature. Furthermore, this survey quantitatively assesses model performance, delves into research challenges and directions, and outlines potential avenues for future exploration.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_05087v3_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）是智能监控系统中的一项关键技术，能够在时间或空间上识别视频中的异常事件。虽然现有的综述主要集中在传统的无监督方法上，但它们往往忽视了弱监督和完全无监督方法的出现。为了解决这一差距，这项调查将VAD的传统范围扩展到了无监督方法之外，涵盖了更广泛的范围，称为广义视频异常事件检测（GVAED）。通过巧妙地结合植根于不同假设和学习框架的最新进展，这项调查引入了一种直观的分类法，该分类法可以无缝地浏览无监督、弱监督、有监督和完全无监督的VAD方法，阐明了这些研究轨迹中的区别和相互联系。此外，这项调查通过汇编研究资源，包括公共数据集、可用代码库、编程工具和相关文献，为未来的研究人员提供了便利。此外，这项调查定量评估了模型性能，深入研究了研究挑战和方向，并概述了未来探索的潜在途径。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.05087v3" target="_blank">2302.05087v3</a>
                              </td>
                              <td>Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models</td>
                              <td>Yang Liu</td>
                              <td>2023-02-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_05087v3_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.05087v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/fudanyliu/gvaed" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13551v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13551v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13551v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13551v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Without human annotations, a typical Unsupervised Video Anomaly Detection (UVAD) method needs to train two models that generate pseudo labels for each other. In previous work, the two models are closely entangled with each other, and it is not known how to upgrade their method without modifying their training framework significantly. Second, previous work usually adopts fixed thresholding to obtain pseudo labels, however the user-specified threshold is not reliable which inevitably introduces errors into the training process. To alleviate these two problems, we propose a novel interleaved framework that alternately trains a One-Class Classification (OCC) model and a Weakly-Supervised (WS) model for UVAD. The OCC or WS models in our method can be easily replaced with other OCC or WS models, which facilitates our method to upgrade with the most recent developments in both fields. For handling the fixed thresholding problem, we break through the conventional cognitive boundary and propose a weighted OCC model that can be trained on both normal and abnormal data. We also propose an adaptive mechanism for automatically finding the optimal threshold for the WS model in a loose to strict manner. Experiments demonstrate that the proposed UVAD method outperforms previous approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13551v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在没有人工注释的情况下，典型的无监督视频异常检测（UVAD）方法需要训练两个为彼此生成伪标签的模型。在以前的工作中，这两个模型彼此紧密纠缠，不知道如何在不显著修改训练框架的情况下升级它们的方法。其次，以前的工作通常采用固定的阈值来获得伪标签，但用户指定的阈值是不可靠的，这不可避免地会在训练过程中引入错误。为了缓解这两个问题，我们提出了一种新的交错框架，该框架交替训练UVAD的一类分类（OCC）模型和弱监督（WS）模型。我们方法中的OCC或WS模型可以很容易地被其他OCC或WS模式取代，这有助于我们的方法随着这两个领域的最新发展而升级。为了处理固定阈值问题，我们突破了传统的认知边界，提出了一种可以在正常和异常数据上训练的加权OCC模型。我们还提出了一种自适应机制，用于以宽松到严格的方式自动找到WS模型的最佳阈值。实验表明，所提出的UVAD方法优于以前的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13551v1" target="_blank">2401.13551v1</a>
                              </td>
                              <td>Interleaving One-Class and Weakly-Supervised Models with Adaptive Thresholding for Unsupervised Video Anomaly Detection</td>
                              <td>Yongwei Nie</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13551v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13551v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_14451v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Prompt-Enhanced Context Features for Weakly-Supervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_14451v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_14451v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_14451v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection under weak supervision presents significant challenges, particularly due to the lack of frame-level annotations during training. While prior research has utilized graph convolution networks and self-attention mechanisms alongside multiple instance learning (MIL)-based classification loss to model temporal relations and learn discriminative features, these methods often employ multi-branch architectures to capture local and global dependencies separately, resulting in increased parameters and computational costs. Moreover, the coarse-grained interclass separability provided by the binary constraint of MIL-based loss neglects the fine-grained discriminability within anomalous classes. In response, this paper introduces a weakly supervised anomaly detection framework that focuses on efficient context modeling and enhanced semantic discriminability. We present a Temporal Context Aggregation (TCA) module that captures comprehensive contextual information by reusing the similarity matrix and implementing adaptive fusion. Additionally, we propose a Prompt-Enhanced Learning (PEL) module that integrates semantic priors using knowledge-based prompts to boost the discriminative capacity of context features while ensuring separability between anomaly sub-classes. Extensive experiments validate the effectiveness of our method's components, demonstrating competitive performance with reduced parameters and computational effort on three challenging benchmarks: UCF-Crime, XD-Violence, and ShanghaiTech datasets. Notably, our approach significantly improves the detection accuracy of certain anomaly sub-classes, underscoring its practical value and efficacy. Our code is available at: https://github.com/yujiangpu20/PEL4VAD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_14451v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>弱监督下的视频异常检测带来了重大挑战，特别是由于训练过程中缺乏帧级注释。虽然先前的研究已经利用图卷积网络和自注意机制以及基于多实例学习（MIL）的分类损失来建模时间关系和学习判别特征，但这些方法通常使用多分支架构来分别捕获局部和全局依赖性，导致参数和计算成本增加。此外，基于MIL的损失的二元约束所提供的粗粒度类间可分性忽略了异常类内的细粒度可分辨性。作为回应，本文介绍了一种弱监督异常检测框架，该框架侧重于高效的上下文建模和增强的语义可分辨性。我们提出了一个时间上下文聚合（TCA）模块，该模块通过重用相似度矩阵和实现自适应融合来捕获全面的上下文信息。此外，我们提出了一个提示增强学习（PEL）模块，该模块使用基于知识的提示集成语义先验，以提高上下文特征的判别能力，同时确保异常子类之间的可分性。大量实验验证了我们方法组件的有效性，在三个具有挑战性的基准上展示了具有竞争力的性能，减少了参数和计算工作量：UCF犯罪、XD暴力和ShanghaiTech数据集。值得注意的是，我们的方法显著提高了某些异常子类的检测精度，突出了其实用价值和有效性。我们的代码位于：https://github.com/yujiangpu20/PEL4VAD.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.14451v2" target="_blank">2306.14451v2</a>
                              </td>
                              <td>Learning Prompt-Enhanced Context Features for Weakly-Supervised Video Anomaly Detection</td>
                              <td>Yujiang Pu</td>
                              <td>2023-06-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_14451v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.14451v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yujiangpu20/pel4vad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_00114v4_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_00114v4_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_00114v4_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_00114v4_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The existing methods for video anomaly detection mostly utilize videos containing identifiable facial and appearance-based features. The use of videos with identifiable faces raises privacy concerns, especially when used in a hospital or community-based setting. Appearance-based features can also be sensitive to pixel-based noise, straining the anomaly detection methods to model the changes in the background and making it difficult to focus on the actions of humans in the foreground. Structural information in the form of skeletons describing the human motion in the videos is privacy-protecting and can overcome some of the problems posed by appearance-based features. In this paper, we present a survey of privacy-protecting deep learning anomaly detection methods using skeletons extracted from videos. We present a novel taxonomy of algorithms based on the various learning approaches. We conclude that skeleton-based approaches for anomaly detection can be a plausible privacy-protecting alternative for video anomaly detection. Lastly, we identify major open research questions and provide guidelines to address them.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_00114v4_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>现有的视频异常检测方法大多利用包含可识别的面部和基于外观的特征的视频。使用可识别人脸的视频会引发隐私问题，尤其是在医院或社区环境中使用时。基于外观的特征也可能对基于像素的噪声敏感，这使异常检测方法难以对背景变化进行建模，并使其难以关注前景中人类的行为。视频中描述人体运动的骨架形式的结构信息是隐私保护，可以克服基于外观的特征带来的一些问题。在本文中，我们对使用从视频中提取的骨架的隐私保护深度学习异常检测方法进行了调查。我们提出了一种基于各种学习方法的新的算法分类法。我们得出的结论是，基于骨架的异常检测方法可以成为视频异常检测的一种合理的隐私保护替代方案。最后，我们确定了主要的开放研究问题，并提供了解决这些问题的指导方针。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.00114v4" target="_blank">2301.00114v4</a>
                              </td>
                              <td>Skeletal Video Anomaly Detection using Deep Learning: Survey, Challenges and Future Directions</td>
                              <td>Pratik K. Mishra</td>
                              <td>2022-12-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_00114v4_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.00114v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_05702v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Video Anomaly Detection and Explanation via Large Language Models</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_05702v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_05702v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_05702v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Anomaly Detection (VAD) aims to localize abnormal events on the timeline of long-range surveillance videos. Anomaly-scoring-based methods have been prevailing for years but suffer from the high complexity of thresholding and low explanability of detection results. In this paper, we conduct pioneer research on equipping video-based large language models (VLLMs) in the framework of VAD, making the VAD model free from thresholds and able to explain the reasons for the detected anomalies. We introduce a novel network module Long-Term Context (LTC) to mitigate the incapability of VLLMs in long-range context modeling. We design a three-phase training method to improve the efficiency of fine-tuning VLLMs by substantially minimizing the requirements for VAD data and lowering the costs of annotating instruction-tuning data. Our trained model achieves the top performance on the anomaly videos of the UCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\% and +4.96\%, respectively. More impressively, our approach can provide textual explanations for detected anomalies.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_05702v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）旨在定位远程监控视频时间线上的异常事件。基于异常评分的方法已经盛行多年，但存在阈值处理的高复杂性和检测结果的低可解释性的问题。在本文中，我们对在VAD框架下配备基于视频的大型语言模型（VLLMs）进行了开创性的研究，使VAD模型不受阈值的限制，能够解释检测到异常的原因。我们引入了一种新的网络模块长期上下文（LTC），以减轻VLLM在长期上下文建模中的无能。我们设计了一种三阶段训练方法，通过显著降低对VAD数据的要求和降低注释指令调整数据的成本，来提高VLLM的微调效率。我们训练的模型在UCF犯罪和TAD基准的异常视频上实现了最高性能，AUC分别提高了+3.86%和+4.96%。更令人印象深刻的是，我们的方法可以为检测到的异常提供文本解释。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.05702v1" target="_blank">2401.05702v1</a>
                              </td>
                              <td>Video Anomaly Detection and Explanation via Large Language Models</td>
                              <td>Hui Lv</td>
                              <td>2024-01-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_05702v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.05702v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04942v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04942v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04942v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04942v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In the past several years, road anomaly segmentation is actively explored in the academia and drawing growing attention in the industry. The rationale behind is straightforward: if the autonomous car can brake before hitting an anomalous object, safety is promoted. However, this rationale naturally calls for a temporally informed setting while existing methods and benchmarks are designed in an unrealistic frame-wise manner. To bridge this gap, we contribute the first video anomaly segmentation dataset for autonomous driving. Since placing various anomalous objects on busy roads and annotating them in every frame are dangerous and expensive, we resort to synthetic data. To improve the relevance of this synthetic dataset to real-world applications, we train a generative adversarial network conditioned on rendering G-buffers for photorealism enhancement. Our dataset consists of 120,000 high-resolution frames at a 60 FPS framerate, as recorded in 7 different towns. As an initial benchmarking, we provide baselines using latest supervised and unsupervised road anomaly segmentation methods. Apart from conventional ones, we focus on two new metrics: temporal consistency and latencyaware streaming accuracy. We believe the latter is valuable as it measures whether an anomaly segmentation algorithm can truly prevent a car from crashing in a temporally informed setting.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04942v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在过去的几年里，道路异常分割在学术界得到了积极的探索，并引起了业界越来越多的关注。背后的理由很简单：如果自动驾驶汽车能够在撞上异常物体之前刹车，那么安全性就会得到提高。然而，这一原理自然需要一个时间知情的设置，而现有的方法和基准是以不切实际的框架方式设计的。为了弥补这一差距，我们贡献了第一个用于自动驾驶的视频异常分割数据集。由于在繁忙的道路上放置各种异常对象并在每一帧中对其进行注释既危险又昂贵，因此我们求助于合成数据。为了提高这个合成数据集与现实世界应用的相关性，我们训练了一个生成对抗性网络，该网络以渲染G缓冲区为条件，用于增强真实感。我们的数据集由120000个高分辨率帧组成，帧速率为60 FPS，记录在7个不同的城镇。作为初始基准，我们使用最新的有监督和无监督道路异常分割方法提供基线。除了传统的指标外，我们还关注两个新的指标：时间一致性和感知延迟的流媒体准确性。我们认为后者是有价值的，因为它可以衡量异常分割算法是否能够真正防止汽车在时间知情的情况下发生碰撞。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04942v1" target="_blank">2401.04942v1</a>
                              </td>
                              <td>Latency-aware Road Anomaly Segmentation in Videos: A Photorealistic Dataset and New Metrics</td>
                              <td>Beiwen Tian</td>
                              <td>2024-01-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04942v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04942v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_11681v3_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_11681v3_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_11681v3_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_11681v3_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The recent contrastive language-image pre-training (CLIP) model has shown great success in a wide range of image-level tasks, revealing remarkable ability for learning powerful visual representations with rich semantics. An open and worthwhile problem is efficiently adapting such a strong model to the video domain and designing a robust video anomaly detector. In this work, we propose VadCLIP, a new paradigm for weakly supervised video anomaly detection (WSVAD) by leveraging the frozen CLIP model directly without any pre-training and fine-tuning process. Unlike current works that directly feed extracted features into the weakly supervised classifier for frame-level binary classification, VadCLIP makes full use of fine-grained associations between vision and language on the strength of CLIP and involves dual branch. One branch simply utilizes visual features for coarse-grained binary classification, while the other fully leverages the fine-grained language-image alignment. With the benefit of dual branch, VadCLIP achieves both coarse-grained and fine-grained video anomaly detection by transferring pre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments on two commonly-used benchmarks, demonstrating that VadCLIP achieves the best performance on both coarse-grained and fine-grained WSVAD, surpassing the state-of-the-art methods by a large margin. Specifically, VadCLIP achieves 84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and features are released at https://github.com/nwpu-zxr/VadCLIP.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_11681v3_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>最近的对比语言图像预训练（CLIP）模型在广泛的图像级任务中取得了巨大成功，显示出学习具有丰富语义的强大视觉表示的非凡能力。一个开放的和有价值的问题是有效地将这样一个强模型应用于视频域，并设计一个鲁棒的视频异常检测器。在这项工作中，我们提出了VadCLIP，这是一种弱监督视频异常检测（WSVAD）的新范式，通过直接利用冻结的CLIP模型，而无需任何预训练和微调过程。与目前直接将提取的特征输入弱监督分类器进行帧级二值分类的工作不同，VadCLIP充分利用了视觉和语言之间的细粒度关联，并涉及双分支。一个分支简单地利用视觉特征进行粗粒度的二进制分类，而另一个分支则充分利用细粒度的语言图像对齐。借助双分支的优势，VadCLIP通过将预先训练的知识从CLIP转移到WSVAD任务中，实现了粗粒度和细粒度视频异常检测。我们在两个常用的基准上进行了广泛的实验，证明VadCLIP在粗粒度和细粒度的WSVAD上都实现了最佳性能，大大超过了最先进的方法。具体而言，VadCLIP在XD暴力和UCF犯罪方面分别达到84.51%的AP和88.02%的AUC。代码和功能发布于https://github.com/nwpu-zxr/VadCLIP.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.11681v3" target="_blank">2308.11681v3</a>
                              </td>
                              <td>VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection</td>
                              <td>Peng Wu</td>
                              <td>2023-08-22</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_11681v3_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.11681v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/nwpu-zxr/vadclip" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_01764v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Dynamic Erasing Network Based on Multi-Scale Temporal Features for Weakly Supervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_01764v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_01764v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_01764v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The goal of weakly supervised video anomaly detection is to learn a detection model using only video-level labeled data. However, prior studies typically divide videos into fixed-length segments without considering the complexity or duration of anomalies. Moreover, these studies usually just detect the most abnormal segments, potentially overlooking the completeness of anomalies. To address these limitations, we propose a Dynamic Erasing Network (DE-Net) for weakly supervised video anomaly detection, which learns multi-scale temporal features. Specifically, to handle duration variations of abnormal events, we first propose a multi-scale temporal modeling module, capable of extracting features from segments of varying lengths and capturing both local and global visual information across different temporal scales. Then, we design a dynamic erasing strategy, which dynamically assesses the completeness of the detected anomalies and erases prominent abnormal segments in order to encourage the model to discover gentle abnormal segments in a video. The proposed method obtains favorable performance compared to several state-of-the-art approaches on three datasets: XD-Violence, TAD, and UCF-Crime. Code will be made available at https://github.com/ArielZc/DE-Net.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_01764v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>弱监督视频异常检测的目标是仅使用视频级别标记的数据来学习检测模型。然而，先前的研究通常将视频划分为固定长度的片段，而不考虑异常的复杂性或持续时间。此外，这些研究通常只检测最不正常的部分，可能忽略了异常的完整性。为了解决这些限制，我们提出了一种用于弱监督视频异常检测的动态擦除网络（DE Net），该网络学习多尺度时间特征。具体来说，为了处理异常事件的持续时间变化，我们首先提出了一个多尺度时间建模模块，能够从不同长度的片段中提取特征，并在不同的时间尺度上捕捉局部和全局视觉信息。然后，我们设计了一种动态擦除策略，该策略动态评估检测到的异常的完整性，并擦除突出的异常片段，以鼓励模型在视频中发现温和的异常片段。在三个数据集上，与几种最先进的方法相比，所提出的方法获得了良好的性能：XD暴力、TAD和UCF犯罪。代码将在提供https://github.com/ArielZc/DE-Net.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.01764v1" target="_blank">2312.01764v1</a>
                              </td>
                              <td>Dynamic Erasing Network Based on Multi-Scale Temporal Features for Weakly Supervised Video Anomaly Detection</td>
                              <td>Chen Zhang</td>
                              <td>2023-12-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_01764v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.01764v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/arielzc/de-net" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_08615v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Online Anomaly Detection over Live Social Video Streaming</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_08615v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_08615v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_08615v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Social video anomaly is an observation in video streams that does not conform to a common pattern of dataset's behaviour. Social video anomaly detection plays a critical role in applications from e-commerce to e-learning. Traditionally, anomaly detection techniques are applied to find anomalies in video broadcasting. However, they neglect the live social video streams which contain interactive talk, speech, or lecture with audience. In this paper, we propose a generic framework for effectively online detecting Anomalies Over social Video LIve Streaming (AOVLIS). Specifically, we propose a novel deep neural network model called Coupling Long Short-Term Memory (CLSTM) that adaptively captures the history behaviours of the presenters and audience, and their mutual interactions to predict their behaviour at next time point over streams. Then we well integrate the CLSTM with a decoder layer, and propose a new reconstruction error-based scoring function $RE_{IA}$ to calculate the anomaly score of each video segment for anomaly detection. After that, we propose a novel model update scheme that incrementally maintains CLSTM and decoder. Moreover, we design a novel upper bound and ADaptive Optimisation Strategy (ADOS) for improving the efficiency of our solution. Extensive experiments are conducted to prove the superiority of AOVLIS.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_08615v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>社交视频异常是指在视频流中观察到的不符合数据集行为的常见模式的现象。社交视频异常检测在从电子商务到电子学习的应用中发挥着关键作用。传统上，异常检测技术被应用于视频广播中的异常发现。然而，他们忽略了包含与观众互动谈话、演讲或讲座的实时社交视频流。在本文中，我们提出了一个通用框架，用于有效地在线检测社交视频直播中的异常（AOVLIS）。具体来说，我们提出了一种新的深度神经网络模型，称为耦合长短期记忆（CLSTM），该模型自适应地捕捉演讲者和观众的历史行为，以及他们的相互互动，以预测他们在流上下一个时间点的行为。然后，我们将CLSTM与解码器层很好地集成在一起，并提出了一种新的基于重建误差的评分函数$RE_{IA}$来计算每个视频片段的异常分数，用于异常检测。之后，我们提出了一种新的模型更新方案，该方案增量地维护CLSTM和解码器。此外，为了提高解决方案的效率，我们设计了一种新的上界自适应优化策略（ADOS）。通过大量实验验证了AOVLIS的优越性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.08615v1" target="_blank">2401.08615v1</a>
                              </td>
                              <td>Online Anomaly Detection over Live Social Video Streaming</td>
                              <td>Chengkun He</td>
                              <td>2023-12-01</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_08615v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.08615v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_16514v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Video Anomaly Detection via Spatio-Temporal Pseudo-Anomaly Generation : A Unified Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_16514v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_16514v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_16514v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Anomaly Detection (VAD) is an open-set recognition task, which is usually formulated as a one-class classification (OCC) problem, where training data is comprised of videos with normal instances while test data contains both normal and anomalous instances. Recent works have investigated the creation of pseudo-anomalies (PAs) using only the normal data and making strong assumptions about real-world anomalies with regards to abnormality of objects and speed of motion to inject prior information about anomalies in an autoencoder (AE) based reconstruction model during training. This work proposes a novel method for generating generic spatio-temporal PAs by inpainting a masked out region of an image using a pre-trained Latent Diffusion Model and further perturbing the optical flow using mixup to emulate spatio-temporal distortions in the data. In addition, we present a simple unified framework to detect real-world anomalies under the OCC setting by learning three types of anomaly indicators, namely reconstruction quality, temporal irregularity and semantic inconsistency. Extensive experiments on four VAD benchmark datasets namely Ped2, Avenue, ShanghaiTech and UBnormal demonstrate that our method performs on par with other existing state-of-the-art PAs generation and reconstruction based methods under the OCC setting. Our analysis also examines the transferability and generalisation of PAs across these datasets, offering valuable insights by identifying real-world anomalies through PAs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_16514v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）是一种开放集识别任务，通常被公式化为一类分类（OCC）问题，其中训练数据由具有正常实例的视频组成，而测试数据同时包含正常和异常实例。最近的工作已经研究了仅使用正常数据创建伪异常（PA），并对关于物体异常和运动速度的真实世界异常做出强有力的假设，以在训练期间在基于自动编码器（AE）的重建模型中注入关于异常的先验信息。这项工作提出了一种新的方法来生成通用的时空PA，方法是使用预先训练的潜在扩散模型修复图像的遮蔽区域，并使用混合来进一步扰动光流，以模拟数据中的时空失真。此外，我们提出了一个简单的统一框架，通过学习三种类型的异常指标，即重建质量、时间不规则性和语义不一致性，来检测OCC设置下的真实世界异常。在Ped2、Avenue、ShanghaiTech和UBnormal四个VAD基准数据集上进行的大量实验表明，在OCC环境下，我们的方法与其他现有最先进的基于PA生成和重建的方法不相上下。我们的分析还考察了PA在这些数据集中的可转移性和通用性，通过PA识别现实世界中的异常现象，提供了有价值的见解。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.16514v1" target="_blank">2311.16514v1</a>
                              </td>
                              <td>Video Anomaly Detection via Spatio-Temporal Pseudo-Anomaly Generation : A Unified Approach</td>
                              <td>Ayush K. Rai</td>
                              <td>2023-11-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_16514v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.16514v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_15367v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">BatchNorm-based Weakly Supervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_15367v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_15367v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_15367v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In weakly supervised video anomaly detection (WVAD), where only video-level labels indicating the presence or absence of abnormal events are available, the primary challenge arises from the inherent ambiguity in temporal annotations of abnormal occurrences. Inspired by the statistical insight that temporal features of abnormal events often exhibit outlier characteristics, we propose a novel method, BN-WVAD, which incorporates BatchNorm into WVAD. In the proposed BN-WVAD, we leverage the Divergence of Feature from Mean vector (DFM) of BatchNorm as a reliable abnormality criterion to discern potential abnormal snippets in abnormal videos. The proposed DFM criterion is also discriminative for anomaly recognition and more resilient to label noise, serving as the additional anomaly score to amend the prediction of the anomaly classifier that is susceptible to noisy labels. Moreover, a batch-level selection strategy is devised to filter more abnormal snippets in videos where more abnormal events occur. The proposed BN-WVAD model demonstrates state-of-the-art performance on UCF-Crime with an AUC of 87.24%, and XD-Violence, where AP reaches up to 84.93%. Our code implementation is accessible at https://github.com/cool-xuan/BN-WVAD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_15367v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在弱监督视频异常检测（WVAD）中，只有指示异常事件存在或不存在的视频级别标签可用，主要挑战来自异常发生的时间注释中固有的模糊性。受异常事件的时间特征通常表现出异常值特征这一统计见解的启发，我们提出了一种新的方法，即BN-WVAD，该方法将BatchNorm合并到WVAD中。在所提出的BN-WVAD中，我们利用BatchNorm的特征与均值向量的偏差（DFM）作为可靠的异常标准来识别异常视频中潜在的异常片段。所提出的DFM准则对于异常识别也是有判别力的，并且对标签噪声更有弹性，作为额外的异常分数来修正对噪声标签敏感的异常分类器的预测。此外，还设计了一种批处理级别的选择策略，以过滤视频中发生更多异常事件的更多异常片段。所提出的BN-WVAD模型在UCF犯罪和XD暴力方面表现出了最先进的性能，AUC为87.24%，AP高达84.93%。我们的代码实现可访问https://github.com/cool-xuan/BN-WVAD.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.15367v1" target="_blank">2311.15367v1</a>
                              </td>
                              <td>BatchNorm-based Weakly Supervised Video Anomaly Detection</td>
                              <td>Yixuan Zhou</td>
                              <td>2023-11-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_15367v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.15367v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/cool-xuan/bn-wvad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_14095v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Video Anomaly Detection using GAN</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_14095v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_14095v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_14095v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Accounting for the increased concern for public safety, automatic abnormal event detection and recognition in a surveillance scene is crucial. It is a current open study subject because of its intricacy and utility. The identification of aberrant events automatically, it's a difficult undertaking because everyone's idea of abnormality is different. A typical occurrence in one circumstance could be seen as aberrant in another. Automatic anomaly identification becomes particularly challenging in the surveillance footage with a large crowd due to congestion and high occlusion. With the use of machine learning techniques, this thesis study aims to offer the solution for this use case so that human resources won't be required to keep an eye out for any unusual activity in the surveillance system records. We have developed a novel generative adversarial network (GAN) based anomaly detection model. This model is trained such that it learns together about constructing a high dimensional picture space and determining the latent space from the video's context. The generator uses a residual Autoencoder architecture made up of a multi-stage channel attention-based decoder and a two-stream, deep convolutional encoder that can realise both spatial and temporal data. We have also offered a technique for refining the GAN model that reduces training time while also generalising the model by utilising transfer learning between datasets. Using a variety of assessment measures, we compare our model to the current state-of-the-art techniques on four benchmark datasets. The empirical findings indicate that, in comparison to existing techniques, our network performs favourably on all datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_14095v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>考虑到人们对公共安全日益关注，监控场景中的自动异常事件检测和识别至关重要。由于其复杂性和实用性，它是当前一个开放的研究课题。自动识别异常事件是一项困难的工作，因为每个人对异常的想法都不一样。一种情况下的典型事件在另一种情况中可能被视为异常。由于拥挤和高度遮挡，在人群众多的监控录像中，自动异常识别变得特别具有挑战性。通过使用机器学习技术，本论文研究旨在为该用例提供解决方案，这样就不需要人力资源来关注监控系统记录中的任何异常活动。我们开发了一种新的基于生成对抗性网络（GAN）的异常检测模型。对该模型进行训练，使其共同学习如何构建高维图片空间并根据视频的上下文确定潜在空间。生成器使用残差自动编码器架构，该架构由基于多级通道注意力的解码器和可实现空间和时间数据的双流深度卷积编码器组成。我们还提供了一种改进GAN模型的技术，该技术减少了训练时间，同时还通过利用数据集之间的迁移学习来推广模型。使用各种评估措施，我们在四个基准数据集上将我们的模型与当前最先进的技术进行了比较。实证结果表明，与现有技术相比，我们的网络在所有数据集上都表现良好。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.14095v1" target="_blank">2311.14095v1</a>
                              </td>
                              <td>Video Anomaly Detection using GAN</td>
                              <td>Anikeit Sethi</td>
                              <td>2023-11-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_14095v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.14095v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_07042v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Open-Vocabulary Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_07042v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_07042v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_07042v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection (VAD) with weak supervision has achieved remarkable performance in utilizing video-level labels to discriminate whether a video frame is normal or abnormal. However, current approaches are inherently limited to a closed-set setting and may struggle in open-world applications where there can be anomaly categories in the test data unseen during training. A few recent studies attempt to tackle a more realistic setting, open-set VAD, which aims to detect unseen anomalies given seen anomalies and normal videos. However, such a setting focuses on predicting frame anomaly scores, having no ability to recognize the specific categories of anomalies, despite the fact that this ability is essential for building more informed video surveillance systems. This paper takes a step further and explores open-vocabulary video anomaly detection (OVVAD), in which we aim to leverage pre-trained large models to detect and categorize seen and unseen anomalies. To this end, we propose a model that decouples OVVAD into two mutually complementary tasks -- class-agnostic detection and class-specific classification -- and jointly optimizes both tasks. Particularly, we devise a semantic knowledge injection module to introduce semantic knowledge from large language models for the detection task, and design a novel anomaly synthesis module to generate pseudo unseen anomaly videos with the help of large vision generation models for the classification task. These semantic knowledge and synthesis anomalies substantially extend our model's capability in detecting and categorizing a variety of seen and unseen anomalies. Extensive experiments on three widely-used benchmarks demonstrate our model achieves state-of-the-art performance on OVVAD task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_07042v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>弱监督的视频异常检测（VAD）在利用视频级别标签来区分视频帧是正常还是异常方面取得了显著的性能。然而，当前的方法本质上局限于封闭集设置，并且在开放世界应用程序中可能会遇到困难，因为在开放世界的应用程序中，训练过程中可能会发现测试数据中的异常类别。最近的一些研究试图解决一种更现实的设置，即开集VAD，该设置旨在检测看到的异常和正常视频中看不见的异常。然而，这种设置专注于预测帧异常分数，而没有能力识别特定类别的异常，尽管这种能力对于构建更知情的视频监控系统至关重要。本文进一步探索了开放词汇视频异常检测（OVVAD），旨在利用预先训练的大型模型来检测和分类可见和不可见的异常。为此，我们提出了一个模型，将OVVAD解耦为两个相互补充的任务——类不可知检测和类特定分类——并联合优化这两个任务。特别地，我们设计了一个语义知识注入模块来引入来自大型语言模型的语义知识用于检测任务，并设计了一种新的异常合成模块来借助于用于分类任务的大型视觉生成模型来生成伪看不见的异常视频。这些语义知识和合成异常大大扩展了我们的模型检测和分类各种可见和不可见异常的能力。在三个广泛使用的基准上进行的大量实验表明，我们的模型在OVVAD任务上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.07042v2" target="_blank">2311.07042v2</a>
                              </td>
                              <td>Open-Vocabulary Video Anomaly Detection</td>
                              <td>Peng Wu</td>
                              <td>2023-11-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_07042v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.07042v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_04351v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Deep Learning Approach to Video Anomaly Detection using Convolutional Autoencoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_04351v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_04351v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_04351v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this research we propose a deep learning approach for detecting anomalies in videos using convolutional autoencoder and decoder neural networks on the UCSD dataset.Our method utilizes a convolutional autoencoder to learn the spatiotemporal patterns of normal videos and then compares each frame of a test video to this learned representation. We evaluated our approach on the UCSD dataset and achieved an overall accuracy of 99.35% on the Ped1 dataset and 99.77% on the Ped2 dataset, demonstrating the effectiveness of our method for detecting anomalies in surveillance videos. The results show that our method outperforms other state-of-the-art methods, and it can be used in real-world applications for video anomaly detection.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_04351v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项研究中，我们提出了一种深度学习方法，用于在UCSD数据集上使用卷积自动编码器和解码器神经网络检测视频中的异常。我们的方法利用卷积自动编码器来学习正常视频的时空模式，然后将测试视频的每一帧与该学习的表示进行比较。我们在UCSD数据集上评估了我们的方法，在Ped1数据集和Ped2数据集上分别获得了99.35%和99.77%的总体准确率，证明了我们检测监控视频异常的方法的有效性。结果表明，我们的方法优于其他最先进的方法，可以在现实世界中用于视频异常检测。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.04351v1" target="_blank">2311.04351v1</a>
                              </td>
                              <td>A Deep Learning Approach to Video Anomaly Detection using Convolutional Autoencoders</td>
                              <td>Gopikrishna Pavuluri</td>
                              <td>2023-11-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_04351v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.04351v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2311_01851v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Holistic Representation Learning for Multitask Trajectory Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2311_01851v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2311_01851v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2311_01851v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection deals with the recognition of abnormal events in videos. Apart from the visual signal, video anomaly detection has also been addressed with the use of skeleton sequences. We propose a holistic representation of skeleton trajectories to learn expected motions across segments at different times. Our approach uses multitask learning to reconstruct any continuous unobserved temporal segment of the trajectory allowing the extrapolation of past or future segments and the interpolation of in-between segments. We use an end-to-end attention-based encoder-decoder. We encode temporally occluded trajectories, jointly learn latent representations of the occluded segments, and reconstruct trajectories based on expected motions across different temporal segments. Extensive experiments on three trajectory-based video anomaly detection datasets show the advantages and effectiveness of our approach with state-of-the-art results on anomaly detection in skeleton trajectories.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2311_01851v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测处理的是对视频中异常事件的识别。除了视觉信号，视频异常检测也已通过使用骨架序列来解决。我们提出了骨架轨迹的整体表示，以学习不同时间跨片段的预期运动。我们的方法使用多任务学习来重建轨迹的任何连续的未观察到的时间段，允许对过去或未来的段进行外推，并对段之间的段进行插值。我们使用端到端的基于注意力的编码器-解码器。我们对时间上被遮挡的轨迹进行编码，共同学习被遮挡片段的潜在表示，并基于不同时间片段上的预期运动重建轨迹。在三个基于轨迹的视频异常检测数据集上进行的大量实验表明了我们的方法的优势和有效性，在骨架轨迹中的异常检测方面取得了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2311.01851v1" target="_blank">2311.01851v1</a>
                              </td>
                              <td>Holistic Representation Learning for Multitask Trajectory Anomaly Detection</td>
                              <td>Alexandros Stergiou</td>
                              <td>2023-11-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2311_01851v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2311.01851v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/alexandrosstergiou/TrajREC" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_17650v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_17650v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_17650v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_17650v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Detection of anomalous events in videos is an important problem in applications such as surveillance. Video anomaly detection (VAD) is well-studied in the one-class classification (OCC) and weakly supervised (WS) settings. However, fully unsupervised (US) video anomaly detection methods, which learn a complete system without any annotation or human supervision, have not been explored in depth. This is because the lack of any ground truth annotations significantly increases the magnitude of the VAD challenge. To address this challenge, we propose a simple-but-effective two-stage pseudo-label generation framework that produces segment-level (normal/anomaly) pseudo-labels, which can be further used to train a segment-level anomaly detector in a supervised manner. The proposed coarse-to-fine pseudo-label (C2FPL) generator employs carefully-designed hierarchical divisive clustering and statistical hypothesis testing to identify anomalous video segments from a set of completely unlabeled videos. The trained anomaly detector can be directly applied on segments of an unseen test video to obtain segment-level, and subsequently, frame-level anomaly predictions. Extensive studies on two large-scale public-domain datasets, UCF-Crime and XD-Violence, demonstrate that the proposed unsupervised approach achieves superior performance compared to all existing OCC and US methods , while yielding comparable performance to the state-of-the-art WS methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_17650v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频中异常事件的检测是监控等应用中的一个重要问题。视频异常检测（VAD）在一类分类（OCC）和弱监督（WS）设置中得到了很好的研究。然而，完全无监督（US）视频异常检测方法尚未深入探索，该方法在没有任何注释或人工监督的情况下学习完整的系统。这是因为缺乏任何基本事实注释显著增加了VAD挑战的规模。为了应对这一挑战，我们提出了一种简单但有效的两阶段伪标签生成框架，该框架生成分段级（正常/异常）伪标签，该伪标签可进一步用于以监督的方式训练分段级异常检测器。所提出的粗到细伪标签（C2FPL）生成器采用精心设计的分层分裂聚类和统计假设检验来从一组完全未标记的视频中识别异常视频片段。经过训练的异常检测器可以直接应用于看不见的测试视频的片段，以获得片段级别的异常预测，并随后获得帧级别的异常预报。对UCF犯罪和XD暴力这两个大规模公共领域数据集的广泛研究表明，与所有现有的OCC和US方法相比，所提出的无监督方法实现了优越的性能，同时产生了与最先进的WS方法相当的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.17650v1" target="_blank">2310.17650v1</a>
                              </td>
                              <td>A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection</td>
                              <td>Anas Al-lahham</td>
                              <td>2023-10-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_17650v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.17650v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/anasemad11/c2fpl" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_05330v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A Lightweight Video Anomaly Detection Model with Weak Supervision and Adaptive Instance Selection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_05330v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_05330v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_05330v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection is to determine whether there are any abnormal events, behaviors or objects in a given video, which enables effective and intelligent public safety management. As video anomaly labeling is both time-consuming and expensive, most existing works employ unsupervised or weakly supervised learning methods. This paper focuses on weakly supervised video anomaly detection, in which the training videos are labeled whether or not they contain any anomalies, but there is no information about which frames the anomalies are located. However, the uncertainty of weakly labeled data and the large model size prevent existing methods from wide deployment in real scenarios, especially the resource-limit situations such as edge-computing. In this paper, we develop a lightweight video anomaly detection model. On the one hand, we propose an adaptive instance selection strategy, which is based on the model's current status to select confident instances, thereby mitigating the uncertainty of weakly labeled data and subsequently promoting the model's performance. On the other hand, we design a lightweight multi-level temporal correlation attention module and an hourglass-shaped fully connected layer to construct the model, which can reduce the model parameters to only 0.56\% of the existing methods (e.g. RTFM). Our extensive experiments on two public datasets UCF-Crime and ShanghaiTech show that our model can achieve comparable or even superior AUC score compared to the state-of-the-art methods, with a significantly reduced number of model parameters.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_05330v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测是确定给定视频中是否存在任何异常事件、行为或对象，从而实现有效和智能的公共安全管理。由于视频异常标记既耗时又昂贵，现有的大多数工作都采用了无监督或弱监督的学习方法。本文主要研究弱监督视频异常检测，即训练视频被标记为是否包含任何异常，但没有关于异常位于哪一帧的信息。然而，弱标记数据的不确定性和较大的模型规模阻碍了现有方法在真实场景中的广泛部署，尤其是在边缘计算等资源受限的情况下。在本文中，我们开发了一个轻量级的视频异常检测模型。一方面，我们提出了一种自适应实例选择策略，该策略基于模型的当前状态来选择有信心的实例，从而减轻弱标记数据的不确定性，从而提高模型的性能。另一方面，我们设计了一个轻量级的多级时间相关性注意力模块和一个沙漏形的全连接层来构建模型，这可以将模型参数减少到现有方法（如RTFM）的0.56\%。我们在UCF-Crime和ShanghaiTech两个公共数据集上进行的广泛实验表明，与最先进的方法相比，我们的模型可以获得相当甚至更高的AUC分数，同时显著减少了模型参数的数量。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.05330v1" target="_blank">2310.05330v1</a>
                              </td>
                              <td>A Lightweight Video Anomaly Detection Model with Weak Supervision and Adaptive Instance Selection</td>
                              <td>Yang Wang</td>
                              <td>2023-10-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_05330v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.05330v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_02835v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Delving into CLIP latent space for Video Anomaly Recognition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_02835v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_02835v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_02835v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We tackle the complex problem of detecting and recognising anomalies in surveillance videos at the frame level, utilising only video-level supervision. We introduce the novel method AnomalyCLIP, the first to combine Large Language and Vision (LLV) models, such as CLIP, with multiple instance learning for joint video anomaly detection and classification. Our approach specifically involves manipulating the latent CLIP feature space to identify the normal event subspace, which in turn allows us to effectively learn text-driven directions for abnormal events. When anomalous frames are projected onto these directions, they exhibit a large feature magnitude if they belong to a particular class. We also introduce a computationally efficient Transformer architecture to model short- and long-term temporal dependencies between frames, ultimately producing the final anomaly score and class prediction probabilities. We compare AnomalyCLIP against state-of-the-art methods considering three major anomaly detection benchmarks, i.e. ShanghaiTech, UCF-Crime, and XD-Violence, and empirically show that it outperforms baselines in recognising video anomalies.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_02835v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们仅利用视频级监控，解决在帧级检测和识别监控视频异常的复杂问题。我们介绍了一种新的方法AnomalyCLIP，它首次将大型语言和视觉（LLV）模型（如CLIP）与多实例学习相结合，用于联合视频异常检测和分类。我们的方法特别涉及操纵潜在的CLIP特征空间来识别正常事件子空间，这反过来又使我们能够有效地学习异常事件的文本驱动方向。当异常帧被投影到这些方向上时，如果它们属于特定的类别，它们就会表现出大的特征量。我们还引入了一种计算高效的Transformer架构来对帧之间的短期和长期时间相关性进行建模，最终产生最终的异常分数和类预测概率。我们将AnomalyCLIP与考虑三个主要异常检测基准（即ShanghaiTech、UCF Crime和XD Violence）的最先进方法进行了比较，并从经验上表明，它在识别视频异常方面优于基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.02835v1" target="_blank">2310.02835v1</a>
                              </td>
                              <td>Delving into CLIP latent space for Video Anomaly Recognition</td>
                              <td>Luca Zanella</td>
                              <td>2023-10-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_02835v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.02835v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/luca-zanella-dvl/AnomalyCLIP" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_14622v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Divide and Conquer in Video Anomaly Detection: A Comprehensive Review and New Approach</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_14622v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_14622v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_14622v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection is a complex task, and the principle of "divide and conquer" is often regarded as an effective approach to tackling intricate issues. It's noteworthy that recent methods in video anomaly detection have revealed the application of the divide and conquer philosophy (albeit with distinct perspectives from traditional usage), yielding impressive outcomes. This paper systematically reviews these literatures from six dimensions, aiming to enhance the use of the divide and conquer strategy in video anomaly detection. Furthermore, based on the insights gained from this review, a novel approach is presented, which integrates human skeletal frameworks with video data analysis techniques. This method achieves state-of-the-art performance on the ShanghaiTech dataset, surpassing all existing advanced methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_14622v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测是一项复杂的任务，“分而治之”原则通常被认为是解决复杂问题的有效方法。值得注意的是，最近的视频异常检测方法揭示了分而治之哲学的应用（尽管与传统用法有着不同的视角），产生了令人印象深刻的结果。本文从六个维度系统地回顾了这些文献，旨在加强分治策略在视频异常检测中的应用。此外，基于这篇综述中获得的见解，提出了一种新的方法，将人类骨骼框架与视频数据分析技术相结合。该方法在ShanghaiTech数据集上实现了最先进的性能，超过了所有现有的先进方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.14622v2" target="_blank">2309.14622v2</a>
                              </td>
                              <td>Divide and Conquer in Video Anomaly Detection: A Comprehensive Review and New Approach</td>
                              <td>Jian Xiao</td>
                              <td>2023-09-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_14622v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.14622v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/XiaoJian923/Divide-and-Conquer" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2310_01904v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Beyond the Benchmark: Detecting Diverse Anomalies in Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2310_01904v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2310_01904v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2310_01904v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Anomaly Detection (VAD) plays a crucial role in modern surveillance systems, aiming to identify various anomalies in real-world situations. However, current benchmark datasets predominantly emphasize simple, single-frame anomalies such as novel object detection. This narrow focus restricts the advancement of VAD models. In this research, we advocate for an expansion of VAD investigations to encompass intricate anomalies that extend beyond conventional benchmark boundaries. To facilitate this, we introduce two datasets, HMDB-AD and HMDB-Violence, to challenge models with diverse action-based anomalies. These datasets are derived from the HMDB51 action recognition dataset. We further present Multi-Frame Anomaly Detection (MFAD), a novel method built upon the AI-VAD framework. AI-VAD utilizes single-frame features such as pose estimation and deep image encoding, and two-frame features such as object velocity. They then apply a density estimation algorithm to compute anomaly scores. To address complex multi-frame anomalies, we add a deep video encoding features capturing long-range temporal dependencies, and logistic regression to enhance final score calculation. Experimental results confirm our assumptions, highlighting existing models limitations with new anomaly types. MFAD excels in both simple and complex anomaly detection scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2310_01904v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）在现代监控系统中发挥着至关重要的作用，旨在识别现实世界中的各种异常。然而，当前的基准数据集主要强调简单的单帧异常，如新颖的对象检测。这种狭隘的关注限制了VAD模型的发展。在这项研究中，我们主张扩大VAD调查，以涵盖超出传统基准边界的复杂异常。为了促进这一点，我们引入了两个数据集，HMDB-AD和HMDB-Volence，以挑战具有不同基于行动的异常的模型。这些数据集源自HMDB51动作识别数据集。我们进一步提出了多帧异常检测（MFAD），这是一种建立在AI-VAD框架上的新方法。AI-VAD利用了单帧特征，如姿态估计和深度图像编码，以及两帧特征，例如物体速度。然后，他们应用密度估计算法来计算异常分数。为了解决复杂的多帧异常，我们添加了深度视频编码特征，捕获长时间依赖性，并添加了逻辑回归以增强最终得分计算。实验结果证实了我们的假设，突出了现有模型对新异常类型的限制。MFAD擅长于简单和复杂的异常检测场景。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2310.01904v1" target="_blank">2310.01904v1</a>
                              </td>
                              <td>Beyond the Benchmark: Detecting Diverse Anomalies in Videos</td>
                              <td>Yoav Arad</td>
                              <td>2023-10-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2310_01904v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2310.01904v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/yoavarad/mfad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_16309v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Weakly-Supervised Video Anomaly Detection with Snippet Anomalous Attention</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_16309v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_16309v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_16309v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>With a focus on abnormal events contained within untrimmed videos, there is increasing interest among researchers in video anomaly detection. Among different video anomaly detection scenarios, weakly-supervised video anomaly detection poses a significant challenge as it lacks frame-wise labels during the training stage, only relying on video-level labels as coarse supervision. Previous methods have made attempts to either learn discriminative features in an end-to-end manner or employ a twostage self-training strategy to generate snippet-level pseudo labels. However, both approaches have certain limitations. The former tends to overlook informative features at the snippet level, while the latter can be susceptible to noises. In this paper, we propose an Anomalous Attention mechanism for weakly-supervised anomaly detection to tackle the aforementioned problems. Our approach takes into account snippet-level encoded features without the supervision of pseudo labels. Specifically, our approach first generates snippet-level anomalous attention and then feeds it together with original anomaly scores into a Multi-branch Supervision Module. The module learns different areas of the video, including areas that are challenging to detect, and also assists the attention optimization. Experiments on benchmark datasets XDViolence and UCF-Crime verify the effectiveness of our method. Besides, thanks to the proposed snippet-level attention, we obtain a more precise anomaly localization.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_16309v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>随着对未经修剪的视频中包含的异常事件的关注，研究人员对视频异常检测的兴趣越来越大。在不同的视频异常检测场景中，弱监督视频异常检测提出了重大挑战，因为它在训练阶段缺乏逐帧标签，仅依赖视频级别的标签作为粗略监督。先前的方法已经尝试以端到端的方式学习判别特征，或者采用两阶段自训练策略来生成片段级伪标签。然而，这两种方法都有一定的局限性。前者倾向于忽略片段级别的信息特征，而后者可能容易受到噪声的影响。在本文中，我们提出了一种用于弱监督异常检测的异常注意机制来解决上述问题。我们的方法在没有伪标签监督的情况下考虑片段级别的编码特征。具体来说，我们的方法首先生成片段级别的异常注意力，然后将其与原始异常分数一起输入多分支监督模块。该模块学习视频的不同区域，包括难以检测的区域，并有助于注意力优化。在基准数据集XDViolence和UCF Crime上的实验验证了我们方法的有效性。此外，由于所提出的片段级关注，我们获得了更精确的异常定位。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.16309v1" target="_blank">2309.16309v1</a>
                              </td>
                              <td>Weakly-Supervised Video Anomaly Detection with Snippet Anomalous Attention</td>
                              <td>Yidan Fan</td>
                              <td>2023-09-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_16309v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.16309v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_15662v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Human Kinematics-inspired Skeleton-based Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_15662v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_15662v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_15662v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Previous approaches to detecting human anomalies in videos have typically relied on implicit modeling by directly applying the model to video or skeleton data, potentially resulting in inaccurate modeling of motion information. In this paper, we conduct an exploratory study and introduce a new idea called HKVAD (Human Kinematic-inspired Video Anomaly Detection) for video anomaly detection, which involves the explicit use of human kinematic features to detect anomalies. To validate the effectiveness and potential of this perspective, we propose a pilot method that leverages the kinematic features of the skeleton pose, with a specific focus on the walking stride, skeleton displacement at feet level, and neck level. Following this, the method employs a normalizing flow model to estimate density and detect anomalies based on the estimated density. Based on the number of kinematic features used, we have devised three straightforward variant methods and conducted experiments on two highly challenging public datasets, ShanghaiTech and UBnormal. Our method achieves good results with minimal computational resources, validating its effectiveness and potential.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_15662v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>先前检测视频中人类异常的方法通常依赖于通过将模型直接应用于视频或骨架数据的隐式建模，这可能导致运动信息的建模不准确。在本文中，我们进行了一项探索性研究，并引入了一种新的视频异常检测思想，称为HKVAD（人类运动学启发视频异常检测），该思想涉及明确使用人类运动学特征来检测异常。为了验证这一观点的有效性和潜力，我们提出了一种试点方法，该方法利用了骨骼姿势的运动学特征，特别关注步行步幅、脚部和颈部的骨骼位移。在此之后，该方法采用归一化流量模型来估计密度并基于估计的密度检测异常。基于所使用的运动学特征的数量，我们设计了三种简单的变体方法，并在两个极具挑战性的公共数据集ShanghaiTech和UBnormal上进行了实验。我们的方法用最少的计算资源取得了良好的结果，验证了其有效性和潜力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.15662v1" target="_blank">2309.15662v1</a>
                              </td>
                              <td>Human Kinematics-inspired Skeleton-based Video Anomaly Detection</td>
                              <td>Jian Xiao</td>
                              <td>2023-09-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_15662v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.15662v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/XiaoJian923/Kinematics-VAD" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_10719v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Memory-augmented Online Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_10719v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_10719v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_10719v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The ability to understand the surrounding scene is of paramount importance for Autonomous Vehicles (AVs). This paper presents a system capable to work in an online fashion, giving an immediate response to the arise of anomalies surrounding the AV, exploiting only the videos captured by a dash-mounted camera. Our architecture, called MOVAD, relies on two main modules: a Short-Term Memory Module to extract information related to the ongoing action, implemented by a Video Swin Transformer (VST), and a Long-Term Memory Module injected inside the classifier that considers also remote past information and action context thanks to the use of a Long-Short Term Memory (LSTM) network. The strengths of MOVAD are not only linked to its excellent performance, but also to its straightforward and modular architecture, trained in a end-to-end fashion with only RGB frames with as less assumptions as possible, which makes it easy to implement and play with. We evaluated the performance of our method on Detection of Traffic Anomaly (DoTA) dataset, a challenging collection of dash-mounted camera videos of accidents. After an extensive ablation study, MOVAD is able to reach an AUC score of 82.17\%, surpassing the current state-of-the-art by +2.87 AUC. Our code will be available on https://github.com/IMPLabUniPr/movad/tree/movad_vad</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_10719v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>了解周围场景的能力对于自动驾驶汽车至关重要。本文介绍了一种能够以在线方式工作的系统，仅利用仪表板摄像头拍摄的视频，对AV周围的异常情况做出即时响应。我们的架构称为MOVAD，依赖于两个主要模块：一个短期内存模块，用于提取与正在进行的动作相关的信息，由视频Swin Transformer（VST）实现；另一个长期内存模块，注入分类器内部，由于使用了长短期内存（LSTM）网络，该模块还考虑了远程过去的信息和动作上下文。MOVAD的优势不仅与其出色的性能有关，还与其直接的模块化架构有关，该架构以端到端的方式进行训练，仅使用RGB帧，假设尽可能少，这使其易于实现和使用。我们在交通异常检测（DoTA）数据集上评估了我们的方法的性能，这是一个具有挑战性的事故行车记录仪视频集。经过广泛的消融研究，MOVAD的AUC得分达到82.17\%，超过目前最先进的+2.87 AUC。我们的代码将在上提供https://github.com/IMPLabUniPr/movad/tree/movad_vad</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.10719v2" target="_blank">2302.10719v2</a>
                              </td>
                              <td>Memory-augmented Online Video Anomaly Detection</td>
                              <td>Leonardo Rossi</td>
                              <td>2023-02-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_10719v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.10719v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/hachreak/movad" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/implabunipr/movad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_03401v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Reasonable Anomaly Detection in Long Sequences</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_03401v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_03401v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_03401v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection is a challenging task due to the lack in approaches for representing samples. The visual representations of most existing approaches are limited by short-term sequences of observations which cannot provide enough clues for achieving reasonable detections. In this paper, we propose to completely represent the motion patterns of objects by learning from long-term sequences. Firstly, a Stacked State Machine (SSM) model is proposed to represent the temporal dependencies which are consistent across long-range observations. Then SSM model functions in predicting future states based on past ones, the divergence between the predictions with inherent normal patterns and observed ones determines anomalies which violate normal motion patterns. Extensive experiments are carried out to evaluate the proposed approach on the dataset and existing ones. Improvements over state-of-the-art methods can be observed. Our code is available at https://github.com/AllenYLJiang/Anomaly-Detection-in-Sequences.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_03401v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于缺乏表示样本的方法，视频异常检测是一项具有挑战性的任务。大多数现有方法的视觉表示都受到短期观测序列的限制，这些观测序列不能为实现合理的检测提供足够的线索。在本文中，我们提出通过从长期序列中学习来完全表示物体的运动模式。首先，提出了一种堆叠状态机（SSM）模型来表示在长距离观测中一致的时间相关性。然后，SSM模型在过去的基础上预测未来的状态，具有固有正常模式的预测与观测到的预测之间的差异决定了违反正常运动模式的异常。在数据集和现有数据集上进行了广泛的实验来评估所提出的方法。可以观察到对最先进方法的改进。我们的代码可在https://github.com/AllenYLJiang/Anomaly-Detection-in-Sequences.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.03401v1" target="_blank">2309.03401v1</a>
                              </td>
                              <td>Reasonable Anomaly Detection in Long Sequences</td>
                              <td>Yalong Jiang</td>
                              <td>2023-09-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_03401v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.03401v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/allenyljiang/anomaly-detection-in-sequences" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_01682v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Prior Knowledge Guided Network for Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_01682v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_01682v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_01682v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Anomaly Detection (VAD) involves detecting anomalous events in videos, presenting a significant and intricate task within intelligent video surveillance. Existing studies often concentrate solely on features acquired from limited normal data, disregarding the latent prior knowledge present in extensive natural image datasets. To address this constraint, we propose a Prior Knowledge Guided Network(PKG-Net) for the VAD task. First, an auto-encoder network is incorporated into a teacher-student architecture to learn two designated proxy tasks: future frame prediction and teacher network imitation, which can provide better generalization ability on unknown samples. Second, knowledge distillation on proper feature blocks is also proposed to increase the multi-scale detection ability of the model. In addition, prediction error and teacher-student feature inconsistency are combined to evaluate anomaly scores of inference samples more comprehensively. Experimental results on three public benchmarks validate the effectiveness and accuracy of our method, which surpasses recent state-of-the-arts.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_01682v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）涉及检测视频中的异常事件，在智能视频监控中是一项重要而复杂的任务。现有的研究往往只关注从有限的正常数据中获得的特征，而忽略了广泛的自然图像数据集中存在的潜在先验知识。为了解决这一约束，我们提出了一种用于VAD任务的先验知识引导网络（PKG-Net）。首先，将自动编码器网络纳入师生架构，以学习两个指定的代理任务：未来帧预测和教师网络模仿，这可以在未知样本上提供更好的泛化能力。其次，还提出了对适当特征块的知识提取，以提高模型的多尺度检测能力。此外，将预测误差和师生特征不一致性相结合，更全面地评估推理样本的异常分数。在三个公共基准上的实验结果验证了我们的方法的有效性和准确性，超过了最近的技术水平。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.01682v1" target="_blank">2309.01682v1</a>
                              </td>
                              <td>Prior Knowledge Guided Network for Video Anomaly Detection</td>
                              <td>Zhewen Deng</td>
                              <td>2023-09-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_01682v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.01682v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_02216v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Industrial Anomaly Detection with Domain Shift: A Real-world Dataset and Masked Multi-scale Reconstruction</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_02216v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_02216v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_02216v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Industrial anomaly detection (IAD) is crucial for automating industrial quality inspection. The diversity of the datasets is the foundation for developing comprehensive IAD algorithms. Existing IAD datasets focus on the diversity of data categories, overlooking the diversity of domains within the same data category. In this paper, to bridge this gap, we propose the Aero-engine Blade Anomaly Detection (AeBAD) dataset, consisting of two sub-datasets: the single-blade dataset and the video anomaly detection dataset of blades. Compared to existing datasets, AeBAD has the following two characteristics: 1.) The target samples are not aligned and at different scales. 2.) There is a domain shift between the distribution of normal samples in the test set and the training set, where the domain shifts are mainly caused by the changes in illumination and view. Based on this dataset, we observe that current state-of-the-art (SOTA) IAD methods exhibit limitations when the domain of normal samples in the test set undergoes a shift. To address this issue, we propose a novel method called masked multi-scale reconstruction (MMR), which enhances the model's capacity to deduce causality among patches in normal samples by a masked reconstruction task. MMR achieves superior performance compared to SOTA methods on the AeBAD dataset. Furthermore, MMR achieves competitive performance with SOTA methods to detect the anomalies of different types on the MVTec AD dataset. Code and dataset are available at https://github.com/zhangzilongc/MMR.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_02216v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>工业异常检测（IAD）是实现工业质量检测自动化的关键。数据集的多样性是开发综合IAD算法的基础。现有的IAD数据集侧重于数据类别的多样性，忽略了同一数据类别内领域的多样性。在本文中，为了弥补这一差距，我们提出了航空发动机叶片异常检测（AeBAD）数据集，该数据集由两个子数据集组成：单叶片数据集和叶片视频异常检测数据集。与现有的数据集相比，AeBAD具有以下两个特点：1.）目标样本不对齐，尺度不同。2.）正态样本在测试集中的分布和训练集中的分布之间存在域偏移，其中域偏移主要是由照明和视图的变化引起的。基于该数据集，我们观察到，当测试集中的正态样本的域发生变化时，当前最先进的（SOTA）IAD方法表现出局限性。为了解决这个问题，我们提出了一种称为掩蔽多尺度重建（MMR）的新方法，该方法通过掩蔽重建任务增强了模型推断正常样本中斑块之间因果关系的能力。在AeBAD数据集上，与SOTA方法相比，MMR实现了卓越的性能。此外，MMR通过SOTA方法检测MVTec AD数据集上不同类型的异常，实现了具有竞争力的性能。代码和数据集可在https://github.com/zhangzilongc/MMR.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.02216v2" target="_blank">2304.02216v2</a>
                              </td>
                              <td>Industrial Anomaly Detection with Domain Shift: A Real-world Dataset and Masked Multi-scale Reconstruction</td>
                              <td>Zilong Zhang</td>
                              <td>2023-04-05</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_02216v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.02216v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhangzilongc/MMR" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_07205v3_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_07205v3_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_07205v3_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_07205v3_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Anomalies are rare and anomaly detection is often therefore framed as One-Class Classification (OCC), i.e. trained solely on normalcy. Leading OCC techniques constrain the latent representations of normal motions to limited volumes and detect as abnormal anything outside, which accounts satisfactorily for the openset'ness of anomalies. But normalcy shares the same openset'ness property since humans can perform the same action in several ways, which the leading techniques neglect. We propose a novel generative model for video anomaly detection (VAD), which assumes that both normality and abnormality are multimodal. We consider skeletal representations and leverage state-of-the-art diffusion probabilistic models to generate multimodal future human poses. We contribute a novel conditioning on the past motion of people and exploit the improved mode coverage capabilities of diffusion processes to generate different-but-plausible future motions. Upon the statistical aggregation of future modes, an anomaly is detected when the generated set of motions is not pertinent to the actual future. We validate our model on 4 established benchmarks: UBnormal, HR-UBnormal, HR-STC, and HR-Avenue, with extensive experiments surpassing state-of-the-art results.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_07205v3_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>异常是罕见的，因此异常检测通常被定义为一类分类（OCC），即仅根据正常情况进行训练。领先的OCC技术将正常运动的潜在表现限制在有限的体积内，并将外部的任何东西检测为异常，这令人满意地解释了异常的开放性。但常态具有相同的开放性，因为人类可以通过多种方式执行相同的动作，而主流技术忽略了这一点。我们提出了一种新的视频异常检测生成模型，该模型假设正常和异常都是多模式的。我们考虑骨骼表示，并利用最先进的扩散概率模型来生成多模式的未来人体姿势。我们对人们过去的运动进行了新的调节，并利用扩散过程的改进模式覆盖能力来产生不同但合理的未来运动。在对未来模式进行统计聚合后，当生成的运动集与实际未来不相关时，检测到异常。我们在4个已建立的基准上验证了我们的模型：UBnormal、HR-UBnormal、HR-STC和HR-Avenue，大量实验超过了最先进的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.07205v3" target="_blank">2307.07205v3</a>
                              </td>
                              <td>Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection</td>
                              <td>Alessandro Flaborea</td>
                              <td>2023-07-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_07205v3_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.07205v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/aleflabo/MoCoDAD" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/aleflabo/MoCoDAD" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_14052v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MM-AU:Towards Multimodal Understanding of Advertisement Videos</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_14052v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_14052v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_14052v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Advertisement videos (ads) play an integral part in the domain of Internet e-commerce as they amplify the reach of particular products to a broad audience or can serve as a medium to raise awareness about specific issues through concise narrative structures. The narrative structures of advertisements involve several elements like reasoning about the broad content (topic and the underlying message) and examining fine-grained details involving the transition of perceived tone due to the specific sequence of events and interaction among characters. In this work, to facilitate the understanding of advertisements along the three important dimensions of topic categorization, perceived tone transition, and social message detection, we introduce a multimodal multilingual benchmark called MM-AU composed of over 8.4K videos (147 hours) curated from multiple web sources. We explore multiple zero-shot reasoning baselines through the application of large language models on the ads transcripts. Further, we demonstrate that leveraging signals from multiple modalities, including audio, video, and text, in multimodal transformer-based supervised models leads to improved performance compared to unimodal approaches.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_14052v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>广告视频（广告）在互联网电子商务领域发挥着不可或缺的作用，因为它们扩大了特定产品对广大受众的影响，或者可以作为一种媒介，通过简洁的叙述结构提高人们对特定问题的认识。广告的叙事结构涉及几个元素，如对广泛内容（主题和潜在信息）的推理，以及检查细粒度的细节，这些细节涉及由于特定的事件序列和人物之间的互动而产生的感知语气的转变。在这项工作中，为了促进从主题分类、感知语气转换和社交消息检测这三个重要维度对广告的理解，我们引入了一种称为MM-AU的多模式多语言基准，该基准由来自多个网络来源的8.4K多个视频（147小时）组成。我们通过在广告文本上应用大型语言模型来探索多个零样本推理基线。此外，我们证明，与单峰方法相比，在基于多模态变换器的监督模型中，利用包括音频、视频和文本在内的多种模态的信号可以提高性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.14052v1" target="_blank">2308.14052v1</a>
                              </td>
                              <td>MM-AU:Towards Multimodal Understanding of Advertisement Videos</td>
                              <td>Digbalay Bose</td>
                              <td>2023-08-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_14052v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.14052v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_11072v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_11072v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_11072v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_11072v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection (VAD) without human monitoring is a complex computer vision task that can have a positive impact on society if implemented successfully. While recent advances have made significant progress in solving this task, most existing approaches overlook a critical real-world concern: privacy. With the increasing popularity of artificial intelligence technologies, it becomes crucial to implement proper AI ethics into their development. Privacy leakage in VAD allows models to pick up and amplify unnecessary biases related to people's personal information, which may lead to undesirable decision making. In this paper, we propose TeD-SPAD, a privacy-aware video anomaly detection framework that destroys visual private information in a self-supervised manner. In particular, we propose the use of a temporally-distinct triplet loss to promote temporally discriminative features, which complements current weakly-supervised VAD methods. Using TeD-SPAD, we achieve a positive trade-off between privacy protection and utility anomaly detection performance on three popular weakly supervised VAD datasets: UCF-Crime, XD-Violence, and ShanghaiTech. Our proposed anonymization model reduces private attribute prediction by 32.25% while only reducing frame-level ROC AUC on the UCF-Crime anomaly detection dataset by 3.69%. Project Page: https://joefioresi718.github.io/TeD-SPAD_webpage/</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_11072v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>无需人工监控的视频异常检测是一项复杂的计算机视觉任务，如果成功实施，将对社会产生积极影响。尽管最近的进展在解决这一任务方面取得了重大进展，但大多数现有方法都忽视了现实世界中一个关键问题：隐私。随着人工智能技术的日益普及，在其发展中实施适当的人工智能伦理变得至关重要。VAD中的隐私泄露允许模型拾取并放大与人们的个人信息相关的不必要的偏见，这可能导致不理想的决策。在本文中，我们提出了TeD SPAD，这是一种具有隐私意识的视频异常检测框架，可以以自我监督的方式破坏视觉隐私信息。特别地，我们建议使用时间上不同的三元组损失来促进时间上的判别特征，这补充了当前的弱监督VAD方法。使用TeD SPAD，我们在三个流行的弱监督VAD数据集上实现了隐私保护和效用异常检测性能之间的积极权衡：UCF犯罪、XD暴力和ShanghaiTech。我们提出的匿名化模型将私有属性预测减少了32.25%，而仅将UCF犯罪异常检测数据集上的帧级ROC AUC减少了3.69%。项目页面：https://joefioresi718.github.io/TeD-SPAD_webpage/</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.11072v1" target="_blank">2308.11072v1</a>
                              </td>
                              <td>TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection</td>
                              <td>Joseph Fioresi</td>
                              <td>2023-08-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_11072v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.11072v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/UCF-CRCV/TeD-SPAD" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_10946v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Normalizing Flows for Human Pose Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_10946v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_10946v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_10946v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection is an ill-posed problem because it relies on many parameters such as appearance, pose, camera angle, background, and more. We distill the problem to anomaly detection of human pose, thus decreasing the risk of nuisance parameters such as appearance affecting the result. Focusing on pose alone also has the side benefit of reducing bias against distinct minority groups. Our model works directly on human pose graph sequences and is exceptionally lightweight (~1K parameters), capable of running on any machine able to run the pose estimation with negligible additional resources. We leverage the highly compact pose representation in a normalizing flows framework, which we extend to tackle the unique characteristics of spatio-temporal pose data and show its advantages in this use case. The algorithm is quite general and can handle training data of only normal examples as well as a supervised setting that consists of labeled normal and abnormal examples. We report state-of-the-art results on two anomaly detection benchmarks - the unsupervised ShanghaiTech dataset and the recent supervised UBnormal dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_10946v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测是一个不适定的问题，因为它依赖于许多参数，如外观、姿势、相机角度、背景等。我们将问题提取到人体姿态的异常检测中，从而降低了外观等干扰参数影响结果的风险。仅关注姿态也有减少对不同少数群体的偏见的副作用。我们的模型直接在人体姿态图序列上工作，并且非常轻（约1K个参数），能够在任何能够运行姿态估计的机器上运行，而无需额外的资源。我们在规范化流框架中利用了高度紧凑的姿态表示，我们对其进行了扩展，以解决时空姿态数据的独特特征，并在本用例中展示了其优势。该算法非常通用，可以处理仅正常示例的训练数据以及由标记的正常和异常示例组成的监督设置。我们报告了两个异常检测基准的最新结果——无监督ShanghaiTech数据集和最近的有监督UBnormal数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.10946v2" target="_blank">2211.10946v2</a>
                              </td>
                              <td>Normalizing Flows for Human Pose Anomaly Detection</td>
                              <td>Or Hirschorn</td>
                              <td>2022-11-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_10946v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.10946v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/orhir/stg-nf" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07783v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Future Video Prediction from a Single Frame for Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07783v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07783v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07783v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection (VAD) is an important but challenging task in computer vision. The main challenge rises due to the rarity of training samples to model all anomaly cases. Hence, semi-supervised anomaly detection methods have gotten more attention, since they focus on modeling normals and they detect anomalies by measuring the deviations from normal patterns. Despite impressive advances of these methods in modeling normal motion and appearance, long-term motion modeling has not been effectively explored so far. Inspired by the abilities of the future frame prediction proxy-task, we introduce the task of future video prediction from a single frame, as a novel proxy-task for video anomaly detection. This proxy-task alleviates the challenges of previous methods in learning longer motion patterns. Moreover, we replace the initial and future raw frames with their corresponding semantic segmentation map, which not only makes the method aware of object class but also makes the prediction task less complex for the model. Extensive experiments on the benchmark datasets (ShanghaiTech, UCSD-Ped1, and UCSD-Ped2) show the effectiveness of the method and the superiority of its performance compared to SOTA prediction-based VAD methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07783v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）是计算机视觉中一项重要但具有挑战性的任务。主要的挑战是由于对所有异常情况进行建模的训练样本很少。因此，半监督异常检测方法越来越受到关注，因为它们专注于建模法线，并且通过测量与法线模式的偏差来检测异常。尽管这些方法在建模正常运动和外观方面取得了令人印象深刻的进展，但到目前为止，长期运动建模还没有得到有效的探索。受未来帧预测代理任务能力的启发，我们引入了从单个帧进行未来视频预测的任务，作为一种新的视频异常检测代理任务。该代理任务减轻了先前方法在学习较长运动模式方面的挑战。此外，我们将初始和未来的原始帧替换为它们相应的语义分割图，这不仅使该方法能够感知对象类，而且使模型的预测任务不那么复杂。在基准数据集（ShanghaiTech、UCSD-Ped1和UCSD-Ped 2）上进行的大量实验表明，与基于SOTA预测的VAD方法相比，该方法的有效性及其性能优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07783v1" target="_blank">2308.07783v1</a>
                              </td>
                              <td>Future Video Prediction from a Single Frame for Video Anomaly Detection</td>
                              <td>Mohammad Baradaran</td>
                              <td>2023-08-15</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07783v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07783v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_07050v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Survey on video anomaly detection in dynamic scenes with moving cameras</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_07050v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_07050v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_07050v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The increasing popularity of compact and inexpensive cameras, e.g.~dash cameras, body cameras, and cameras equipped on robots, has sparked a growing interest in detecting anomalies within dynamic scenes recorded by moving cameras. However, existing reviews primarily concentrate on Video Anomaly Detection (VAD) methods assuming static cameras. The VAD literature with moving cameras remains fragmented, lacking comprehensive reviews to date. To address this gap, we endeavor to present the first comprehensive survey on Moving Camera Video Anomaly Detection (MC-VAD). We delve into the research papers related to MC-VAD, critically assessing their limitations and highlighting associated challenges. Our exploration encompasses three application domains: security, urban transportation, and marine environments, which in turn cover six specific tasks. We compile an extensive list of 25 publicly-available datasets spanning four distinct environments: underwater, water surface, ground, and aerial. We summarize the types of anomalies these datasets correspond to or contain, and present five main categories of approaches for detecting such anomalies. Lastly, we identify future research directions and discuss novel contributions that could advance the field of MC-VAD. With this survey, we aim to offer a valuable reference for researchers and practitioners striving to develop and advance state-of-the-art MC-VAD methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_07050v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>紧凑型和廉价相机（如行车记录仪、人体摄像头和机器人上配备的摄像头）的日益普及，激发了人们对检测移动摄像头记录的动态场景中的异常现象的兴趣。然而，现有的综述主要集中在假设静态摄像机的视频异常检测（VAD）方法上。迄今为止，带有移动摄像机的VAD文献仍然零散，缺乏全面的综述。为了解决这一差距，我们致力于首次对运动摄像机视频异常检测（MC-VAD）进行全面调查。我们深入研究了与MC-VAD相关的研究论文，批判性地评估了它们的局限性，并强调了相关的挑战。我们的探索包括三个应用领域：安全、城市交通和海洋环境，这些领域又涵盖了六项具体任务。我们编制了一份涵盖四种不同环境的25个公开可用数据集的广泛列表：水下、水面、地面和空中。我们总结了这些数据集对应或包含的异常类型，并提出了检测此类异常的五大类方法。最后，我们确定了未来的研究方向，并讨论了可能推进MC-VAD领域的新贡献。通过这项调查，我们旨在为致力于开发和推进最先进的MC-VAD方法的研究人员和从业者提供有价值的参考。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.07050v1" target="_blank">2308.07050v1</a>
                              </td>
                              <td>Survey on video anomaly detection in dynamic scenes with moving cameras</td>
                              <td>Runyu Jiao</td>
                              <td>2023-08-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_07050v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.07050v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2308_01537v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning Causality-inspired Representation Consistency for Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2308_01537v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2308_01537v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2308_01537v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection is an essential yet challenging task in the multimedia community, with promising applications in smart cities and secure communities. Existing methods attempt to learn abstract representations of regular events with statistical dependence to model the endogenous normality, which discriminates anomalies by measuring the deviations to the learned distribution. However, conventional representation learning is only a crude description of video normality and lacks an exploration of its underlying causality. The learned statistical dependence is unreliable for diverse regular events in the real world and may cause high false alarms due to overgeneralization. Inspired by causal representation learning, we think that there exists a causal variable capable of adequately representing the general patterns of regular events in which anomalies will present significant variations. Therefore, we design a causality-inspired representation consistency (CRC) framework to implicitly learn the unobservable causal variables of normality directly from available normal videos and detect abnormal events with the learned representation consistency. Extensive experiments show that the causality-inspired normality is robust to regular events with label-independent shifts, and the proposed CRC framework can quickly and accurately detect various complicated anomalies from real-world surveillance videos.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2308_01537v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测是多媒体社区中一项重要但具有挑战性的任务，在智能城市和安全社区中有着很好的应用前景。现有的方法试图学习具有统计相关性的规则事件的抽象表示，以对内生正态性进行建模，该正态性通过测量与所学分布的偏差来区分异常。然而，传统的表示学习只是对视频常态的粗略描述，缺乏对其潜在因果关系的探索。对于现实世界中的各种常规事件，所学习的统计依赖性是不可靠的，并且可能由于过度概括而导致高误报。受因果表示学习的启发，我们认为存在一个因果变量，能够充分表示规则事件的一般模式，其中异常将呈现显著变化。因此，我们设计了一个受因果关系启发的表示一致性（CRC）框架，直接从可用的正常视频中隐式地学习不可观测的正态性因果变量，并用所学习的表示一致度检测异常事件。大量实验表明，因果关系启发的正态性对具有标签无关转移的规则事件是鲁棒的，并且所提出的CRC框架可以快速准确地检测现实世界监控视频中的各种复杂异常。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2308.01537v1" target="_blank">2308.01537v1</a>
                              </td>
                              <td>Learning Causality-inspired Representation Consistency for Video Anomaly Detection</td>
                              <td>Yang Liu</td>
                              <td>2023-08-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2308_01537v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2308.01537v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_15326v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_15326v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_15326v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_15326v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Online ads showing e-commerce products typically rely on the product images in a catalog sent to the advertising platform by an e-commerce platform. In the broader ads industry such ads are called dynamic product ads (DPA). It is common for DPA catalogs to be in the scale of millions (corresponding to the scale of products which can be bought from the e-commerce platform). However, not all product images in the catalog may be appealing when directly re-purposed as an ad image, and this may lead to lower click-through rates (CTRs). In particular, products just placed against a solid background may not be as enticing and realistic as a product staged in a natural environment. To address such shortcomings of DPA images at scale, we propose a generative adversarial network (GAN) based approach to generate staged backgrounds for un-staged product images. Generating the entire staged background is a challenging task susceptible to hallucinations. To get around this, we introduce a simpler approach called copy-paste staging using retrieval assisted GANs. In copy paste staging, we first retrieve (from the catalog) staged products similar to the un-staged input product, and then copy-paste the background of the retrieved product in the input image. A GAN based in-painting model is used to fill the holes left after this copy-paste operation. We show the efficacy of our copy-paste staging method via offline metrics, and human evaluation. In addition, we show how our staging approach can enable animations of moving products leading to a video ad from a product image.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_15326v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>显示电子商务产品的在线广告通常依赖于电子商务平台发送到广告平台的目录中的产品图像。在更广泛的广告行业中，这种广告被称为动态产品广告（DPA）。DPA目录的规模通常为数百万（对应于可以从电子商务平台购买的产品的规模）。然而，当直接重新用作广告图像时，并非目录中的所有产品图像都可能具有吸引力，这可能导致点击率（CTR）降低。特别是，仅仅放在坚实背景下的产品可能不如在自然环境中上演的产品那么诱人和逼真。为了在规模上解决DPA图像的这些缺点，我们提出了一种基于生成对抗性网络（GAN）的方法来为未分级的产品图像生成分级背景。生成整个舞台背景是一项容易产生幻觉的具有挑战性的任务。为了解决这个问题，我们引入了一种更简单的方法，称为使用检索辅助GANs的复制粘贴阶段。在复制粘贴暂存中，我们首先检索（从目录中）与未暂存输入产品相似的暂存产品，然后将检索到的产品的背景复制粘贴到输入图像中。基于GAN的绘制内模型用于填充此复制粘贴操作后留下的孔。我们通过离线指标和人工评估展示了复制粘贴分期方法的有效性。此外，我们还展示了我们的分级方法如何实现移动产品的动画，从而从产品图像中生成视频广告。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.15326v1" target="_blank">2307.15326v1</a>
                              </td>
                              <td>Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation</td>
                              <td>Yueh-Ning Ku</td>
                              <td>2023-07-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_15326v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.15326v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_12545v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Video Anomaly Retrieval from Video Anomaly Detection: New Benchmarks and Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_12545v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_12545v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_12545v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection (VAD) has been paid increasing attention due to its potential applications, its current dominant tasks focus on online detecting anomalies% at the frame level, which can be roughly interpreted as the binary or multiple event classification. However, such a setup that builds relationships between complicated anomalous events and single labels, e.g., ``vandalism'', is superficial, since single labels are deficient to characterize anomalous events. In reality, users tend to search a specific video rather than a series of approximate videos. Therefore, retrieving anomalous events using detailed descriptions is practical and positive but few researches focus on this. In this context, we propose a novel task called Video Anomaly Retrieval (VAR), which aims to pragmatically retrieve relevant anomalous videos by cross-modalities, e.g., language descriptions and synchronous audios. Unlike the current video retrieval where videos are assumed to be temporally well-trimmed with short duration, VAR is devised to retrieve long untrimmed videos which may be partially relevant to the given query. To achieve this, we present two large-scale VAR benchmarks, UCFCrime-AR and XDViolence-AR, constructed on top of prevalent anomaly datasets. Meanwhile, we design a model called Anomaly-Led Alignment Network (ALAN) for VAR. In ALAN, we propose an anomaly-led sampling to focus on key segments in long untrimmed videos. Then, we introduce an efficient pretext task to enhance semantic associations between video-text fine-grained representations. Besides, we leverage two complementary alignments to further match cross-modal contents. Experimental results on two benchmarks reveal the challenges of VAR task and also demonstrate the advantages of our tailored method.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_12545v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）由于其潜在的应用而越来越受到关注，其目前的主要任务集中在帧级的在线检测异常%，大致可以解释为二进制或多事件分类。然而，这种在复杂的异常事件和单个标签（例如“故意破坏”）之间建立关系的设置是肤浅的，因为单个标签不足以表征异常事件。事实上，用户倾向于搜索特定的视频，而不是一系列近似的视频。因此，使用详细的描述来检索异常事件是实用和积极的，但很少有研究关注这一点。在这种情况下，我们提出了一种新的任务，称为视频异常检索（VAR），旨在通过跨模态（如语言描述和同步音频）务实地检索相关的异常视频。与当前视频检索不同，在当前视频检索中，假设视频在短持续时间内被很好地修剪，VAR被设计为检索可能与给定查询部分相关的长的未修剪视频。为了实现这一点，我们提出了两个大规模VAR基准，即UCFCrime AR和XDViolence AR，它们是在流行的异常数据集之上构建的。同时，我们为VAR设计了一个名为异常引导对齐网络（ALAN）的模型。在ALAN中，我们提出了一种异常引导采样，以关注长时间未修剪视频中的关键片段。然后，我们引入了一种有效的借口任务来增强视频文本细粒度表示之间的语义关联。此外，我们利用两个互补的对齐来进一步匹配跨模态内容。在两个基准上的实验结果揭示了VAR任务的挑战，也展示了我们定制方法的优势。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.12545v1" target="_blank">2307.12545v1</a>
                              </td>
                              <td>Towards Video Anomaly Retrieval from Video Anomaly Detection: New Benchmarks and Model</td>
                              <td>Peng Wu</td>
                              <td>2023-07-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_12545v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.12545v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2307_01533v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2307_01533v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2307_01533v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2307_01533v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper aims to address the unsupervised video anomaly detection (VAD) problem, which involves classifying each frame in a video as normal or abnormal, without any access to labels. To accomplish this, the proposed method employs conditional diffusion models, where the input data is the spatiotemporal features extracted from a pre-trained network, and the condition is the features extracted from compact motion representations that summarize a given video segment in terms of its motion and appearance. Our method utilizes a data-driven threshold and considers a high reconstruction error as an indicator of anomalous events. This study is the first to utilize compact motion representations for VAD and the experiments conducted on two large-scale VAD benchmarks demonstrate that they supply relevant information to the diffusion model, and consequently improve VAD performances w.r.t the prior art. Importantly, our method exhibits better generalization performance across different datasets, notably outperforming both the state-of-the-art and baseline methods. The code of our method is available at https://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2307_01533v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文旨在解决无监督视频异常检测（VAD）问题，该问题涉及将视频中的每一帧分类为正常或异常，而无需任何标签。为了实现这一点，所提出的方法采用了条件扩散模型，其中输入数据是从预先训练的网络中提取的时空特征，而条件是从紧凑运动表示中提取的特征，该紧凑运动表示根据给定视频片段的运动和外观来概括给定视频片段。我们的方法利用了数据驱动的阈值，并将高重建误差视为异常事件的指标。这项研究首次将紧凑运动表示用于VAD，在两个大型VAD基准上进行的实验表明，它们为扩散模型提供了相关信息，从而与现有技术相比提高了VAD性能。重要的是，我们的方法在不同的数据集上表现出更好的泛化性能，显著优于最先进的方法和基线方法。我们方法的代码可在https://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2307.01533v2" target="_blank">2307.01533v2</a>
                              </td>
                              <td>Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations</td>
                              <td>Anil Osman Tur</td>
                              <td>2023-07-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2307_01533v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2307.01533v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/anilosmantur/conditioned_video_anomaly_diffusion" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_10239v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-scale Spatial-temporal Interaction Network for Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_10239v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_10239v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_10239v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video Anomaly Detection (VAD) is an essential yet challenging task in signal processing. Since certain anomalies cannot be detected by isolated analysis of either temporal or spatial information, the interaction between these two types of data is considered crucial for VAD. However, current dual-stream architectures either confine this integral interaction to the bottleneck of the autoencoder or introduce anomaly-irrelevant background pixels into the interactive process, hindering the accuracy of VAD. To address these deficiencies, we propose a Multi-scale Spatial-Temporal Interaction Network (MSTI-Net) for VAD. First, to prioritize the detection of moving objects in the scene and harmonize the substantial semantic discrepancies between the two types of data, we propose an Attention-based Spatial-Temporal Fusion Module (ASTFM) as a substitute for the conventional direct fusion. Furthermore, we inject multi-ASTFM-based connections that bridge the appearance and motion streams of the dual-stream network, thus fostering multi-scale spatial-temporal interaction. Finally, to bolster the delineation between normal and abnormal activities, our system records the regular information in a memory module. Experimental results on three benchmark datasets validate the effectiveness of our approach, which achieves AUCs of 96.8%, 87.6%, and 73.9% on the UCSD Ped2, CUHK Avenue, and ShanghaiTech datasets, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_10239v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）是信号处理中一项重要而富有挑战性的任务。由于某些异常无法通过对时间或空间信息的单独分析来检测，因此这两种类型的数据之间的相互作用被认为是VAD的关键。然而，当前的双流架构要么将这种积分交互限制在自动编码器的瓶颈上，要么将与异常无关的背景像素引入交互过程，阻碍了VAD的准确性。为了解决这些不足，我们提出了一种用于VAD的多尺度时空交互网络（MSTI-Net）。首先，为了优先检测场景中的运动对象，并协调两种类型数据之间的实质语义差异，我们提出了一种基于注意力的时空融合模块（ASTFM）来代替传统的直接融合。此外，我们注入了多个基于ASTFM的连接，这些连接连接了双流网络的外观和运动流，从而促进了多尺度时空交互。最后，为了支持正常和异常活动之间的划分，我们的系统将规则信息记录在内存模块中。在三个基准数据集上的实验结果验证了我们方法的有效性，在UCSD Ped2、中大大道和ShanghaiTech数据集上分别实现了96.8%、87.6%和73.9%的AUC。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.10239v2" target="_blank">2306.10239v2</a>
                              </td>
                              <td>Multi-scale Spatial-temporal Interaction Network for Video Anomaly Detection</td>
                              <td>Zhiyuan Ning</td>
                              <td>2023-06-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_10239v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.10239v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_05136v3_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_05136v3_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_05136v3_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_05136v3_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection (VAD) -- commonly formulated as a multiple-instance learning problem in a weakly-supervised manner due to its labor-intensive nature -- is a challenging problem in video surveillance where the frames of anomaly need to be localized in an untrimmed video. In this paper, we first propose to utilize the ViT-encoded visual features from CLIP, in contrast with the conventional C3D or I3D features in the domain, to efficiently extract discriminative representations in the novel technique. We then model temporal dependencies and nominate the snippets of interest by leveraging our proposed Temporal Self-Attention (TSA). The ablation study confirms the effectiveness of TSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA outperforms the existing state-of-the-art (SOTA) methods by a large margin on three commonly-used benchmark datasets in the VAD problem (UCF-Crime, ShanghaiTech Campus, and XD-Violence). Our source code is available at https://github.com/joos2010kj/CLIP-TSA.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_05136v3_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）——由于其劳动密集型的性质，通常被表述为一个弱监督的多实例学习问题——是视频监控中的一个具有挑战性的问题，其中异常帧需要定位在未修剪的视频中。在本文中，我们首先提出利用来自CLIP的ViT编码的视觉特征，与该领域中的传统C3D或I3D特征相比，在新技术中有效地提取判别表示。然后，我们对时间依赖性进行建模，并通过利用我们提出的时间自注意（TSA）来指定感兴趣的片段。消融研究证实了TSA和ViT特征的有效性。广泛的实验表明，在VAD问题中三个常用的基准数据集（UCF犯罪、上海科技园区和XD暴力）上，我们提出的CLIP-TSA大大优于现有的最先进的（SOTA）方法。我们的源代码可在https://github.com/joos2010kj/CLIP-TSA.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.05136v3" target="_blank">2212.05136v3</a>
                              </td>
                              <td>CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised Video Anomaly Detection</td>
                              <td>Hyekang Kevin Joo</td>
                              <td>2022-12-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_05136v3_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.05136v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/joos2010kj/clip-tsa" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2304_05841v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring Diffusion Models for Unsupervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2304_05841v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2304_05841v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2304_05841v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper investigates the performance of diffusion models for video anomaly detection (VAD) within the most challenging but also the most operational scenario in which the data annotations are not used. As being sparse, diverse, contextual, and often ambiguous, detecting abnormal events precisely is a very ambitious task. To this end, we rely only on the information-rich spatio-temporal data, and the reconstruction power of the diffusion models such that a high reconstruction error is utilized to decide the abnormality. Experiments performed on two large-scale video anomaly detection datasets demonstrate the consistent improvement of the proposed method over the state-of-the-art generative models while in some cases our method achieves better scores than the more complex models. This is the first study using a diffusion model and examining its parameters' influence to present guidance for VAD in surveillance scenarios.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2304_05841v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文研究了在不使用数据注释的最具挑战性但也是最具操作性的场景中，视频异常检测（VAD）的扩散模型的性能。由于异常事件的稀疏性、多样性、上下文关联性以及常常模棱两可，准确检测异常事件是一项非常艰巨的任务。为此，我们仅依赖于信息丰富的时空数据和扩散模型的重建能力，从而利用高重建误差来决定异常。在两个大规模视频异常检测数据集上进行的实验表明，与最先进的生成模型相比，所提出的方法得到了一致的改进，而在某些情况下，我们的方法比更复杂的模型获得了更好的分数。这是第一项使用扩散模型并检查其参数影响的研究，为监测场景中的VAD提供指导。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2304.05841v2" target="_blank">2304.05841v2</a>
                              </td>
                              <td>Exploring Diffusion Models for Unsupervised Video Anomaly Detection</td>
                              <td>Anil Osman Tur</td>
                              <td>2023-04-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2304_05841v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2304.05841v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_12041v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_12041v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_12041v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_12041v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose an efficient abnormal event detection model based on a lightweight masked auto-encoder (AE) applied at the video frame level. The novelty of the proposed model is threefold. First, we introduce an approach to weight tokens based on motion gradients, thus avoiding learning to reconstruct the static background scene. Second, we integrate a teacher decoder and a student decoder into our architecture, leveraging the discrepancy between the outputs given by the two decoders to improve anomaly detection. Third, we generate synthetic abnormal events to augment the training videos, and task the masked AE model to jointly reconstruct the original frames (without anomalies) and the corresponding pixel-level anomaly maps. Our design leads to an efficient and effective model, as demonstrated by the extensive experiments carried out on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. The empirical results show that our model achieves an excellent trade-off between speed and accuracy, obtaining competitive AUC scores, while processing 1670 FPS. Hence, our model is between 8 and 70 times faster than competing methods. We also conduct an ablation study to justify our design.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_12041v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种基于应用于视频帧级别的轻量级屏蔽自动编码器（AE）的高效异常事件检测模型。所提出的模型的新颖性有三个方面。首先，我们介绍了一种基于运动梯度的权重标记方法，从而避免了学习重建静态背景场景。其次，我们将教师解码器和学生解码器集成到我们的架构中，利用两个解码器给出的输出之间的差异来改进异常检测。第三，我们生成合成异常事件来增强训练视频，并让掩蔽的AE模型联合重建原始帧（没有异常）和相应的像素级异常图。正如在Avenue、ShanghaiTech和UCSD Ped2三个基准上进行的广泛实验所证明的那样，我们的设计带来了一个高效有效的模型。经验结果表明，我们的模型在处理1670 FPS的同时，在速度和准确性之间实现了极好的权衡，获得了有竞争力的AUC分数。因此，我们的模型比竞争方法快8到70倍。我们还进行了消融研究，以证明我们的设计是合理的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>40</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.12041v1" target="_blank">2306.12041v1</a>
                              </td>
                              <td>Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly Detectors</td>
                              <td>Nicolae-Catalin Ristea</td>
                              <td>2023-06-21</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_12041v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.12041v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_02592v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_02592v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_02592v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_02592v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>To increase brand awareness, many advertisers conclude contracts with advertising platforms to purchase traffic and then deliver advertisements to target audiences. In a whole delivery period, advertisers usually desire a certain impression count for the ads, and they also expect that the delivery performance is as good as possible (e.g., obtaining high click-through rate). Advertising platforms employ pacing algorithms to satisfy the demands via adjusting the selection probabilities to traffic requests in real-time. However, the delivery procedure is also affected by the strategies from publishers, which cannot be controlled by advertising platforms. Preloading is a widely used strategy for many types of ads (e.g., video ads) to make sure that the response time for displaying after a traffic request is legitimate, which results in delayed impression phenomenon. Traditional pacing algorithms cannot handle the preloading nature well because they rely on immediate feedback signals, and may fail to guarantee the demands from advertisers.   In this paper, we focus on a new research problem of impression pacing for preloaded ads, and propose a Reinforcement Learning To Pace framework RLTP. It learns a pacing agent that sequentially produces selection probabilities in the whole delivery period. To jointly optimize the two objectives of impression count and delivery performance, RLTP employs tailored reward estimator to satisfy the guaranteed impression count, penalize the over-delivery and maximize the traffic value. Experiments on large-scale industrial datasets verify that RLTP outperforms baseline pacing algorithms by a large margin. We have deployed the RLTP framework online to our advertising platform, and results show that it achieves significant uplift to core metrics including delivery completion rate and click-through rate.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_02592v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>为了提高品牌知名度，许多广告商与广告平台签订合同，购买流量，然后向目标受众投放广告。在整个投放期内，广告商通常希望广告有一定的印象，他们也希望投放效果尽可能好（例如，获得高点击率）。广告平台采用定步算法，通过实时调整流量请求的选择概率来满足需求。然而，交付过程也受到来自出版商的策略的影响，这是广告平台无法控制的。预加载是许多类型的广告（例如视频广告）广泛使用的策略，以确保流量请求后显示的响应时间是合法的，这会导致印象延迟现象。传统的起搏算法不能很好地处理预加载性质，因为它们依赖于即时反馈信号，并且可能无法保证广告商的需求。在本文中，我们重点研究了一个新的预加载广告印象节奏问题，并提出了一个强化学习-节奏框架RLTP。它学习一个起搏代理，该起搏代理在整个递送期内顺序产生选择概率。为了联合优化印象数和交付性能这两个目标，RLTP采用定制的奖励估计器来满足保证的印象数，惩罚过度交付并最大化流量价值。在大规模工业数据集上的实验验证了RLTP在很大程度上优于基线起搏算法。我们已经将RLTP框架在线部署到我们的广告平台上，结果表明，它显著提升了核心指标，包括交付完成率和点击率。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>41</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.02592v2" target="_blank">2302.02592v2</a>
                              </td>
                              <td>RLTP: Reinforcement Learning to Pace for Delayed Impression Modeling in Preloaded Ads</td>
                              <td>Penghui Wei</td>
                              <td>2023-02-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_02592v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.02592v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_04466v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Point Cloud Video Anomaly Detection Based on Point Spatio-Temporal Auto-Encoder</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_04466v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_04466v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_04466v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection has great potential in enhancing safety in the production and monitoring of crucial areas. Currently, most video anomaly detection methods are based on RGB modality, but its redundant semantic information may breach the privacy of residents or patients. The 3D data obtained by depth camera and LiDAR can accurately locate anomalous events in 3D space while preserving human posture and motion information. Identifying individuals through the point cloud is difficult due to its sparsity, which protects personal privacy. In this study, we propose Point Spatio-Temporal Auto-Encoder (PSTAE), an autoencoder framework that uses point cloud videos as input to detect anomalies in point cloud videos. We introduce PSTOp and PSTTransOp to maintain spatial geometric and temporal motion information in point cloud videos. To measure the reconstruction loss of the proposed autoencoder framework, we propose a reconstruction loss measurement strategy based on a shallow feature extractor. Experimental results on the TIMo dataset show that our method outperforms currently representative depth modality-based methods in terms of AUROC and has superior performance in detecting Medical Issue anomalies. These results suggest the potential of point cloud modality in video anomaly detection. Our method sets a new state-of-the-art (SOTA) on the TIMo dataset.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_04466v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测在提高关键区域的安全生产和监控方面具有巨大潜力。目前，大多数视频异常检测方法都是基于RGB模式的，但其冗余的语义信息可能会侵犯居民或患者的隐私。通过深度相机和激光雷达获得的三维数据可以准确定位三维空间中的异常事件，同时保留人体姿态和运动信息。通过点云识别个人是困难的，因为它的稀疏性保护了个人隐私。在这项研究中，我们提出了点时空自动编码器（PSTAE），这是一种自动编码器框架，使用点云视频作为输入来检测点云视频中的异常。我们引入了PSTOp和PSTTransOp来维护点云视频中的空间几何和时间运动信息。为了测量所提出的自动编码器框架的重建损失，我们提出了一种基于浅层特征提取器的重建损失测量策略。在TIMo数据集上的实验结果表明，我们的方法在AUROC方面优于目前具有代表性的基于深度模态的方法，并且在检测医疗问题异常方面具有优越的性能。这些结果表明了点云模式在视频异常检测中的潜力。我们的方法在TIMo数据集上设置了一个新的最先进的（SOTA）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>42</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.04466v1" target="_blank">2306.04466v1</a>
                              </td>
                              <td>Point Cloud Video Anomaly Detection Based on Point Spatio-Temporal Auto-Encoder</td>
                              <td>Tengjiao He</td>
                              <td>2023-06-04</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_04466v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.04466v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2212_09258v3_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CHAD: Charlotte Anomaly Dataset</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2212_09258v3_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2212_09258v3_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2212_09258v3_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In recent years, we have seen a significant interest in data-driven deep learning approaches for video anomaly detection, where an algorithm must determine if specific frames of a video contain abnormal behaviors. However, video anomaly detection is particularly context-specific, and the availability of representative datasets heavily limits real-world accuracy. Additionally, the metrics currently reported by most state-of-the-art methods often do not reflect how well the model will perform in real-world scenarios. In this article, we present the Charlotte Anomaly Dataset (CHAD). CHAD is a high-resolution, multi-camera anomaly dataset in a commercial parking lot setting. In addition to frame-level anomaly labels, CHAD is the first anomaly dataset to include bounding box, identity, and pose annotations for each actor. This is especially beneficial for skeleton-based anomaly detection, which is useful for its lower computational demand in real-world settings. CHAD is also the first anomaly dataset to contain multiple views of the same scene. With four camera views and over 1.15 million frames, CHAD is the largest fully annotated anomaly detection dataset including person annotations, collected from continuous video streams from stationary cameras for smart video surveillance applications. To demonstrate the efficacy of CHAD for training and evaluation, we benchmark two state-of-the-art skeleton-based anomaly detection algorithms on CHAD and provide comprehensive analysis, including both quantitative results and qualitative examination. The dataset is available at https://github.com/TeCSAR-UNCC/CHAD.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2212_09258v3_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>近年来，我们发现人们对视频异常检测的数据驱动深度学习方法非常感兴趣，在这种方法中，算法必须确定视频的特定帧是否包含异常行为。然而，视频异常检测是特定于上下文的，并且代表性数据集的可用性严重限制了真实世界的准确性。此外，目前大多数最先进方法报告的指标往往不能反映模型在现实世界场景中的表现。在本文中，我们介绍了Charlotte异常数据集（CHAD）。CHAD是一个商业停车场环境中的高分辨率、多摄像机异常数据集。除了帧级异常标签外，CHAD是第一个包含每个参与者的边界框、身份和姿势注释的异常数据集。这对于基于骨架的异常检测尤其有益，因为它在现实世界中的计算需求较低。CHAD也是第一个包含同一场景的多个视图的异常数据集。CHAD有四个摄像头视图和超过115万帧，是最大的全注释异常检测数据集，包括人物注释，从用于智能视频监控应用的固定摄像头的连续视频流中收集。为了证明CHAD在训练和评估中的有效性，我们在CHAD上对两种最先进的基于骨架的异常检测算法进行了基准测试，并提供了全面的分析，包括定量结果和定性检查。数据集位于https://github.com/TeCSAR-UNCC/CHAD.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>43</td>
                              <td>
                                <a href="https://arxiv.org/abs/2212.09258v3" target="_blank">2212.09258v3</a>
                              </td>
                              <td>CHAD: Charlotte Anomaly Dataset</td>
                              <td>Armin Danesh Pazho</td>
                              <td>2022-12-19</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2212_09258v3_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2212.09258v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/tecsar-uncc/chad" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2301_09489v4_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2301_09489v4_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2301_09489v4_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2301_09489v4_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Detecting the anomaly of human behavior is paramount to timely recognizing endangering situations, such as street fights or elderly falls. However, anomaly detection is complex since anomalous events are rare and because it is an open set recognition task, i.e., what is anomalous at inference has not been observed at training. We propose COSKAD, a novel model that encodes skeletal human motion by a graph convolutional network and learns to COntract SKeletal kinematic embeddings onto a latent hypersphere of minimum volume for Video Anomaly Detection. We propose three latent spaces: the commonly-adopted Euclidean and the novel spherical and hyperbolic. All variants outperform the state-of-the-art on the most recent UBnormal dataset, for which we contribute a human-related version with annotated skeletons. COSKAD sets a new state-of-the-art on the human-related versions of ShanghaiTech Campus and CUHK Avenue, with performance comparable to video-based methods. Source code and dataset will be released upon acceptance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2301_09489v4_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>检测人类行为的异常对于及时识别危险情况至关重要，例如街头斗殴或老年人摔倒。然而，异常检测是复杂的，因为异常事件是罕见的，并且因为它是一个开集识别任务，即，在训练中没有观察到推理中的异常。我们提出了COSKAD，这是一种新的模型，通过图卷积网络对人体骨骼运动进行编码，并学习将骨骼运动学嵌入集中到最小体积的潜在超球体上，用于视频异常检测。我们提出了三个潜在空间：常用的欧几里得空间和新颖的球面和双曲面空间。在最新的UBnormal数据集上，所有变体都优于最先进的变体，我们为其提供了一个带有注释骨架的人类相关版本。COSKAD在上海科技大学校园和中大大道的人类相关版本上设置了一个新的最先进的技术，其性能可与基于视频的方法相媲美。源代码和数据集将在验收后发布。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>44</td>
                              <td>
                                <a href="https://arxiv.org/abs/2301.09489v4" target="_blank">2301.09489v4</a>
                              </td>
                              <td>Contracting Skeletal Kinematics for Human-Related Video Anomaly Detection</td>
                              <td>Alessandro Flaborea</td>
                              <td>2023-01-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2301_09489v4_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2301.09489v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_13611v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A New Comprehensive Benchmark for Semi-supervised Video Anomaly Detection and Anticipation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_13611v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_13611v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_13611v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Semi-supervised video anomaly detection (VAD) is a critical task in the intelligent surveillance system. However, an essential type of anomaly in VAD named scene-dependent anomaly has not received the attention of researchers. Moreover, there is no research investigating anomaly anticipation, a more significant task for preventing the occurrence of anomalous events. To this end, we propose a new comprehensive dataset, NWPU Campus, containing 43 scenes, 28 classes of abnormal events, and 16 hours of videos. At present, it is the largest semi-supervised VAD dataset with the largest number of scenes and classes of anomalies, the longest duration, and the only one considering the scene-dependent anomaly. Meanwhile, it is also the first dataset proposed for video anomaly anticipation. We further propose a novel model capable of detecting and anticipating anomalous events simultaneously. Compared with 7 outstanding VAD algorithms in recent years, our method can cope with scene-dependent anomaly detection and anomaly anticipation both well, achieving state-of-the-art performance on ShanghaiTech, CUHK Avenue, IITB Corridor and the newly proposed NWPU Campus datasets consistently. Our dataset and code is available at: https://campusvad.github.io.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_13611v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>半监督视频异常检测（VAD）是智能监控系统中的一项关键任务。然而，VAD中一种重要的异常类型——场景相关异常并没有引起研究人员的注意。此外，还没有研究异常预期，这是预防异常事件发生的一项更重要的任务。为此，我们提出了一个新的综合数据集，NWPU Campus，包含43个场景、28类异常事件和16个小时的视频。目前，它是最大的半监督VAD数据集，具有最多的场景和异常类别，持续时间最长，也是唯一一个考虑场景相关异常的数据集。同时，它也是第一个提出用于视频异常预测的数据集。我们进一步提出了一种能够同时检测和预测异常事件的新模型。与近年来7种优秀的VAD算法相比，我们的方法能够很好地处理场景相关的异常检测和异常预测，在上海科技、中大大道、IITB走廊和新提出的NWPU校园数据集上始终如一地实现了最先进的性能。我们的数据集和代码位于：https://campusvad.github.io.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>45</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.13611v1" target="_blank">2305.13611v1</a>
                              </td>
                              <td>A New Comprehensive Benchmark for Semi-supervised Video Anomaly Detection and Anticipation</td>
                              <td>Congqi Cao</td>
                              <td>2023-05-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_13611v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.13611v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2203_03677v4_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Object-centric and memory-guided normality reconstruction for video anomaly detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2203_03677v4_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2203_03677v4_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2203_03677v4_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper addresses video anomaly detection problem for videosurveillance. Due to the inherent rarity and heterogeneity of abnormal events, the problem is viewed as a normality modeling strategy, in which our model learns object-centric normal patterns without seeing anomalous samples during training. The main contributions consist in coupling pretrained object-level action features prototypes with a cosine distance-based anomaly estimation function, therefore extending previous methods by introducing additional constraints to the mainstream reconstruction-based strategy. Our framework leverages both appearance and motion information to learn object-level behavior and captures prototypical patterns within a memory module. Experiments on several well-known datasets demonstrate the effectiveness of our method as it outperforms current state-of-the-art on most relevant spatio-temporal evaluation metrics.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2203_03677v4_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文讨论了视频监控中的视频异常检测问题。由于异常事件固有的罕见性和异质性，该问题被视为一种正态建模策略，在该策略中，我们的模型在训练过程中学习以对象为中心的正态模式，而不会看到异常样本。主要贡献在于将预训练的对象级动作特征原型与基于余弦距离的异常估计函数相耦合，从而通过向主流的基于重建的策略引入额外的约束来扩展以前的方法。我们的框架利用外观和运动信息来学习对象级别的行为，并在内存模块中捕获原型模式。在几个著名数据集上的实验证明了我们的方法的有效性，因为它在大多数相关的时空评估指标上都优于当前最先进的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>46</td>
                              <td>
                                <a href="https://arxiv.org/abs/2203.03677v4" target="_blank">2203.03677v4</a>
                              </td>
                              <td>Object-centric and memory-guided normality reconstruction for video anomaly detection</td>
                              <td>Khalil Bergaoui</td>
                              <td>2022-03-07</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2203_03677v4_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2203.03677v4" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2305_07328v1_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Configurable Spatial-Temporal Hierarchical Analysis for Flexible Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2305_07328v1_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2305_07328v1_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2305_07328v1_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Video anomaly detection (VAD) is a vital task with great practical applications in industrial surveillance, security system, and traffic control. Unlike previous unsupervised VAD methods that adopt a fixed structure to learn normality without considering different detection demands, we design a spatial-temporal hierarchical architecture (STHA) as a configurable architecture to flexibly detect different degrees of anomaly. The comprehensive structure of the STHA is delineated into a tripartite hierarchy, encompassing the following tiers: the stream level, the stack level, and the block level. Specifically, we design several auto-encoder-based blocks that possess varying capacities for extracting normal patterns. Then, we stack blocks according to the complexity degrees with both intra-stack and inter-stack residual links to learn hierarchical normality gradually. Considering the multisource knowledge of videos, we also model the spatial normality of video frames and temporal normality of RGB difference by designing two parallel streams consisting of stacks. Thus, STHA can provide various representation learning abilities by expanding or contracting hierarchically to detect anomalies of different degrees. Since the anomaly set is complicated and unbounded, our STHA can adjust its detection ability to adapt to the human detection demands and the complexity degree of anomaly that happened in the history of a scene. We conduct experiments on three benchmarks and perform extensive analysis, and the results demonstrate that our method performs comparablely to the state-of-the-art methods. In addition, we design a toy dataset to prove that our model can better balance the learning ability to adapt to different detection demands.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2305_07328v1_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频异常检测（VAD）在工业监控、安全系统和交通控制中具有重要的实际应用价值。与以前的无监督VAD方法采用固定结构来学习正态性而不考虑不同的检测需求不同，我们设计了一种时空分层结构（STHA）作为一种可配置的结构，以灵活地检测不同程度的异常。STHA的综合结构被划分为三个层次，包括以下层次：流级别、堆栈级别和块级别。具体来说，我们设计了几个基于自动编码器的块，这些块具有不同的提取正常模式的能力。然后，我们根据复杂度将块与堆栈内和堆栈间的残差链接进行叠加，以逐步学习层次正规性。考虑到视频的多源知识，我们还通过设计两个由堆栈组成的并行流来对视频帧的空间正态性和RGB差的时间正态性进行建模。因此，STHA可以通过分层扩展或收缩来检测不同程度的异常，从而提供各种表示学习能力。由于异常集是复杂且无边界的，我们的STHA可以调整其检测能力，以适应人类的检测需求和场景历史中发生的异常的复杂程度。我们在三个基准上进行了实验，并进行了广泛的分析，结果表明，我们的方法与最先进的方法性能相当。此外，我们设计了一个玩具数据集来证明我们的模型可以更好地平衡学习能力，以适应不同的检测需求。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>47</td>
                              <td>
                                <a href="https://arxiv.org/abs/2305.07328v1" target="_blank">2305.07328v1</a>
                              </td>
                              <td>Configurable Spatial-Temporal Hierarchical Analysis for Flexible Video Anomaly Detection</td>
                              <td>Kai Cheng</td>
                              <td>2023-05-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2305_07328v1_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2305.07328v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_07697v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multi-Task Learning based Video Anomaly Detection with Attention</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_07697v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_07697v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_07697v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Multi-task learning based video anomaly detection methods combine multiple proxy tasks in different branches to detect video anomalies in different situations. Most existing methods either do not combine complementary tasks to effectively cover all motion patterns, or the class of the objects is not explicitly considered. To address the aforementioned shortcomings, we propose a novel multi-task learning based method that combines complementary proxy tasks to better consider the motion and appearance features. We combine the semantic segmentation and future frame prediction tasks in a single branch to learn the object class and consistent motion patterns, and to detect respective anomalies simultaneously. In the second branch, we added several attention mechanisms to detect motion anomalies with attention to object parts, the direction of motion, and the distance of the objects from the camera. Our qualitative results show that the proposed method considers the object class effectively and learns motion with attention to the aforementioned important factors which results in a precise motion modeling and a better motion anomaly detection. Additionally, quantitative results show the superiority of our method compared with state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_07697v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于多任务学习的视频异常检测方法将不同分支中的多个代理任务相结合，以检测不同情况下的视频异常。大多数现有的方法要么没有结合互补的任务来有效地覆盖所有的运动模式，要么没有明确考虑对象的类别。为了解决上述缺点，我们提出了一种新的基于多任务学习的方法，该方法结合互补代理任务来更好地考虑运动和外观特征。我们将语义分割和未来帧预测任务组合在一个分支中，以学习对象类别和一致的运动模式，并同时检测各自的异常。在第二个分支中，我们添加了几个注意力机制来检测运动异常，注意物体的部位、运动方向和物体与相机的距离。我们的定性结果表明，所提出的方法有效地考虑了对象类别，并在注意上述重要因素的情况下学习运动，从而实现了精确的运动建模和更好的运动异常检测。此外，定量结果表明，与最先进的方法相比，我们的方法具有优越性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>48</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.07697v2" target="_blank">2210.07697v2</a>
                              </td>
                              <td>Multi-Task Learning based Video Anomaly Detection with Attention</td>
                              <td>Mohammad Baradaran</td>
                              <td>2022-10-14</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_07697v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.07697v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2210_06688v2_7">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Overlooked Video Classification in Weakly Supervised Video Anomaly Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2210_06688v2_7_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2210_06688v2_7_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2210_06688v2_7_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Current weakly supervised video anomaly detection algorithms mostly use multiple instance learning (MIL) or their varieties. Almost all recent approaches focus on how to select the correct snippets for training to improve the performance. They overlook or do not realize the power of video classification in boosting the performance of anomaly detection. In this paper, we study explicitly the power of video classification supervision using a BERT or LSTM. With this BERT or LSTM, CNN features of all snippets of a video can be aggregated into a single feature which can be used for video classification. This simple yet powerful video classification supervision, combined into the MIL framework, brings extraordinary performance improvement on all three major video anomaly detection datasets. Particularly it improves the mean average precision (mAP) on the XD-Violence from SOTA 78.84\% to new 82.10\%. The source code is available at https://github.com/wjtan99/BERT_Anomaly_Video_Classification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2210_06688v2_7_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>目前的弱监督视频异常检测算法大多使用多实例学习（MIL）或其变体。几乎所有最近的方法都集中在如何选择正确的片段进行训练以提高性能上。他们忽视或没有意识到视频分类在提高异常检测性能方面的作用。在本文中，我们明确地研究了使用BERT或LSTM的视频分类监督的能力。通过这种BERT或LSTM，可以将视频的所有片段的CNN特征聚合为可用于视频分类的单个特征。这种简单而强大的视频分类监督，结合到MIL框架中，在所有三个主要的视频异常检测数据集上都带来了非凡的性能改进。特别是它将XD暴力的平均精度（mAP）从SOTA的78.84%提高到新的82.10%。源代码位于https://github.com/wjtan99/BERT_Anomaly_Video_Classification.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>49</td>
                              <td>
                                <a href="https://arxiv.org/abs/2210.06688v2" target="_blank">2210.06688v2</a>
                              </td>
                              <td>Overlooked Video Classification in Weakly Supervised Video Anomaly Detection</td>
                              <td>Weijun Tan</td>
                              <td>2022-10-13</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2210_06688v2_7"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2210.06688v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/wjtan99/bert_anomaly_video_classification" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <div class="tab-pane fade" id="MAE"
              role="tabpanel">
              <div class="py-2">
                <div class="container">
                  <div class="row">
                    <div class="col-md-12">
                      <div class="table-responsive">
                        <table class="table">
                          <thead>
                            <tr>
                              <th>Index</th>
                              <th>Arxiv ID</th>
                              <th>Title</th>
                              <th>First Author</th>
                              <th>Submit Date</th>
                              <th>Abstract</th>
                              <th>PDF Links</th>
                              <th>Github Code</th>
                              <th>Paper With Code</th>
                            </tr>
                          </thead>
                          <tbody>
                            
                            <div class="modal" id="ID_2402_13217v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">VideoPrism: A Foundational Visual Encoder for Video Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_13217v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_13217v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_13217v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_13217v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们介绍VideoPrism，这是一种通用视频编码器，可通过单个冻结模型处理各种视频理解任务。我们在异构语料库上预训练VideoPrism，该语料库包含36M个高质量视频字幕对和582M个具有噪声并行文本的视频片段（例如ASR转录本）。预训练方法通过语义视频嵌入的全局局部提取和令牌混洗方案改进了掩蔽自动编码，使VideoPrism能够主要关注视频模态，同时利用与视频相关的宝贵文本。我们在四大类视频理解任务上广泛测试了VideoPrism，从网络视频问答到科学简历，在33个视频理解基准中的30个上实现了最先进的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>0</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.13217v1" target="_blank">2402.13217v1</a>
                              </td>
                              <td>VideoPrism: A Foundational Visual Encoder for Video Understanding</td>
                              <td>Long Zhao</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_13217v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.13217v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_12814v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Exploring the Interaction of Creative Writers with AI-Powered Writing Tools</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_12814v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_12814v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_12814v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>AI-based virtual assistants are increasingly used to support daily ideation tasks. The values or bias present in these agents can influence output in hidden ways. They may also affect how people perceive the ideas produced with these AI agents and lead to implications for the design of AI-based tools. We explored the effects of AI agents with different values on the ideation process and user perception of idea quality, ownership, agent competence, and values present in the output. Our study tasked 180 participants with brainstorming practical solutions to a set of problems with AI agents of different values. Results show no significant difference in self-evaluation of idea quality and perception of the agent based on value alignment; however, ideas generated reflected the AI's values and feeling of ownership is affected. This highlights an intricate interplay between AI values and human ideation, suggesting careful design considerations for future AI-supported brainstorming tools.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_12814v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>基于人工智能的虚拟助理越来越多地用于支持日常构思任务。这些代理中存在的值或偏差可以以隐藏的方式影响输出。它们还可能影响人们对这些人工智能代理产生的想法的感知，并对基于人工智能的工具的设计产生影响。我们探讨了具有不同价值观的人工智能主体对构思过程和用户对想法质量、所有权、主体能力和输出中存在的价值观的感知的影响。我们的研究让180名参与者集思广益，用不同价值的人工智能代理解决一系列问题。结果显示，基于价值取向的主体对观念质量和感知的自我评价没有显著差异；然而，产生的想法反映了人工智能的价值观，所有权感受到了影响。这突出了人工智能价值观和人类思维之间错综复杂的相互作用，为未来人工智能支持的头脑风暴工具提供了仔细的设计考虑。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>1</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.12814v1" target="_blank">2402.12814v1</a>
                              </td>
                              <td>Exploring the Interaction of Creative Writers with AI-Powered Writing Tools</td>
                              <td>Alicia Guo</td>
                              <td>2024-02-20</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_12814v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.12814v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11664v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Interpretable Short-Term Load Forecasting via Multi-Scale Temporal Decomposition</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11664v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11664v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11664v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Rapid progress in machine learning and deep learning has enabled a wide range of applications in the electricity load forecasting of power systems, for instance, univariate and multivariate short-term load forecasting. Though the strong capabilities of learning the non-linearity of the load patterns and the high prediction accuracy have been achieved, the interpretability of typical deep learning models for electricity load forecasting is less studied. This paper proposes an interpretable deep learning method, which learns a linear combination of neural networks that each attends to an input time feature. We also proposed a multi-scale time series decomposition method to deal with the complex time patterns. Case studies have been carried out on the Belgium central grid load dataset and the proposed model demonstrated better accuracy compared to the frequently applied baseline model. Specifically, the proposed multi-scale temporal decomposition achieves the best MSE, MAE and RMSE of 0.52, 0.57 and 0.72 respectively. As for interpretability, on one hand, the proposed method displays generalization capability. On the other hand, it can demonstrate not only the feature but also the temporal interpretability compared to other baseline methods. Besides, the global time feature interpretabilities are also obtained. Obtaining global feature interpretabilities allows us to catch the overall patterns, trends, and cyclicality in load data while also revealing the significance of various time-related features in forming the final outputs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11664v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>机器学习和深度学习的快速发展使其在电力系统的电力负荷预测中有了广泛的应用，例如单变量和多变量短期负荷预测。尽管已经实现了学习负荷模式非线性的强大能力和高预测精度，但用于电力负荷预测的典型深度学习模型的可解释性研究较少。本文提出了一种可解释的深度学习方法，该方法学习每个神经网络都关注输入时间特征的线性组合。我们还提出了一种多尺度时间序列分解方法来处理复杂的时间模式。对比利时中央电网负荷数据集进行了案例研究，与常用的基线模型相比，所提出的模型具有更好的准确性。具体而言，所提出的多尺度时间分解分别实现了0.52、0.57和0.72的最佳MSE、MAE和RMSE。在可解释性方面，一方面，该方法具有泛化能力。另一方面，与其他基线方法相比，它不仅可以证明特征，还可以证明时间上的可解释性。此外，还获得了全局时间特征的可解释性。获得全局特征的可解释性使我们能够捕捉负载数据的总体模式、趋势和周期性，同时揭示各种与时间相关的特征在形成最终输出中的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>2</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11664v1" target="_blank">2402.11664v1</a>
                              </td>
                              <td>Interpretable Short-Term Load Forecasting via Multi-Scale Temporal Decomposition</td>
                              <td>Yuqi Jiang</td>
                              <td>2024-02-18</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11664v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11664v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_11337v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Learning by Reconstruction Produces Uninformative Features For Perception</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_11337v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_11337v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_11337v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Input space reconstruction is an attractive representation learning paradigm. Despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90\% of the pixel variance can be solved with 45\% test accuracy. Using the bottom subspace instead, accounting for only 20\% of the pixel variance, reaches 55\% test accuracy. The features for perception being learned last explains the need for long training time, e.g., with Masked Autoencoders. Learning by denoising is a popular strategy to alleviate that misalignment. We prove that while some noise strategies such as masking are indeed beneficial, others such as additive Gaussian noise are not. Yet, even in the case of masking, we find that the benefits vary as a function of the mask's shape, ratio, and the considered dataset. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide first clues on how to detect if a noise strategy is never beneficial regardless of the perception task.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_11337v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>输入空间重构是一种极具吸引力的表征学习范式。尽管重建和生成是可解释的，但我们发现了通过重建学习和感知学习之间的错位。我们证明，前者将模型的容量分配给解释观测方差的数据子空间——后者的子空间具有无信息特征。例如，将图像投影到顶部子空间上解释90%像素方差的监督TinyImagenet任务可以以45%的测试精度解决。使用底部子空间，只占像素方差的20%，达到55%的测试精度。正在学习的感知功能最后解释了对长训练时间的需求，例如，使用掩蔽自动编码器。通过去噪学习是一种流行的策略来缓解这种错位。我们证明，虽然一些噪声策略（如掩蔽）确实是有益的，但其他策略（如加性高斯噪声）则不然。然而，即使在掩蔽的情况下，我们也发现其益处随着掩蔽的形状、比率和所考虑的数据集的变化而变化。虽然在不了解感知任务的情况下调整噪声策略似乎很有挑战性，但我们提供了关于如何检测噪声策略是否无论感知任务如何都是有益的第一条线索。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>3</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.11337v1" target="_blank">2402.11337v1</a>
                              </td>
                              <td>Learning by Reconstruction Produces Uninformative Features For Perception</td>
                              <td>Randall Balestriero</td>
                              <td>2024-02-17</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_11337v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.11337v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_10434v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Parametric Augmentation for Time Series Contrastive Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_10434v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_10434v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_10434v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Modern techniques like contrastive learning have been effectively used in many areas, including computer vision, natural language processing, and graph-structured data. Creating positive examples that assist the model in learning robust and discriminative representations is a crucial stage in contrastive learning approaches. Usually, preset human intuition directs the selection of relevant data augmentations. Due to patterns that are easily recognized by humans, this rule of thumb works well in the vision and language domains. However, it is impractical to visually inspect the temporal structures in time series. The diversity of time series augmentations at both the dataset and instance levels makes it difficult to choose meaningful augmentations on the fly. In this study, we address this gap by analyzing time series data augmentation using information theory and summarizing the most commonly adopted augmentations in a unified format. We then propose a contrastive learning framework with parametric augmentation, AutoTCL, which can be adaptively employed to support time series representation learning. The proposed approach is encoder-agnostic, allowing it to be seamlessly integrated with different backbone encoders. Experiments on univariate forecasting tasks demonstrate the highly competitive results of our method, with an average 6.5\% reduction in MSE and 4.7\% in MAE over the leading baselines. In classification tasks, AutoTCL achieves a $1.2\%$ increase in average accuracy.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_10434v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>对比学习等现代技术已在许多领域得到有效应用，包括计算机视觉、自然语言处理和图形结构化数据。创建积极的例子来帮助模型学习稳健和有区别的表征是对比学习方法的关键阶段。通常，预设的人类直觉指导相关数据增强的选择。由于模式很容易被人类识别，这种经验法则在视觉和语言领域很有效。然而，以视觉方式检查时间序列中的时间结构是不切实际的。数据集和实例级别的时间序列增强的多样性使得很难在动态中选择有意义的增强。在这项研究中，我们通过使用信息论分析时间序列数据增强，并以统一的格式总结最常用的增强，来解决这一差距。然后，我们提出了一个具有参数增强的对比学习框架AutoTCL，它可以自适应地用于支持时间序列表示学习。所提出的方法与编码器无关，允许它与不同的骨干编码器无缝集成。在单变量预测任务上的实验证明了我们方法的高度竞争性结果，与领先的基线相比，MSE平均减少了6.5%，MAE平均减少了4.7%。在分类任务中，AutoTCL的平均准确度提高了1.2%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>4</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.10434v1" target="_blank">2402.10434v1</a>
                              </td>
                              <td>Parametric Augmentation for Time Series Contrastive Learning</td>
                              <td>Xu Zheng</td>
                              <td>2024-02-16</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_10434v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.10434v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2211_14307v2_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MAEDAY: MAE for few and zero shot AnomalY-Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2211_14307v2_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2211_14307v2_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2211_14307v2_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We propose using Masked Auto-Encoder (MAE), a transformer model self-supervisedly trained on image inpainting, for anomaly detection (AD). Assuming anomalous regions are harder to reconstruct compared with normal regions. MAEDAY is the first image-reconstruction-based anomaly detection method that utilizes a pre-trained model, enabling its use for Few-Shot Anomaly Detection (FSAD). We also show the same method works surprisingly well for the novel tasks of Zero-Shot AD (ZSAD) and Zero-Shot Foreign Object Detection (ZSFOD), where no normal samples are available. Code is available at https://github.com/EliSchwartz/MAEDAY .</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2211_14307v2_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们建议使用掩模自动编码器（MAE）进行异常检测（AD），这是一种在图像修复方面自我监督训练的变换器模型。假设异常区域与正常区域相比更难重建。MAEDAY是第一种利用预训练模型的基于图像重建的异常检测方法，使其能够用于少镜头异常检测（FSAD）。我们还表明，相同的方法对于零样本AD（ZSAD）和零样本异物检测（ZSFOD）的新任务效果惊人地好，其中没有正常样本可用。代码位于https://github.com/EliSchwartz/MAEDAY .</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>5</td>
                              <td>
                                <a href="https://arxiv.org/abs/2211.14307v2" target="_blank">2211.14307v2</a>
                              </td>
                              <td>MAEDAY: MAE for few and zero shot AnomalY-Detection</td>
                              <td>Eli Schwartz</td>
                              <td>2022-11-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2211_14307v2_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2211.14307v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/elischwartz/maeday" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04632v2_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Hypercomplex neural network in time series forecasting of stock data</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04632v2_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04632v2_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04632v2_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The goal of this paper is to test three classes of neural network (NN) architectures based on four-dimensional (4D) hypercomplex algebras for time series prediction. We evaluate different architectures, varying the input layers to include convolutional, Long Short-Term Memory (LSTM), or dense hypercomplex layers for 4D algebras. Four related Stock Market time series are used as input data, with the prediction focused on one of them. Hyperparameter optimization for each architecture class was conducted to compare the best-performing neural networks within each class. The results indicate that, in most cases, architectures with hypercomplex dense layers achieve similar Mean Absolute Error (MAE) accuracy compared to other architectures, but with significantly fewer trainable parameters. Consequently, hypercomplex neural networks demonstrate the ability to learn and process time series data faster than the other tested architectures. Additionally, it was found that the ordering of the input time series have a notable impact on effectiveness.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04632v2_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文的目标是测试三类基于四维（4D）超复代数的神经网络结构用于时间序列预测。我们评估不同的体系结构，改变输入层以包括4D代数的卷积、长短期记忆（LSTM）或密集超复数层。四个相关的股市时间序列被用作输入数据，预测集中在其中一个时间序列上。对每个体系结构类别进行超参数优化，以比较每个类别中性能最好的神经网络。结果表明，在大多数情况下，与其他架构相比，具有超复杂密集层的架构实现了相似的平均绝对误差（MAE）精度，但可训练参数明显较少。因此，超复杂神经网络表现出比其他测试架构更快地学习和处理时间序列数据的能力。此外，还发现输入时间序列的排序对有效性有显著影响。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>6</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04632v2" target="_blank">2401.04632v2</a>
                              </td>
                              <td>Hypercomplex neural network in time series forecasting of stock data</td>
                              <td>Radosław Kycia</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04632v2_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04632v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08035v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Multiple Random Masking Autoencoder Ensembles for Robust Multimodal Semi-supervised Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08035v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08035v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08035v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>There is an increasing number of real-world problems in computer vision and machine learning requiring to take into consideration multiple interpretation layers (modalities or views) of the world and learn how they relate to each other. For example, in the case of Earth Observations from satellite data, it is important to be able to predict one observation layer (e.g. vegetation index) from other layers (e.g. water vapor, snow cover, temperature etc), in order to best understand how the Earth System functions and also be able to reliably predict information for one layer when the data is missing (e.g. due to measurement failure or error).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08035v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在计算机视觉和机器学习中，越来越多的现实世界问题需要考虑世界的多个解释层（模态或视图），并了解它们之间的关系。例如，在根据卫星数据进行地球观测的情况下，重要的是能够从其他层（如水蒸气、积雪、温度等）预测一个观测层（如植被指数），以便最好地了解地球系统的功能，并能够在数据丢失时（如由于测量失败或误差）可靠地预测一层的信息。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>7</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08035v1" target="_blank">2402.08035v1</a>
                              </td>
                              <td>Multiple Random Masking Autoencoder Ensembles for Robust Multimodal Semi-supervised Learning</td>
                              <td>Alexandru-Raul Todoran</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08035v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08035v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_08023v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">UGMAE: A Unified Framework for Graph Masked Autoencoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_08023v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_08023v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_08023v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Generative self-supervised learning on graphs, particularly graph masked autoencoders, has emerged as a popular learning paradigm and demonstrated its efficacy in handling non-Euclidean data. However, several remaining issues limit the capability of existing methods: 1) the disregard of uneven node significance in masking, 2) the underutilization of holistic graph information, 3) the ignorance of semantic knowledge in the representation space due to the exclusive use of reconstruction loss in the output space, and 4) the unstable reconstructions caused by the large volume of masked contents. In light of this, we propose UGMAE, a unified framework for graph masked autoencoders to address these issues from the perspectives of adaptivity, integrity, complementarity, and consistency. Specifically, we first develop an adaptive feature mask generator to account for the unique significance of nodes and sample informative masks (adaptivity). We then design a ranking-based structure reconstruction objective joint with feature reconstruction to capture holistic graph information and emphasize the topological proximity between neighbors (integrity). After that, we present a bootstrapping-based similarity module to encode the high-level semantic knowledge in the representation space, complementary to the low-level reconstruction in the output space (complementarity). Finally, we build a consistency assurance module to provide reconstruction objectives with extra stabilized consistency targets (consistency). Extensive experiments demonstrate that UGMAE outperforms both contrastive and generative state-of-the-art baselines on several tasks across multiple datasets.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_08023v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图上的生成自监督学习，特别是图屏蔽自动编码器，已经成为一种流行的学习范式，并证明了其在处理非欧几里得数据方面的有效性。然而，剩下的几个问题限制了现有方法的能力：1）在掩蔽中忽略不均匀的节点显著性，2）整体图信息的利用不足，3）由于在输出空间中独占使用重建损失而忽略表示空间中的语义知识，以及4）由大量掩蔽内容引起的不稳定重建。有鉴于此，我们提出了UGMAE，这是一个用于图屏蔽自动编码器的统一框架，从自适应、完整性、互补性和一致性的角度来解决这些问题。具体来说，我们首先开发了一个自适应特征掩码生成器，以考虑节点和样本信息掩码的独特意义（自适应性）。然后，我们设计了一个基于排序的结构重建目标与特征重建相结合，以捕获整体图信息，并强调邻居之间的拓扑接近性（完整性）。之后，我们提出了一个基于自举的相似性模块来对表示空间中的高级语义知识进行编码，与输出空间中的低级重构（互补性）互补。最后，我们构建了一个一致性保证模块，为重建目标提供额外稳定的一致性目标（一致性）。大量实验表明，UGMAE在多个数据集的几个任务上都优于对比和生成最先进的基线。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>8</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.08023v1" target="_blank">2402.08023v1</a>
                              </td>
                              <td>UGMAE: A Unified Framework for Graph Masked Autoencoders</td>
                              <td>Yijun Tian</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_08023v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.08023v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07370v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07370v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07370v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07370v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Face swapping has gained significant attention for its varied applications. The majority of previous face swapping approaches have relied on the seesaw game training scheme, which often leads to the instability of the model training and results in undesired samples with blended identities due to the target identity leakage problem. This paper introduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach designed to enhance face swapping model training. Our training scheme addresses the limitations of traditional training methods by circumventing the conventional seesaw game and introducing clear ground truth through its self-reconstruction training regime. It effectively mitigates identity leakage by masking facial regions of the input images and utilizing learned disentangled identity and non-identity features. Additionally, we tackle the shape misalignment problem with new techniques including perforation confusion and random mesh scaling, and establishes a new state-of-the-art, surpassing other baseline methods, preserving both identity and non-identity attributes, without sacrificing on either aspect.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07370v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>人脸交换因其应用的多样性而备受关注。以前的大多数人脸交换方法都依赖于跷跷板游戏训练方案，这往往导致模型训练的不稳定性，并由于目标身份泄漏问题而导致混合身份的不期望样本。本文介绍了形状不可知掩模自动编码器（SAMAE）训练方案，这是一种新的自监督方法，旨在增强人脸交换模型的训练。我们的训练方案解决了传统训练方法的局限性，绕过了传统的拉锯游戏，并通过其自我重建训练机制引入了明确的基本事实。它通过掩蔽输入图像的面部区域并利用学习到的解纠缠的身份和非身份特征，有效地缓解了身份泄漏。此外，我们使用新技术解决了形状错位问题，包括穿孔混淆和随机网格缩放，并建立了一种新的最先进技术，超越了其他基线方法，在不牺牲任何一个方面的情况下保留了同一性和非同一性属性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>9</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07370v1" target="_blank">2402.07370v1</a>
                              </td>
                              <td>SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder</td>
                              <td>Jaeseong Lee</td>
                              <td>2024-02-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07370v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07370v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07225v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rethinking Graph Masked Autoencoders through Alignment and Uniformity</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07225v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07225v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07225v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Self-supervised learning on graphs can be bifurcated into contrastive and generative methods. Contrastive methods, also known as graph contrastive learning (GCL), have dominated graph self-supervised learning in the past few years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles the momentum behind generative methods. Despite the empirical success of GraphMAE, there is still a dearth of theoretical understanding regarding its efficacy. Moreover, while both generative and contrastive methods have been shown to be effective, their connections and differences have yet to be thoroughly investigated. Therefore, we theoretically build a bridge between GraphMAE and GCL, and prove that the node-level reconstruction objective in GraphMAE implicitly performs context-level GCL. Based on our theoretical analysis, we further identify the limitations of the GraphMAE from the perspectives of alignment and uniformity, which have been considered as two key properties of high-quality representations in GCL. We point out that GraphMAE's alignment performance is restricted by the masking strategy, and the uniformity is not strictly guaranteed. To remedy the aforementioned limitations, we propose an Alignment-Uniformity enhanced Graph Masked AutoEncoder, named AUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy to provide hard-to-align samples, which improves the alignment performance. Meanwhile, we introduce an explicit uniformity regularizer to ensure the uniformity of the learned representations. Experimental results on benchmark datasets demonstrate the superiority of our model over existing state-of-the-art methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07225v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>图上的自监督学习可以分为对比和生成两种方法。对比方法，也称为图对比学习（GCL），在过去几年中主导了图自监督学习，但最近出现的图屏蔽自动编码器（GraphMAE）重新点燃了生成方法背后的动力。尽管GraphMAE在经验上取得了成功，但对其功效仍缺乏理论理解。此外，尽管生成方法和对比方法都被证明是有效的，但它们之间的联系和差异还有待深入研究。因此，我们在理论上搭建了GraphMAE和GCL之间的桥梁，并证明了GraphMAE中的节点级重构目标隐含地执行上下文级GCL。基于我们的理论分析，我们从对齐和一致性的角度进一步确定了GraphMAE的局限性，这被认为是GCL中高质量表示的两个关键特性。我们指出，GraphMAE的对准性能受到掩蔽策略的限制，并且不能严格保证一致性。为了弥补上述限制，我们提出了一种增强对齐一致性的图掩码自动编码器，名为AUG-MAE。具体来说，我们提出了一种易到难的对抗性掩蔽策略来提供难以对准的样本，这提高了对准性能。同时，我们引入了一个显式一致性正则化子来确保学习表示的一致性。在基准数据集上的实验结果表明，我们的模型优于现有的最先进的方法。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>10</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07225v1" target="_blank">2402.07225v1</a>
                              </td>
                              <td>Rethinking Graph Masked Autoencoders through Alignment and Uniformity</td>
                              <td>Liang Wang</td>
                              <td>2024-02-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07225v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07225v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/azureleon1/aug-mae" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07164v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07164v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07164v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07164v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Air pollution represents a pivotal environmental challenge globally, playing a major role in climate change via greenhouse gas emissions and negatively affecting the health of billions. However predicting the spatial and temporal patterns of pollutants remains challenging. The scarcity of ground-based monitoring facilities and the dependency of air pollution modeling on comprehensive datasets, often inaccessible for numerous areas, complicate this issue. In this work, we introduce GeoFormer, a compact model that combines a vision transformer module with a highly efficient time-series transformer module to predict surface-level nitrogen dioxide (NO2) concentrations from Sentinel-5P satellite imagery. We train the proposed model to predict surface-level NO2 measurements using a dataset we constructed with Sentinel-5P images of ground-level monitoring stations, and their corresponding NO2 concentration readings. The proposed model attains high accuracy (MAE 5.65), demonstrating the efficacy of combining vision and time-series transformer architectures to harness satellite-derived data for enhanced GHG emission insights, proving instrumental in advancing climate change monitoring and emission regulation efforts globally.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07164v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>空气污染是全球一个关键的环境挑战，通过温室气体排放在气候变化中发挥着重要作用，并对数十亿人的健康产生了负面影响。然而，预测污染物的空间和时间模式仍然具有挑战性。地面监测设施的稀缺性以及空气污染建模依赖于综合数据集，而许多地区往往无法获得这些数据集，使这一问题复杂化。在这项工作中，我们介绍了GeoFormer，这是一个紧凑的模型，它将视觉转换器模块与高效的时间序列转换器模块相结合，用于从Sentinel-5P卫星图像中预测地表二氧化氮（NO2）浓度。我们使用我们用地面监测站的Sentinel-5P图像及其相应的NO2浓度读数构建的数据集，对所提出的模型进行训练，以预测地表NO2测量值。所提出的模型达到了高精度（MAE 5.65），证明了将视觉和时间序列转换器架构相结合，利用卫星衍生数据增强温室气体排放洞察力的有效性，证明了其有助于推进全球气候变化监测和排放监管工作。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>11</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07164v1" target="_blank">2402.07164v1</a>
                              </td>
                              <td>GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring</td>
                              <td>Madhav Khirwar</td>
                              <td>2024-02-11</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07164v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07164v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_06986v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cacophony: An Improved Contrastive Audio-Text Model</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_06986v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_06986v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_06986v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Despite recent improvements in audio-text modeling, audio-text contrastive models still lag behind their image-text counterparts in scale and performance. We propose a method to improve both the scale and the training of audio-text contrastive models. Specifically, we craft a large-scale audio-text dataset consisting of over 13,000 hours of text-labeled audio, aided by large language model (LLM) processing and audio captioning. Further, we employ an masked autoencoder (MAE) pre-pretraining phase with random patch dropout, which allows us to both scale unlabeled audio datasets and train efficiently with variable length audio. After MAE pre-pretraining of our audio encoder, we train a contrastive model with an auxiliary captioning objective. Our final model, which we name Cacophony, achieves state-of-the-art performance on audio-text retrieval tasks, and exhibits competitive results on other downstream tasks such as zero-shot classification.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_06986v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>尽管最近在音频文本建模方面有所改进，但音频文本对比模型在规模和性能上仍落后于图像文本对比模型。我们提出了一种提高音频文本对比模型的规模和训练的方法。具体来说，我们在大语言模型（LLM）处理和音频字幕的帮助下，制作了一个由13000多小时的文本标记音频组成的大规模音频文本数据集。此外，我们采用了具有随机补丁丢弃的掩蔽自动编码器（MAE）预预训练阶段，这使我们能够缩放未标记的音频数据集，并使用可变长度的音频进行有效训练。在对我们的音频编码器进行MAE预预训练后，我们训练了一个具有辅助字幕目标的对比模型。我们的最终模型，我们将其命名为Cacophony，在音频文本检索任务上实现了最先进的性能，并在其他下游任务（如零样本分类）上显示了具有竞争力的结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>12</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.06986v1" target="_blank">2402.06986v1</a>
                              </td>
                              <td>Cacophony: An Improved Contrastive Audio-Text Model</td>
                              <td>Ge Zhu</td>
                              <td>2024-02-10</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_06986v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.06986v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/gzhu06/cacophony" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2306_06881v2_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Unmasking Deepfakes: Masked Autoencoding Spatiotemporal Transformers for Enhanced Video Forgery Detection</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2306_06881v2_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2306_06881v2_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2306_06881v2_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present a novel approach for the detection of deepfake videos using a pair of vision transformers pre-trained by a self-supervised masked autoencoding setup. Our method consists of two distinct components, one of which focuses on learning spatial information from individual RGB frames of the video, while the other learns temporal consistency information from optical flow fields generated from consecutive frames. Unlike most approaches where pre-training is performed on a generic large corpus of images, we show that by pre-training on smaller face-related datasets, namely Celeb-A (for the spatial learning component) and YouTube Faces (for the temporal learning component), strong results can be obtained. We perform various experiments to evaluate the performance of our method on commonly used datasets namely FaceForensics++ (Low Quality and High Quality, along with a new highly compressed version named Very Low Quality) and Celeb-DFv2 datasets. Our experiments show that our method sets a new state-of-the-art on FaceForensics++ (LQ, HQ, and VLQ), and obtains competitive results on Celeb-DFv2. Moreover, our method outperforms other methods in the area in a cross-dataset setup where we fine-tune our model on FaceForensics++ and test on CelebDFv2, pointing to its strong cross-dataset generalization ability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2306_06881v2_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们提出了一种新的检测深度伪造视频的方法，使用一对由自监督掩蔽自动编码装置预训练的视觉转换器。我们的方法由两个不同的组件组成，其中一个专注于从视频的各个RGB帧中学习空间信息，而另一个则从连续帧生成的光流场中学习时间一致性信息。与大多数方法不同的是，预训练是在通用的大型图像语料库上进行的，我们表明，通过在较小的人脸相关数据集上进行预训练，即Celeb-a（用于空间学习组件）和YouTube人脸（用于时间学习组件），可以获得很强的结果。我们进行了各种实验，以评估我们的方法在常用数据集上的性能，即FaceForensics++（低质量和高质量，以及名为Very Low Quality的新的高度压缩版本）和Celeb-DFv2数据集。我们的实验表明，我们的方法在FaceForensics++（LQ、HQ和VLQ）上建立了新的最先进的技术，并在Celeb-DFv2上获得了有竞争力的结果。此外，在跨数据集设置中，我们的方法优于该领域的其他方法，我们在FaceForensics++上微调了我们的模型，并在CelebDFv2上进行了测试，这表明它具有强大的跨数据集泛化能力。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>13</td>
                              <td>
                                <a href="https://arxiv.org/abs/2306.06881v2" target="_blank">2306.06881v2</a>
                              </td>
                              <td>Unmasking Deepfakes: Masked Autoencoding Spatiotemporal Transformers for Enhanced Video Forgery Detection</td>
                              <td>Sayantan Das</td>
                              <td>2023-06-12</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2306_06881v2_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2306.06881v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_05830v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_05830v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_05830v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_05830v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Time series analysis is vital for numerous applications, and transformers have become increasingly prominent in this domain. Leading methods customize the transformer architecture from NLP and CV, utilizing a patching technique to convert continuous signals into segments. Yet, time series data are uniquely challenging due to significant distribution shifts and intrinsic noise levels. To address these two challenges,we introduce the Sparse Vector Quantized FFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse vector quantization technique coupled with Reverse Instance Normalization (RevIN) to reduce noise impact and capture sufficient statistics for forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the transformer architecture. Our FFN-free approach trims the parameter count, enhancing computational efficiency and reducing overfitting. Through evaluations across ten benchmark datasets, including the newly introduced CAISO dataset, Sparse-VQ surpasses leading models with a 7.84% and 4.17% decrease in MAE for univariate and multivariate time series forecasting, respectively. Moreover, it can be seamlessly integrated with existing transformer-based models to elevate their performance.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_05830v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>时间序列分析对许多应用至关重要，变压器在这一领域越来越突出。领先的方法利用修补技术将连续信号转换为分段，从NLP和CV定制变压器架构。然而，由于显著的分布变化和固有噪声水平，时间序列数据具有独特的挑战性。为了解决这两个挑战，我们介绍了稀疏矢量量化FFN自由变换器（稀疏VQ）。我们的方法利用稀疏矢量量化技术与反向实例归一化（RevIN）相结合，以减少噪声影响并捕获足够的统计数据进行预测，作为变压器架构中前馈层（FFN）的替代方案。我们的无FFN方法修剪了参数计数，提高了计算效率并减少了过拟合。通过对包括新引入的CAISO数据集在内的十个基准数据集的评估，Sparse VQ在单变量和多变量时间序列预测中的MAE分别下降了7.84%和4.17%，超过了领先的模型。此外，它可以与现有的基于变压器的模型无缝集成，以提高其性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>14</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.05830v1" target="_blank">2402.05830v1</a>
                              </td>
                              <td>Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting</td>
                              <td>Yanjun Zhao</td>
                              <td>2024-02-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_05830v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.05830v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_05382v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_05382v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_05382v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_05382v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\% on average. It also obtains new state-of-the-art self-supervised learning results on detection and segmentation.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_05382v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩蔽自动编码器（MAE）是一种流行的自监督学习方法，在模型预训练中取得了良好的效果。然而，当各种下游任务的数据分布与预训练数据不同时，语义无关的预训练信息可能会导致负迁移，阻碍MAE的可扩展性。为了解决这个问题，我们提出了一种新的基于MAE的预训练范式，即集群条件专家混合（MoCE），它可以训练一次，但为不同的下游任务提供定制的预训练模型。与专家混合（MoE）不同，我们的MoCE通过使用聚类条件门仅用语义相关的图像来训练每个专家。因此，每个下游任务可以被分配到其定制的模型，该模型用与下游数据最相似的数据预训练。在11个下游任务的集合上进行的实验表明，MoCE平均比香草MAE高2.45%。它还在检测和分割方面获得了最先进的自监督学习结果。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>15</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.05382v1" target="_blank">2402.05382v1</a>
                              </td>
                              <td>Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts</td>
                              <td>Zhili Liu</td>
                              <td>2024-02-08</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_05382v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.05382v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_03406v3_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_03406v3_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_03406v3_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_03406v3_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Spatio-temporal forecasting, pivotal in numerous fields, hinges on the delicate equilibrium between isolating nuanced patterns and sifting out noise. To tackle this, we introduce Sparse Regression-based Vector Quantization (SVQ), a novel technique that leverages sparse regression for succinct representation, an approach theoretically and practically favored over classical clustering-based vector quantization methods. This approach preserves critical details from the original vectors using a regression model while filtering out noise via sparse design. Moreover, we approximate the sparse regression process using a blend of a two-layer MLP and an extensive codebook. This approach not only substantially cuts down on computational costs but also grants SVQ differentiability and training simplicity, resulting in a notable enhancement of performance. Our empirical studies on five spatial-temporal benchmark datasets demonstrate that SVQ achieves state-of-the-art results. Specifically, on the WeatherBench-S temperature dataset, SVQ improves the top baseline by 7.9%. In video prediction benchmarks-Human, KTH, and KittiCaltech-it reduces MAE by an average of 9.4% and improves image quality by 17.3% (LPIPS).</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_03406v3_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>时空预测在许多领域都至关重要，它取决于隔离细微模式和筛选噪音之间的微妙平衡。为了解决这一问题，我们引入了基于稀疏回归的矢量量化（SVQ），这是一种利用稀疏回归进行简洁表示的新技术，在理论和实践上都优于经典的基于聚类的矢量量化方法。这种方法使用回归模型保留了原始向量的关键细节，同时通过稀疏设计滤除噪声。此外，我们使用两层MLP和扩展码本的混合来近似稀疏回归过程。这种方法不仅大大降低了计算成本，而且赋予SVQ可微性和训练的简单性，从而显著提高了性能。我们对五个时空基准数据集的实证研究表明，SVQ取得了最先进的结果。具体而言，在WeatherBench-S温度数据集上，SVQ将顶部基线提高了7.9%。在视频预测基准Human、KTH和KittiCaltech中，它平均将MAE降低9.4%，并将图像质量提高17.3%（LPIPS）。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>16</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.03406v3" target="_blank">2312.03406v3</a>
                              </td>
                              <td>SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting</td>
                              <td>Chao Chen</td>
                              <td>2023-12-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_03406v3_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.03406v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/Pachark/SVQ" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_17152v3_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Mixed Autoencoder for Self-supervised Visual Representation Learning</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_17152v3_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_17152v3_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_17152v3_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on different downstream tasks with significant efficiency. Specifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9 AP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base. Moreover, MixedAE surpasses iBOT, a strong MIM method combined with instance discrimination, while accelerating training by 2x. To our best knowledge, this is the very first work to consider mixing for MIM from the perspective of pretext task design. Code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_17152v3_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩模自动编码器（MAE）通过随机掩模图像块和重建，在各种视觉任务中表现出卓越的性能。然而，与作为最重要部分的对比学习不同，MAE的有效数据扩充策略仍然是悬而未决的问题。本文研究了MAE的主要混合增强。我们首先证明，由于互信息（MI）的增加，天真混合将使模型性能退化。为了解决这个问题，我们提出了同源识别，这是一种辅助借口任务，不仅通过明确要求每个补丁识别同源补丁来缓解MI的增加，而且还可以执行对象感知自监督预训练，以获得更好的下游密集感知性能。通过广泛的实验，我们证明了我们提出的混合自动编码器（MixedAE）在不同下游任务的掩模图像建模（MIM）增强之间以显著的效率实现了最先进的传输结果。具体而言，我们的MixedAE在ImageNet-1K、ADE20K和COCO上的准确率分别比MAE高出+0.3%、+1.7mIoU和+0.9AP。此外，MixedAE超越了iBOT，这是一种结合实例判别的强大MIM方法，同时将训练速度提高了2倍。据我们所知，这是第一个从借口任务设计的角度考虑MIM混合的工作。将提供代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>17</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.17152v3" target="_blank">2303.17152v3</a>
                              </td>
                              <td>Mixed Autoencoder for Self-supervised Visual Representation Learning</td>
                              <td>Kai Chen</td>
                              <td>2023-03-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_17152v3_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.17152v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2309_02908v2_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2309_02908v2_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2309_02908v2_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2309_02908v2_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Energy prediction in buildings plays a crucial role in effective energy management. Precise predictions are essential for achieving optimal energy consumption and distribution within the grid. This paper introduces a Long Short-Term Memory (LSTM) model designed to forecast building energy consumption using historical energy data, occupancy patterns, and weather conditions. The LSTM model provides accurate short, medium, and long-term energy predictions for residential and commercial buildings compared to existing prediction models. We compare our LSTM model with established prediction methods, including linear regression, decision trees, and random forest. Encouragingly, the proposed LSTM model emerges as the superior performer across all metrics. It demonstrates exceptional prediction accuracy, boasting the highest R2 score of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An additional advantage of our developed model is its capacity to achieve efficient energy consumption forecasts even when trained on a limited dataset. We address concerns about overfitting (variance) and underfitting (bias) through rigorous training and evaluation on real-world data. In summary, our research contributes to energy prediction by offering a robust LSTM model that outperforms alternative methods and operates with remarkable efficiency, generalizability, and reliability.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2309_02908v2_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>建筑物的能源预测在有效的能源管理中起着至关重要的作用。精确的预测对于实现电网内的最佳能源消耗和分配至关重要。本文介绍了一种长短期记忆（LSTM）模型，该模型旨在利用历史能源数据、占用模式和天气条件预测建筑能耗。与现有预测模型相比，LSTM模型为住宅和商业建筑提供了准确的短期、中期和长期能源预测。我们将我们的LSTM模型与已建立的预测方法进行了比较，包括线性回归、决策树和随机森林。令人鼓舞的是，所提出的LSTM模型在所有指标中都表现出色。它展示了非凡的预测准确性，R2得分最高为0.97，最有利的平均绝对误差（MAE）为0.007。我们开发的模型的另一个优势是，即使在有限的数据集上进行训练，它也能够实现高效的能源消耗预测。我们通过对真实世界数据的严格训练和评估，解决了过度拟合（方差）和拟合不足（偏差）的问题。总之，我们的研究通过提供一个稳健的LSTM模型来为能量预测做出贡献，该模型优于其他方法，并且具有显著的效率、可推广性和可靠性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>18</td>
                              <td>
                                <a href="https://arxiv.org/abs/2309.02908v2" target="_blank">2309.02908v2</a>
                              </td>
                              <td>DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings</td>
                              <td>Aditya Mishra</td>
                              <td>2023-09-06</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2309_02908v2_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2309.02908v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2312_16046v2_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2312_16046v2_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2312_16046v2_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2312_16046v2_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Previous post-processing studies on rainfall forecasts using numerical weather prediction (NWP) mainly focus on statistics-based aspects, while learning-based aspects are rarely investigated. Although some manually-designed models are proposed to raise accuracy, they are customized networks, which need to be repeatedly tried and verified, at a huge cost in time and labor. Therefore, a self-supervised neural architecture search (NAS) method without significant manual efforts called AdaNAS is proposed in this study to perform rainfall forecast post-processing and predict rainfall with high accuracy. In addition, we design a rainfall-aware search space to significantly improve forecasts for high-rainfall areas. Furthermore, we propose a rainfall-level regularization function to eliminate the effect of noise data during the training. Validation experiments have been performed under the cases of \emph{None}, \emph{Light}, \emph{Moderate}, \emph{Heavy} and \emph{Violent} on a large-scale precipitation benchmark named TIGGE. Finally, the average mean-absolute error (MAE) and average root-mean-square error (RMSE) of the proposed AdaNAS model are 0.98 and 2.04 mm/day, respectively. Additionally, the proposed AdaNAS model is compared with other neural architecture search methods and previous studies. Compared results reveal the satisfactory performance and superiority of the proposed AdaNAS model in terms of precipitation amount prediction and intensity classification. Concretely, the proposed AdaNAS model outperformed previous best-performing manual methods with MAE and RMSE improving by 80.5\% and 80.3\%, respectively.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2312_16046v2_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>以往使用数值天气预报（NWP）进行降雨预报的后处理研究主要集中在基于统计的方面，而很少研究基于学习的方面。尽管提出了一些手动设计的模型来提高精度，但它们是定制的网络，需要反复尝试和验证，付出了巨大的时间和人力成本。因此，本研究提出了一种无需大量人工操作的自监督神经结构搜索（NAS）方法，称为AdaNAS，以进行降雨量预测后处理，并高精度预测降雨量。此外，我们还设计了一个降雨感知搜索空间，以显著提高对高降雨量地区的预测。此外，我们提出了一个降雨水平正则化函数来消除训练过程中噪声数据的影响。在名为TIGGE的大规模降水基准上，在\emph｛None｝、\emph{Light｝、.emph{Medium｝、\emph｛Heavy｝和\emph｝的情况下进行了验证实验。最后，所提出的AdaNAS模型的平均平均绝对误差（MAE）和平均均方根误差（RMSE）分别为0.98和2.04 mm/天。此外，将所提出的AdaNAS模型与其他神经结构搜索方法和先前的研究进行了比较。对比结果表明，所提出的AdaNAS模型在降水量预测和强度分类方面具有令人满意的性能和优越性。具体而言，所提出的AdaNAS模型优于以前性能最好的手动方法，MAE和RMSE分别提高了80.5%和80.3%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>19</td>
                              <td>
                                <a href="https://arxiv.org/abs/2312.16046v2" target="_blank">2312.16046v2</a>
                              </td>
                              <td>AdaNAS: Adaptively Post-processing with Self-supervised Neural Architecture Search for Ensemble Rainfall Forecasts</td>
                              <td>Yingpeng Wen</td>
                              <td>2023-12-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2312_16046v2_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2312.16046v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02224v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MSPM: A Multi-Site Physiological Monitoring Dataset for Remote Pulse, Respiration, and Blood Pressure Estimation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02224v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02224v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02224v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Visible-light cameras can capture subtle physiological biomarkers without physical contact with the subject. We present the Multi-Site Physiological Monitoring (MSPM) dataset, which is the first dataset collected to support the study of simultaneous camera-based vital signs estimation from multiple locations on the body. MSPM enables research on remote photoplethysmography (rPPG), respiration rate, and pulse transit time (PTT); it contains ground-truth measurements of pulse oximetry (at multiple body locations) and blood pressure using contacting sensors. We provide thorough experiments demonstrating the suitability of MSPM to support research on rPPG, respiration rate, and PTT. Cross-dataset rPPG experiments reveal that MSPM is a challenging yet high quality dataset, with intra-dataset pulse rate mean absolute error (MAE) below 4 beats per minute (BPM), and cross-dataset pulse rate MAE below 2 BPM in certain cases. Respiration experiments find a MAE of 1.09 breaths per minute by extracting motion features from the chest. PTT experiments find that across the pairs of different body sites, there is high correlation between remote PTT and contact-measured PTT, which facilitates the possibility for future camera-based PTT research.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02224v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>可见光相机可以在不与受试者发生物理接触的情况下捕捉细微的生理生物标志物。我们提出了多点生理监测（MSPM）数据集，这是第一个收集的数据集，用于支持从身体多个位置同时进行基于摄像头的生命体征估计的研究。MSPM能够研究远程光电体积描记术（rPG）、呼吸速率和脉冲传输时间（PTT）；它包含使用接触传感器对脉搏血氧计（在多个身体部位）和血压进行的真实测量。我们提供了全面的实验，证明了MSPM适用于支持rPG、呼吸速率和PTT的研究。跨数据集rPG实验表明，MSPM是一个具有挑战性但高质量的数据集，数据集内脉率平均绝对误差（MAE）低于每分钟4次，在某些情况下，跨数据集脉率MAE低于每分钟2次。通过从胸部提取运动特征，呼吸实验发现MAE为每分钟1.09次呼吸。PTT实验发现，在不同身体部位的配对中，远程PTT和接触测量的PTT之间存在高度相关性，这为未来基于相机的PTT研究提供了可能性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>20</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02224v1" target="_blank">2402.02224v1</a>
                              </td>
                              <td>MSPM: A Multi-Site Physiological Monitoring Dataset for Remote Pulse, Respiration, and Blood Pressure Estimation</td>
                              <td>Jeremy Speth</td>
                              <td>2024-02-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02224v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02224v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_02088v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">DCS-Net: Pioneering Leakage-Free Point Cloud Pretraining Framework with Global Insights</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_02088v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_02088v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_02088v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked autoencoding and generative pretraining have achieved remarkable success in computer vision and natural language processing, and more recently, they have been extended to the point cloud domain. Nevertheless, existing point cloud models suffer from the issue of information leakage due to the pre-sampling of center points, which leads to trivial proxy tasks for the models. These approaches primarily focus on local feature reconstruction, limiting their ability to capture global patterns within point clouds. In this paper, we argue that the reduced difficulty of pretext tasks hampers the model's capacity to learn expressive representations. To address these limitations, we introduce a novel solution called the Differentiable Center Sampling Network (DCS-Net). It tackles the information leakage problem by incorporating both global feature reconstruction and local feature reconstruction as non-trivial proxy tasks, enabling simultaneous learning of both the global and local patterns within point cloud. Experimental results demonstrate that our method enhances the expressive capacity of existing point cloud models and effectively addresses the issue of information leakage.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_02088v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩蔽自动编码和生成预训练在计算机视觉和自然语言处理领域取得了显著的成功，最近还扩展到了点云领域。然而，由于中心点的预采样，现有的点云模型存在信息泄漏问题，这导致模型的代理任务微不足道。这些方法主要侧重于局部特征重建，限制了它们在点云中捕获全局模式的能力。在本文中，我们认为借口任务的难度降低阻碍了模型学习表达表征的能力。为了解决这些限制，我们引入了一种新的解决方案，称为可微分中心采样网络（DCS-Net）。它通过将全局特征重构和局部特征重构作为非平凡的代理任务来解决信息泄漏问题，从而能够同时学习点云中的全局和局部模式。实验结果表明，该方法提高了现有点云模型的表达能力，有效地解决了信息泄漏问题。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>21</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.02088v1" target="_blank">2402.02088v1</a>
                              </td>
                              <td>DCS-Net: Pioneering Leakage-Free Point Cloud Pretraining Framework with Global Insights</td>
                              <td>Zhe Li</td>
                              <td>2024-02-03</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_02088v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.02088v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_05943v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">A hybrid IndRNNLSTM approach for real-time anomaly detection in software-defined networks</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_05943v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_05943v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_05943v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Anomaly detection in SDN using data flow prediction is a difficult task. This problem is included in the category of time series and regression problems. Machine learning approaches are challenging in this field due to the manual selection of features. On the other hand, deep learning approaches have important features due to the automatic selection of features. Meanwhile, RNN-based approaches have been used the most. The LSTM and GRU approaches learn dependent entities well; on the other hand, the IndRNN approach learns non-dependent entities in time series. The proposed approach tried to use a combination of IndRNN and LSTM approaches to learn dependent and non-dependent features. Feature selection approaches also provide a suitable view of features for the models; for this purpose, four feature selection models, Filter, Wrapper, Embedded, and Autoencoder were used. The proposed IndRNNLSTM algorithm, in combination with Embedded, was able to achieve MAE=1.22 and RMSE=9.92 on NSL-KDD data.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_05943v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>利用数据流预测进行SDN中的异常检测是一项艰巨的任务。这个问题属于时间序列和回归问题的范畴。由于手动选择特征，机器学习方法在该领域具有挑战性。另一方面，由于特征的自动选择，深度学习方法具有重要特征。同时，基于RNN的方法被使用得最多。LSTM和GRU方法可以很好地学习依赖实体；另一方面，IndRNN方法学习时间序列中的非依赖实体。所提出的方法试图使用IndRNN和LSTM方法的组合来学习依赖和非依赖特征。特征选择方法还为模型提供了合适的特征视图；为此，使用了四个特征选择模型，Filter、Wrapper、Embedded和Autoencoder。所提出的IndRNNLSTM算法与嵌入式算法相结合，能够在NSL-KDD数据上实现MAE=1.22和RMSE=9.92。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>22</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.05943v1" target="_blank">2402.05943v1</a>
                              </td>
                              <td>A hybrid IndRNNLSTM approach for real-time anomaly detection in software-defined networks</td>
                              <td>Sajjad Salem</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_05943v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.05943v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_01462v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">3D Vertebrae Measurements: Assessing Vertebral Dimensions in Human Spine Mesh Models Using Local Anatomical Vertebral Axes</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_01462v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_01462v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_01462v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Vertebral morphological measurements are important across various disciplines, including spinal biomechanics and clinical applications, pre- and post-operatively. These measurements also play a crucial role in anthropological longitudinal studies, where spinal metrics are repeatedly documented over extended periods. Traditionally, such measurements have been manually conducted, a process that is time-consuming. In this study, we introduce a novel, fully automated method for measuring vertebral morphology using 3D meshes of lumbar and thoracic spine models.Our experimental results demonstrate the method's capability to accurately measure low-resolution patient-specific vertebral meshes with mean absolute error (MAE) of 1.09 mm and those derived from artificially created lumbar spines, where the average MAE value was 0.7 mm. Our qualitative analysis indicates that measurements obtained using our method on 3D spine models can be accurately reprojected back onto the original medical images if these images are available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_01462v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>脊椎形态测量在手术前和手术后的各个学科中都很重要，包括脊柱生物力学和临床应用。这些测量在人类学纵向研究中也发挥着至关重要的作用，在这些研究中，脊柱指标会在很长一段时间内被反复记录下来。传统上，这种测量是手动进行的，这一过程非常耗时。在这项研究中，我们介绍了一种新的、全自动的方法，使用腰椎和胸椎模型的3D网格来测量椎骨形态。我们的实验结果证明了该方法能够准确测量低分辨率患者特定的脊椎网格，平均绝对误差（MAE）为1.09 mm，以及人工制作的腰椎网格，其中平均MAE值为0.7 mm。我们的定性分析表明，如果这些图像可用，使用我们的方法在3D脊柱模型上获得的测量结果可以准确地重新投影到原始医学图像上。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>23</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.01462v1" target="_blank">2402.01462v1</a>
                              </td>
                              <td>3D Vertebrae Measurements: Assessing Vertebral Dimensions in Human Spine Mesh Models Using Local Anatomical Vertebral Axes</td>
                              <td>Ivanna Kramer</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_01462v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.01462v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2402_07917v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Smart Water Irrigation for Rice Farming through the Internet of Things</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2402_07917v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2402_07917v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2402_07917v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This study intends to build smart water irrigation for rice farming using IoT and microcontroller devices with solar panel support. The system demonstrates the capabilities of automated irrigation by reducing physical labor through smart monitoring of the temperature, soil moisture, and humidity using multiple sensors. This study uses an agile methodology as it is suitable for reiterative operation for the development of the prototype. The mean result for the interpretation of data gathered for the systems' adaptability and flexibility is 4.32. The researchers were able to develop smart water irrigation for rice farming using IoT and microcontroller devices with solar panel support and the respondents also agreed that Smart water irrigation for rice farming using IoT and microcontroller devices with solar panel support is practical and valuable. A decision support system is recommended that can analyze data collected from IoT sensors and provide further recommendations. Based on the results, it is also suggested that future researchers use drip irrigation, instead of flood irrigation. Smart water irrigation has the potential to revolutionize agriculture, enhance environmental sustainability, and address pressing global challenges related to water resources and food security. These implications highlight the importance of continued research and innovation in this field.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2402_07917v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>这项研究旨在利用物联网和微控制器设备，在太阳能电池板的支持下，为水稻种植构建智能灌溉。该系统通过使用多个传感器对温度、土壤湿度和湿度进行智能监测，减少体力劳动，展示了自动灌溉的能力。本研究采用了敏捷方法，因为它适合于原型开发的重复操作。对为系统的适应性和灵活性而收集的数据进行解释的平均结果为4.32。研究人员能够使用物联网和具有太阳能电池板支持的微控制器设备开发用于水稻种植的智能灌溉，受访者还一致认为，使用具有太阳能电池面板支持的物联网和微控制器设备的水稻种植智能灌溉是实用和有价值的。建议使用决策支持系统，该系统可以分析从物联网传感器收集的数据，并提供进一步的建议。基于这些结果，还建议未来的研究人员使用滴灌，而不是洪水灌溉。智能灌溉有可能彻底改变农业，提高环境可持续性，并应对与水资源和粮食安全相关的紧迫全球挑战。这些影响凸显了该领域持续研究和创新的重要性。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>24</td>
                              <td>
                                <a href="https://arxiv.org/abs/2402.07917v1" target="_blank">2402.07917v1</a>
                              </td>
                              <td>Smart Water Irrigation for Rice Farming through the Internet of Things</td>
                              <td>Regie Porras Binayao</td>
                              <td>2024-02-02</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2402_07917v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2402.07917v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_18240v2_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_18240v2_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_18240v2_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_18240v2_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 4.3M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. Finally, we present real-world hardware experiments, in which VC-1 and VC-1 (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_18240v2_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>我们对嵌入式人工智能的预训练视觉表示（PVR）或视觉“基础模型”进行了规模最大、最全面的实证研究。首先，我们策划了CortexBench，由17个不同的任务组成，涵盖移动、导航、灵巧和移动操作。接下来，我们系统地评估了现有的PVR，发现没有一个是普遍主导的。为了研究预训练数据大小和多样性的影响，我们将来自7个不同来源（超过430万张图像）的4000多个小时的以自我为中心的视频与ImageNet相结合，在这些数据的切片上使用掩蔽自动编码（MAE）来训练不同大小的视觉转换器。与先前工作的推断相反，我们发现，扩展数据集的大小和多样性并不能普遍提高性能（但平均而言是这样）。我们最大的模型名为VC-1，平均而言优于所有先前的PVR，但也不是普遍主导。接下来，我们展示了VC-1的任务或领域特定适应带来了巨大的收益，VC-1（适应）在CortexBench中的所有基准测试中实现了比最知名的结果更具竞争力或更优越的性能。最后，我们展示了真实世界的硬件实验，其中VC-1和VC-1（自适应）优于最强的预先存在的PVR。总的来说，本文没有提出新的技术，而是提出了严格的系统评估、关于PVR的一系列广泛发现（在某些情况下，反驳了先前工作中在狭窄领域中提出的发现），以及开源代码和模型（需要超过10000个GPU小时的训练），以造福于研究界。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>25</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.18240v2" target="_blank">2303.18240v2</a>
                              </td>
                              <td>Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?</td>
                              <td>Arjun Majumdar</td>
                              <td>2023-03-31</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_18240v2_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.18240v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15893v2_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Arbitrary-Scale Downscaling of Tidal Current Data Using Implicit Continuous Representation</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15893v2_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15893v2_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15893v2_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Numerical models have long been used to understand geoscientific phenomena, including tidal currents, crucial for renewable energy production and coastal engineering. However, their computational cost hinders generating data of varying resolutions. As an alternative, deep learning-based downscaling methods have gained traction due to their faster inference speeds. But most of them are limited to only inference fixed scale and overlook important characteristics of target geoscientific data. In this paper, we propose a novel downscaling framework for tidal current data, addressing its unique characteristics, which are dissimilar to images: heterogeneity and local dependency. Moreover, our framework can generate any arbitrary-scale output utilizing a continuous representation model. Our proposed framework demonstrates significantly improved flow velocity predictions by 93.21% (MSE) and 63.85% (MAE) compared to the Baseline model while achieving a remarkable 33.2% reduction in FLOPs.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15893v2_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>长期以来，数值模型一直被用于理解地球科学现象，包括对可再生能源生产和海岸工程至关重要的潮流。然而，它们的计算成本阻碍了生成不同分辨率的数据。作为一种替代方案，基于深度学习的降尺度方法由于其更快的推理速度而受到关注。但大多局限于固定规模的推断，忽略了目标地学数据的重要特征。在本文中，我们提出了一种新的潮流数据降尺度框架，解决了其与图像不同的独特特征：异质性和局部依赖性。此外，我们的框架可以利用连续表示模型生成任意规模的输出。我们提出的框架表明，与基线模型相比，流速预测显著提高了93.21%（MSE）和63.85%（MAE），同时FLOP显著降低了33.2%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>26</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15893v2" target="_blank">2401.15893v2</a>
                              </td>
                              <td>Arbitrary-Scale Downscaling of Tidal Current Data Using Implicit Continuous Representation</td>
                              <td>Dongheon Lee</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15893v2_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15893v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_17497v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Towards Visual Syntactical Understanding</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_17497v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_17497v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_17497v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Syntax is usually studied in the realm of linguistics and refers to the arrangement of words in a sentence. Similarly, an image can be considered as a visual 'sentence', with the semantic parts of the image acting as 'words'. While visual syntactic understanding occurs naturally to humans, it is interesting to explore whether deep neural networks (DNNs) are equipped with such reasoning. To that end, we alter the syntax of natural images (e.g. swapping the eye and nose of a face), referred to as 'incorrect' images, to investigate the sensitivity of DNNs to such syntactic anomaly. Through our experiments, we discover an intriguing property of DNNs where we observe that state-of-the-art convolutional neural networks, as well as vision transformers, fail to discriminate between syntactically correct and incorrect images when trained on only correct ones. To counter this issue and enable visual syntactic understanding with DNNs, we propose a three-stage framework- (i) the 'words' (or the sub-features) in the image are detected, (ii) the detected words are sequentially masked and reconstructed using an autoencoder, (iii) the original and reconstructed parts are compared at each location to determine syntactic correctness. The reconstruction module is trained with BERT-like masked autoencoding for images, with the motivation to leverage language model inspired training to better capture the syntax. Note, our proposed approach is unsupervised in the sense that the incorrect images are only used during testing and the correct versus incorrect labels are never used for training. We perform experiments on CelebA, and AFHQ datasets and obtain classification accuracy of 92.10%, and 90.89%, respectively. Notably, the approach generalizes well to ImageNet samples which share common classes with CelebA and AFHQ without explicitly training on them.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_17497v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>语法通常在语言学领域研究，指的是句子中单词的排列。同样，图像可以被视为视觉“句子”，图像的语义部分充当“单词”。虽然视觉句法理解是人类自然产生的，但探索深度神经网络（DNN）是否具备这种推理是很有趣的。为此，我们改变了被称为“不正确”图像的自然图像的语法（例如，交换人脸的眼睛和鼻子），以研究DNN对这种语法异常的敏感性。通过我们的实验，我们发现了DNN的一个有趣的特性，即我们观察到最先进的卷积神经网络以及视觉转换器，当只在正确的图像上训练时，无法区分语法正确和不正确的图像。为了解决这个问题并实现DNN的视觉句法理解，我们提出了一个三阶段框架——（i）检测图像中的“单词”（或子特征），（ii）使用自动编码器顺序屏蔽和重建检测到的单词，（iii）在每个位置比较原始部分和重建部分，以确定句法正确性。重建模块使用类似BERT的图像掩蔽自动编码进行训练，目的是利用语言模型启发的训练来更好地捕捉语法。注意，我们提出的方法是无监督的，因为不正确的图像只在测试期间使用，而正确与不正确的标签从不用于训练。我们在CelebA和AFHQ数据集上进行了实验，分别获得了92.10%和90.89%的分类准确率。值得注意的是，该方法很好地推广到与CelebA和AFHQ共享公共类的ImageNet样本，而无需对它们进行明确的训练。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>27</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.17497v1" target="_blank">2401.17497v1</a>
                              </td>
                              <td>Towards Visual Syntactical Understanding</td>
                              <td>Sayeed Shafayet Chowdhury</td>
                              <td>2024-01-30</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_17497v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.17497v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15900v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">MV2MAE: Multi-View Video Masked Autoencoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15900v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15900v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15900v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Videos captured from multiple viewpoints can help in perceiving the 3D structure of the world and benefit computer vision tasks such as action recognition, tracking, etc. In this paper, we present a method for self-supervised learning from synchronized multi-view videos. We use a cross-view reconstruction task to inject geometry information in the model. Our approach is based on the masked autoencoder (MAE) framework. In addition to the same-view decoder, we introduce a separate cross-view decoder which leverages cross-attention mechanism to reconstruct a target viewpoint video using a video from source viewpoint, to help representations robust to viewpoint changes. For videos, static regions can be reconstructed trivially which hinders learning meaningful representations. To tackle this, we introduce a motion-weighted reconstruction loss which improves temporal modeling. We report state-of-the-art results on the NTU-60, NTU-120 and ETRI datasets, as well as in the transfer learning setting on NUCLA, PKU-MMD-II and ROCOG-v2 datasets, demonstrating the robustness of our approach. Code will be made available.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15900v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>从多个视点捕获的视频可以帮助感知世界的3D结构，并有利于计算机视觉任务，如动作识别、跟踪等。在本文中，我们提出了一种从同步多视点视频中进行自我监督学习的方法。我们使用跨视图重建任务在模型中注入几何信息。我们的方法基于掩蔽自动编码器（MAE）框架。除了相同的视图解码器外，我们还引入了一个单独的交叉视图解码器，该解码器利用交叉注意力机制，使用来自源视点的视频重建目标视点视频，以帮助表示对视点变化具有鲁棒性。对于视频，静态区域可以被琐碎地重建，这阻碍了学习有意义的表示。为了解决这个问题，我们引入了一种运动加权重建损失，它改进了时间建模。我们在NTU-60、NTU-120和ETRI数据集上，以及在NUCLA、PKU-MMD-II和ROCOG-v2数据集上的迁移学习环境中报告了最先进的结果，证明了我们方法的稳健性。将提供代码。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>28</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15900v1" target="_blank">2401.15900v1</a>
                              </td>
                              <td>MV2MAE: Multi-View Video Masked Autoencoders</td>
                              <td>Ketul Shah</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15900v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15900v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15855v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Cross-Scale MAE: A Tale of Multi-Scale Exploitation in Remote Sensing</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15855v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15855v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15855v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Remote sensing images present unique challenges to image analysis due to the extensive geographic coverage, hardware limitations, and misaligned multi-scale images. This paper revisits the classical multi-scale representation learning problem but under the general framework of self-supervised learning for remote sensing image understanding. We present Cross-Scale MAE, a self-supervised model built upon the Masked Auto-Encoder (MAE).During pre-training, Cross-Scale MAE employs scale augmentation techniques and enforces cross-scale consistency constraints through both contrastive and generative losses to ensure consistent and meaningful representations well-suited for a wide range of downstream tasks. Further, our implementation leverages the xFormers library to accelerate network pre-training on a single GPU while maintaining the quality of learned representations. Experimental evaluations demonstrate that Cross-Scale MAE exhibits superior performance compared to standard MAE and other state-of-the-art remote sensing MAE methods.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15855v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于广泛的地理覆盖范围、硬件限制和多尺度图像错位，遥感图像给图像分析带来了独特的挑战。本文在遥感图像理解的自监督学习的一般框架下，重新审视了经典的多尺度表示学习问题。我们提出了Cross-Scale MAE，这是一个基于掩蔽自动编码器（MAE）的自监督模型。在预训练期间，跨尺度MAE采用尺度增强技术，并通过对比和生成损失来实施跨尺度一致性约束，以确保一致和有意义的表示非常适合广泛的下游任务。此外，我们的实现利用xFormers库在单个GPU上加速网络预训练，同时保持学习表示的质量。实验评估表明，与标准MAE和其他最先进的遥感MAE方法相比，跨尺度MAE表现出优越的性能。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>29</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15855v1" target="_blank">2401.15855v1</a>
                              </td>
                              <td>Cross-Scale MAE: A Tale of Multi-Scale Exploitation in Remote Sensing</td>
                              <td>Maofeng Tang</td>
                              <td>2024-01-29</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15855v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15855v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/aicip/Cross-Scale-MAE" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2207_13532v3_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Contrastive Masked Autoencoders are Stronger Vision Learners</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2207_13532v3_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2207_13532v3_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2207_13532v3_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Masked image modeling (MIM) has achieved promising results on various vision tasks. However, the limited discriminability of learned representation manifests there is still plenty to go for making a stronger vision learner. Towards this goal, we propose Contrastive Masked Autoencoders (CMAE), a new self-supervised pre-training method for learning more comprehensive and capable vision representations. By elaboratively unifying contrastive learning (CL) and masked image model (MIM) through novel designs, CMAE leverages their respective advantages and learns representations with both strong instance discriminability and local perceptibility. Specifically, CMAE consists of two branches where the online branch is an asymmetric encoder-decoder and the momentum branch is a momentum updated encoder. During training, the online encoder reconstructs original images from latent representations of masked images to learn holistic features. The momentum encoder, fed with the full images, enhances the feature discriminability via contrastive learning with its online counterpart. To make CL compatible with MIM, CMAE introduces two new components, i.e. pixel shifting for generating plausible positive views and feature decoder for complementing features of contrastive pairs. Thanks to these novel designs, CMAE effectively improves the representation quality and transfer performance over its MIM counterpart. CMAE achieves the state-of-the-art performance on highly competitive benchmarks of image classification, semantic segmentation and object detection. Notably, CMAE-Base achieves $85.3\%$ top-1 accuracy on ImageNet and $52.5\%$ mIoU on ADE20k, surpassing previous best results by $0.7\%$ and $1.8\%$ respectively. The source code is publicly accessible at \url{https://github.com/ZhichengHuang/CMAE}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2207_13532v3_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>掩模图像建模（MIM）在各种视觉任务中取得了良好的效果。然而，学习表征的有限可分辨性表明，要培养一个更强的视觉学习者，还有很多路要走。为了实现这一目标，我们提出了对比掩模自动编码器（CMAE），这是一种新的自监督预训练方法，用于学习更全面、更有能力的视觉表示。通过新颖的设计，将对比学习（CL）和掩蔽图像模型（MIM）精细地统一起来，CMAE利用了它们各自的优势，学习了具有强实例可分辨性和局部可感知性的表示。具体而言，CMAE由两个分支组成，其中在线分支是非对称编码器-解码器，动量分支是动量更新编码器。在训练过程中，在线编码器从掩蔽图像的潜在表示重建原始图像，以学习整体特征。动量编码器提供了完整的图像，通过与在线编码器的对比学习增强了特征的可分辨性。为了使CL与MIM兼容，CMAE引入了两个新的组件，即用于生成可信正视图的像素移位和用于补充对比对的特征的特征解码器。得益于这些新颖的设计，CMAE比MIM有效地提高了表示质量和传输性能。CMAE在极具竞争力的图像分类、语义分割和对象检测基准上实现了最先进的性能。值得注意的是，CMAE Base在ImageNet上实现了85.3\%%$的前1精度，在ADE20k上实现了52.5\%%$mIoU，分别比以前的最佳结果高出0.7\%%$和1.8\%%$。源代码可在\url上公开访问{https://github.com/ZhichengHuang/CMAE}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>30</td>
                              <td>
                                <a href="https://arxiv.org/abs/2207.13532v3" target="_blank">2207.13532v3</a>
                              </td>
                              <td>Contrastive Masked Autoencoders are Stronger Vision Learners</td>
                              <td>Zhicheng Huang</td>
                              <td>2022-07-27</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2207_13532v3_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2207.13532v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/zhichenghuang/cmae" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_15236v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_15236v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_15236v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_15236v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Sub-10cm diameter nano-drones are gaining momentum thanks to their applicability in scenarios prevented to bigger flying drones, such as in narrow environments and close to humans. However, their tiny form factor also brings their major drawback: ultra-constrained memory and processors for the onboard execution of their perception pipelines. Therefore, lightweight deep learning-based approaches are becoming increasingly popular, stressing how computational efficiency and energy-saving are paramount as they can make the difference between a fully working closed-loop system and a failing one. In this work, to maximize the exploitation of the ultra-limited resources aboard nano-drones, we present a novel adaptive deep learning-based mechanism for the efficient execution of a vision-based human pose estimation task. We leverage two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different regression performance vs. computational costs trade-offs. By combining these CNNs with three novel adaptation strategies based on the output's temporal consistency and on auxiliary tasks to swap the CNN being executed proactively, we present six different systems. On a real-world dataset and the actual nano-drone hardware, our best-performing system, compared to executing only the bigger and most accurate SoA model, shows 28% latency reduction while keeping the same mean absolute error (MAE), 3% MAE reduction while being iso-latency, and the absolute peak performance, i.e., 6% better than SoA model.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_15236v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>由于直径小于10厘米的纳米无人机适用于大型飞行无人机无法使用的场景，如狭窄环境和靠近人类的场景，因此其发展势头越来越大。然而，它们微小的外形因素也带来了它们的主要缺点：用于板载执行感知管道的内存和处理器受到了极大的限制。因此，基于轻量级深度学习的方法越来越受欢迎，强调计算效率和节能是至关重要的，因为它们可以区分完全工作的闭环系统和失败的闭环系统。在这项工作中，为了最大限度地利用纳米无人机上的超有限资源，我们提出了一种新的基于自适应深度学习的机制，用于高效执行基于视觉的人体姿态估计任务。我们利用了两种最先进的卷积神经网络，它们具有不同的回归性能和计算成本权衡。通过将这些CNN与三种基于输出的时间一致性和辅助任务的新适应策略相结合，以交换主动执行的CNN，我们提出了六种不同的系统。在真实世界的数据集和实际的纳米无人机硬件上，与仅执行更大、最准确的SoA模型相比，我们性能最好的系统在保持相同的平均绝对误差（MAE）的同时，延迟减少了28%，在等延迟的情况下，MAE减少了3%，并且绝对峰值性能比SoA模型好6%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>31</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.15236v1" target="_blank">2401.15236v1</a>
                              </td>
                              <td>Adaptive Deep Learning for Efficient Visual Pose Estimation aboard Ultra-low-power Nano-drones</td>
                              <td>Beatrice Alessandra Motetti</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_15236v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.15236v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14695v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Continuously Evolving Graph Neural Controlled Differential Equations for Traffic Forecasting</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14695v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14695v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14695v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>As a crucial technique for developing a smart city, traffic forecasting has become a popular research focus in academic and industrial communities for decades. This task is highly challenging due to complex and dynamic spatial-temporal dependencies in traffic networks. Existing works ignore continuous temporal dependencies and spatial dependencies evolving over time. In this paper, we propose Continuously Evolving Graph Neural Controlled Differential Equations (CEGNCDE) to capture continuous temporal dependencies and spatial dependencies over time simultaneously. Specifically, a continuously evolving graph generator (CEGG) based on NCDE is introduced to generate the spatial dependencies graph that continuously evolves over time from discrete historical observations. Then, a graph neural controlled differential equations (GNCDE) framework is introduced to capture continuous temporal dependencies and spatial dependencies over time simultaneously. Extensive experiments demonstrate that CEGNCDE outperforms the SOTA methods by average 2.34% relative MAE reduction, 0.97% relative RMSE reduction, and 3.17% relative MAPE reduction.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14695v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>交通预测作为发展智慧城市的关键技术，几十年来一直是学术界和工业界的热门研究热点。由于交通网络中复杂且动态的时空依赖性，这项任务极具挑战性。现有的工作忽略了连续的时间依赖性和随时间演变的空间依赖性。在本文中，我们提出了连续进化图神经控制微分方程（CEGNCDE），以同时捕获随时间的连续时间依赖性和空间依赖性。具体来说，引入了一种基于NCDE的连续演化图生成器（CEGG），从离散的历史观测中生成随时间连续演化的空间相关性图。然后，引入图神经控制微分方程（GNCDE）框架来同时捕获随时间的连续时间依赖性和空间依赖性。大量实验表明，CEGNCDE优于SOTA方法，平均相对MAE减少2.34%，相对RMSE减少0.97%，相对MAPE减少3.17%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>32</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14695v1" target="_blank">2401.14695v1</a>
                              </td>
                              <td>Continuously Evolving Graph Neural Controlled Differential Equations for Traffic Forecasting</td>
                              <td>Jiajia Wu</td>
                              <td>2024-01-26</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14695v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14695v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_04345v2_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">RomniStereo: Recurrent Omnidirectional Stereo Matching</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_04345v2_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_04345v2_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_04345v2_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Omnidirectional stereo matching (OSM) is an essential and reliable means for $360^{\circ}$ depth sensing. However, following earlier works on conventional stereo matching, prior state-of-the-art (SOTA) methods rely on a 3D encoder-decoder block to regularize the cost volume, causing the whole system complicated and sub-optimal results. Recently, the Recurrent All-pairs Field Transforms (RAFT) based approach employs the recurrent update in 2D and has efficiently improved image-matching tasks, ie, optical flow, and stereo matching. To bridge the gap between OSM and RAFT, we mainly propose an opposite adaptive weighting scheme to seamlessly transform the outputs of spherical sweeping of OSM into the required inputs for the recurrent update, thus creating a recurrent omnidirectional stereo matching (RomniStereo) algorithm. Furthermore, we introduce two techniques, ie, grid embedding and adaptive context feature generation, which also contribute to RomniStereo's performance. Our best model improves the average MAE metric by 40.7\% over the previous SOTA baseline across five datasets. When visualizing the results, our models demonstrate clear advantages on both synthetic and realistic examples. The code is available at \url{https://github.com/HalleyJiang/RomniStereo}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_04345v2_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>全方位立体匹配（OSM）是360美元深度传感的一种重要而可靠的手段。然而，在传统立体声匹配的早期工作之后，现有技术（SOTA）方法依赖于3D编码器-解码器块来正则化成本体积，导致整个系统复杂且次优的结果。最近，基于递归全对场变换（RAFT）的方法在2D中采用了递归更新，并有效地改进了图像匹配任务，即光流和立体匹配。为了弥补OSM和RAFT之间的差距，我们主要提出了一种相反的自适应加权方案，将OSM的球面扫频输出无缝转换为递归更新所需的输入，从而创建了一种递归全向立体声匹配（RomniStereo）算法。此外，我们还介绍了两种技术，即网格嵌入和自适应上下文特征生成，这两种技术也有助于RomniStereo的性能。我们的最佳模型将五个数据集的平均MAE指标比之前的SOTA基线提高了40.7%。当可视化结果时，我们的模型在合成和现实的例子中都显示出明显的优势。代码位于\url{https://github.com/HalleyJiang/RomniStereo}.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>33</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.04345v2" target="_blank">2401.04345v2</a>
                              </td>
                              <td>RomniStereo: Recurrent Omnidirectional Stereo Matching</td>
                              <td>Hualie Jiang</td>
                              <td>2024-01-09</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_04345v2_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.04345v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/halleyjiang/romnistereo" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14486v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">CloudTracks: A Dataset for Localizing Ship Tracks in Satellite Images of Clouds</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14486v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14486v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14486v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Clouds play a significant role in global temperature regulation through their effect on planetary albedo. Anthropogenic emissions of aerosols can alter the albedo of clouds, but the extent of this effect, and its consequent impact on temperature change, remains uncertain. Human-induced clouds caused by ship aerosol emissions, commonly referred to as ship tracks, provide visible manifestations of this effect distinct from adjacent cloud regions and therefore serve as a useful sandbox to study human-induced clouds. However, the lack of large-scale ship track data makes it difficult to deduce their general effects on cloud formation. Towards developing automated approaches to localize ship tracks at scale, we present CloudTracks, a dataset containing 3,560 satellite images labeled with more than 12,000 ship track instance annotations. We train semantic segmentation and instance segmentation model baselines on our dataset and find that our best model substantially outperforms previous state-of-the-art for ship track localization (61.29 vs. 48.65 IoU). We also find that the best instance segmentation model is able to identify the number of ship tracks in each image more accurately than the previous state-of-the-art (1.64 vs. 4.99 MAE). However, we identify cases where the best model struggles to accurately localize and count ship tracks, so we believe CloudTracks will stimulate novel machine learning approaches to better detect elongated and overlapping features in satellite images. We release our dataset openly at {zenodo.org/records/10042922}.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14486v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>云层通过其对行星反照率的影响，在全球温度调节中发挥着重要作用。人为排放的气溶胶会改变云层的反照率，但这种影响的程度及其对温度变化的影响仍不确定。由船舶气溶胶排放引起的人类诱导云，通常被称为船迹，提供了这种影响的可见表现，与邻近的云区域不同，因此可以作为研究人类诱导云的有用沙盒。然而，由于缺乏大规模的船只轨迹数据，很难推断出它们对云层形成的总体影响。为了开发大规模定位船舶轨迹的自动化方法，我们提出了CloudTracks，这是一个包含3560张卫星图像的数据集，标记了12000多个船舶轨迹实例注释。我们在数据集上训练语义分割和实例分割模型基线，发现我们的最佳模型在船舶轨迹定位方面大大优于以前的先进技术（61.29对48.65 IoU）。我们还发现，最佳实例分割模型能够比以前的现有技术（1.64对4.99 MAE）更准确地识别每个图像中的船舶轨迹数量。然而，我们发现了最佳模型难以准确定位和计数船只轨迹的情况，因此我们相信CloudTracks将激发新的机器学习方法，更好地检测卫星图像中的细长和重叠特征。我们在{zenodo.org/records/10042922}上公开发布了我们的数据集。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>34</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14486v1" target="_blank">2401.14486v1</a>
                              </td>
                              <td>CloudTracks: A Dataset for Localizing Ship Tracks in Satellite Images of Clouds</td>
                              <td>Muhammad Ahmed Chaudhry</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14486v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14486v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14391v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Rethinking Patch Dependence for Masked Autoencoders</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14391v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14391v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14391v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>In this work, we re-examine inter-patch dependencies in the decoding mechanism of masked autoencoders (MAE). We decompose this decoding mechanism for masked patch reconstruction in MAE into self-attention and cross-attention. Our investigations suggest that self-attention between mask patches is not essential for learning good representations. To this end, we propose a novel pretraining framework: Cross-Attention Masked Autoencoders (CrossMAE). CrossMAE's decoder leverages only cross-attention between masked and visible tokens, with no degradation in downstream performance. This design also enables decoding only a small subset of mask tokens, boosting efficiency. Furthermore, each decoder block can now leverage different encoder features, resulting in improved representation learning. CrossMAE matches MAE in performance with 2.5 to 3.7$\times$ less decoding compute. It also surpasses MAE on ImageNet classification and COCO instance segmentation under the same compute. Code and models: https://crossmae.github.io</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14391v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>在这项工作中，我们重新审视了屏蔽自动编码器（MAE）解码机制中的补丁间依赖关系。我们将这种用于MAE中掩蔽补丁重建的解码机制分解为自注意和交叉注意。我们的研究表明，掩模贴片之间的自我关注对于学习良好的表征并不重要。为此，我们提出了一种新的预训练框架：交叉注意力掩蔽自动编码器（CrossMAE）。CrossMAE的解码器仅利用掩码和可见令牌之间的交叉关注，不会降低下游性能。这种设计还能够仅解码掩码令牌的一小部分，从而提高效率。此外，每个解码器块现在可以利用不同的编码器特征，从而改进表示学习。CrossMAE在性能上与MAE相匹配，解码计算量减少了2.5到3.7$\倍。在同一计算下，它在ImageNet分类和COCO实例分割上也超过了MAE。代码和型号：https://crossmae.github.io</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>35</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14391v1" target="_blank">2401.14391v1</a>
                              </td>
                              <td>Rethinking Patch Dependence for Masked Autoencoders</td>
                              <td>Letian Fu</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14391v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14391v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_14292v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">AST-2: Single and bi-layered 2-D acoustic soft tactile skin</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_14292v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_14292v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_14292v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper aims to present an innovative and cost-effective design for Acoustic Soft Tactile (AST) Skin, with the primary goal of significantly enhancing the accuracy of 2-D tactile feature estimation. The existing challenge lies in achieving precise tactile feature estimation, especially concerning contact geometry characteristics, using cost-effective solutions. We hypothesise that by harnessing acoustic energy through dedicated acoustic channels in 2 layers beneath the sensing surface and analysing amplitude modulation, we can effectively decode interactions on the sensory surface, thereby improving tactile feature estimation. Our approach involves the distinct separation of hardware components responsible for emitting and receiving acoustic signals, resulting in a modular and highly customizable skin design. Practical tests demonstrate the effectiveness of this novel design, achieving remarkable precision in estimating contact normal forces (MAE < 0.8 N), 2D contact localisation (MAE < 0.7 mm), and contact surface diameter (MAE < 0.3 mm). In conclusion, the AST skin, with its innovative design and modular architecture, successfully addresses the challenge of tactile feature estimation. The presented results showcase its ability to precisely estimate various tactile features, making it a practical and cost-effective solution for robotic applications.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_14292v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文旨在提出一种创新且具有成本效益的声学软触觉（AST）皮肤设计，其主要目标是显著提高二维触觉特征估计的准确性。现有的挑战在于使用具有成本效益的解决方案来实现精确的触觉特征估计，特别是在接触几何特征方面。我们假设，通过利用传感表面下2层专用声道的声能并分析振幅调制，我们可以有效地解码传感表面上的相互作用，从而改进触觉特征估计。我们的方法涉及负责发射和接收声学信号的硬件组件的独特分离，从而实现模块化和高度可定制的皮肤设计。实际测试证明了这种新设计的有效性，在估计接触法向力（MAE<0.8 N）、二维接触定位（MAE<0.7 mm）和接触表面直径（MAE<0.03 mm）方面实现了显著的精度。总之，AST皮肤凭借其创新的设计和模块化架构，成功应对了触觉特征估计的挑战。所给出的结果展示了其精确估计各种触觉特征的能力，使其成为机器人应用的实用且具有成本效益的解决方案。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>36</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.14292v1" target="_blank">2401.14292v1</a>
                              </td>
                              <td>AST-2: Single and bi-layered 2-D acoustic soft tactile skin</td>
                              <td>Vishnu Rajendran</td>
                              <td>2024-01-25</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_14292v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.14292v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2303_13496v3_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">The effectiveness of MAE pre-pretraining for billion-scale pretraining</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2303_13496v3_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2303_13496v3_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2303_13496v3_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video recognition, object detection, low-shot classification and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.7%), ImageNet-ReaL (91.1%), 1-shot ImageNet-1k (63.6%), and zero-shot transfer on Food-101 (96.2%). Our study reveals that model initialization plays a significant role, even for web-scale pretraining with billions of images, and our models are available publicly.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2303_13496v3_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>本文重新审视了计算机视觉中用于视觉识别任务的标准预训练-然后微调范式。通常，最先进的基础模型是使用具有数十亿图像的大规模（弱）监督数据集进行预训练的。我们引入了一个额外的预训练阶段，它很简单，并使用自监督MAE技术来初始化模型。虽然MAE仅被证明与模型的大小有关，但我们发现它也与训练数据集的大小有关。因此，我们基于MAE的预预训练量表具有模型和数据大小，使其适用于训练基础模型。预预训练在一系列模型规模（数百万到数十亿个参数）和数据集大小（数百万到几十亿个图像）上始终如一地提高了模型收敛性和下游传输性能。我们测量了预预训练对10种不同视觉识别任务的有效性，这些任务包括图像分类、视频识别、目标检测、低镜头分类和零样本识别。我们最大的模型在iNaturalist-18（91.7%）、ImageNet-ReaL（91.1%）、1-shot ImageNet-1k（63.6%）和Food-101（96.2%）上的零样本转移上获得了最先进的新结果。我们的研究表明，模型初始化起着重要作用，即使是在使用数十亿张图像进行网络规模的预训练时也是如此，而且我们的模型是公开的。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>37</td>
                              <td>
                                <a href="https://arxiv.org/abs/2303.13496v3" target="_blank">2303.13496v3</a>
                              </td>
                              <td>The effectiveness of MAE pre-pretraining for billion-scale pretraining</td>
                              <td>Mannat Singh</td>
                              <td>2023-03-23</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2303_13496v3_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2303.13496v3" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                                <a href="https://github.com/facebookresearch/maws" target="_blank">Link</a>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/facebookresearch/maws" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2302_14465v2_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Video Quality Assessment with Texture Information Fusion for Streaming Applications</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2302_14465v2_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2302_14465v2_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2302_14465v2_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>The rise in video streaming applications has increased the demand for video quality assessment (VQA). In 2016, Netflix introduced Video Multi-Method Assessment Fusion (VMAF), a full reference VQA metric that strongly correlates with perceptual quality, but its computation is time-intensive. We propose a Discrete Cosine Transform (DCT)-energy-based VQA with texture information fusion (VQ-TIF) model for video streaming applications that determines the visual quality of the reconstructed video compared to the original video. VQ-TIF extracts Structural Similarity (SSIM) and spatiotemporal features of the frames from the original and reconstructed videos and fuses them using a long short-term memory (LSTM)-based model to estimate the visual quality. Experimental results show that VQ-TIF estimates the visual quality with a Pearson Correlation Coefficient (PCC) of 0.96 and a Mean Absolute Error (MAE) of 2.71, on average, compared to the ground truth VMAF scores. Additionally, VQ-TIF estimates the visual quality at a rate of 9.14 times faster than the state-of-the-art VMAF implementation, along with an 89.44 % reduction in energy consumption, assuming an Ultra HD (2160p) display resolution.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2302_14465v2_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>视频流应用的兴起增加了对视频质量评估（VQA）的需求。2016年，Netflix推出了视频多方法评估融合（VMAF），这是一种与感知质量密切相关的全参考VQA指标，但其计算耗时。我们提出了一种用于视频流应用的基于离散余弦变换（DCT）能量的VQA和纹理信息融合（VQ-TIF）模型，该模型确定了重建视频与原始视频相比的视觉质量。VQ-TIF从原始和重建的视频中提取帧的结构相似性（SSIM）和时空特征，并使用基于长短期记忆（LSTM）的模型将其融合以估计视觉质量。实验结果表明，与真实的VMAF分数相比，VQ-TIF以0.96的Pearson相关系数（PCC）和2.71的平均绝对误差（MAE）来估计视觉质量。此外，VQ-TIF估计视觉质量比最先进的VMAF实现快9.14倍，同时在假设超高清（2160p）显示分辨率的情况下，能耗降低89.44%。</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>38</td>
                              <td>
                                <a href="https://arxiv.org/abs/2302.14465v2" target="_blank">2302.14465v2</a>
                              </td>
                              <td>Video Quality Assessment with Texture Information Fusion for Streaming Applications</td>
                              <td>Vignesh V Menon</td>
                              <td>2023-02-28</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2302_14465v2_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2302.14465v2" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                              </td>
                            </tr>
                            
                            <div class="modal" id="ID_2401_13171v1_8">
                              <div class="modal-dialog" role="document">
                                <div class="modal-content">
                                  <div class="modal-header">
                                    <h5 class="modal-title">Compositional Generative Inverse Design</h5> <button type="button" class="close"
                                      data-dismiss="modal"> <span>×</span> </button>
                                  </div>
                                  <ul class="nav nav-tabs">
                                    <li class="nav-item"> <a href="" class="nav-link active" data-toggle="pill"
                                        data-target="#ID_2401_13171v1_8_EN"><i
                                          class="fa fa-home"></i> EN</a> </li>
                                    <li class="nav-item"> <a href="" class="nav-link" data-toggle="pill"
                                        data-target="#ID_2401_13171v1_8_ZH"><i
                                          class="fa fa-home"></i> ZH</a> </li>
                                  </ul>
                                  <div class="tab-content mt-2">
                                    <div class="tab-pane fade active show"
                                      id="ID_2401_13171v1_8_EN"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent works leveraging optimization across learned dynamics models. However, as models are optimized they tend to fall into adversarial modes, preventing effective sampling. We illustrate that by instead optimizing over the learned energy function captured by the diffusion model, we can avoid such adversarial examples and significantly improve design performance. We further illustrate how such a design system is compositional, enabling us to combine multiple different diffusion models representing subcomponents of our desired system to design systems with every specified component. In an N-body interaction task and a challenging 2D multi-airfoil design task, we demonstrate that by composing the learned diffusion model at test time, our method allows us to design initial states and boundary shapes that are more complex than those in the training data. Our method outperforms state-of-the-art neural inverse design method by an average of 41.5% in prediction MAE and 14.3% in design objective for the N-body dataset and discovers formation flying to minimize drag in the multi-airfoil design task. Project website and code can be found at https://github.com/AI4Science-WestlakeU/cindm.</p>
                                      </div>
                                    </div>
                                    <div class="tab-pane fade"
                                      id="ID_2401_13171v1_8_ZH"
                                      role="tabpanel">
                                      <div class="modal-body">
                                        <p>逆向设计是机械工程和航空航天工程等领域出现的一个重要问题，我们试图设计输入变量以优化潜在的目标函数。逆向设计通常被表述为一个优化问题，最近的工作利用了学习动力学模型的优化。然而，随着模型的优化，它们往往会陷入对抗性模式，从而阻碍有效采样。我们说明，通过对扩散模型捕获的学习能量函数进行优化，我们可以避免这种对抗性的例子，并显著提高设计性能。我们进一步说明了这样的设计系统是如何组成的，使我们能够将代表我们所需系统的子组件的多个不同的扩散模型结合起来，设计具有每个特定组件的系统。在N体相互作用任务和具有挑战性的2D多翼型设计任务中，我们证明，通过在测试时组成学习的扩散模型，我们的方法使我们能够设计比训练数据中更复杂的初始状态和边界形状。我们的方法在N体数据集的预测MAE和设计目标方面的平均值分别比最先进的神经逆向设计方法高出41.5%和14.3%，并发现编队飞行可以在多翼型设计任务中最大限度地减少阻力。项目网站和代码可在https://github.com/AI4Science-WestlakeU/cindm.</p>
                                      </div>
                                    </div>
                                  </div>
                                  <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                                </div>
                              </div>
                            </div>
                            <tr>
                              <td>39</td>
                              <td>
                                <a href="https://arxiv.org/abs/2401.13171v1" target="_blank">2401.13171v1</a>
                              </td>
                              <td>Compositional Generative Inverse Design</td>
                              <td>Tailin Wu</td>
                              <td>2024-01-24</td>
                              <td>
                                <a data-toggle="modal"
                                  href="#ID_2401_13171v1_8"
                                  class="btn btn-primary">Show Abstract</a>
                              </td>
                              <td>
                                <a href="http://arxiv.org/pdf/2401.13171v1" target="_blank">PDF</a>
                              </td>
                              <td>
                                
                              </td>
                              <td>
                                
                                <a href="https://github.com/ai4science-westlakeu/cindm" target="_blank">Link</a>
                                
                              </td>
                            </tr>
                            
                          </tbody>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="text-muted text-center py-2">
    <div class="container">
      <div class="row">
        <div class="col-md-12 my-4">
          <p class="mb-1">© 2023 Li Yingping - Powered by Arxiv API.</p>
        </div>
      </div>
    </div>
  </div>
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js"
    integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"
    integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"
    crossorigin="anonymous"></script>
  <pingendo onclick="window.open('https://pingendo.com/', '_blank')"
    style="cursor:pointer;position: fixed;bottom: 20px;right:20px;padding:4px;background-color: #00b0eb;border-radius: 8px; width:220px;display:flex;flex-direction:row;align-items:center;justify-content:center;font-size:14px;color:white">
    Made with Pingendo Free&nbsp;&nbsp;<img src="https://pingendo.com/site-assets/Pingendo_logo_big.png" class="d-block"
      alt="Pingendo logo" height="16"></pingendo>
</body>

</html>